{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bff7ba1-3caa-4f2c-bd1a-dfbebc6a3111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler, IterableDataset\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import logging\n",
    "import addict\n",
    "from pathlib import Path\n",
    "import pickle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648e6fa4-3e14-4f6a-9e2b-bd0e7824b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"../storage/FGH_spec_ind_claims_triplet_v0.3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "831893fc-eee4-4bf6-be82-df42c0a4bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(t):\n",
    "    x = re.sub(\"\\d+.\",\"\", t) \n",
    "    x = x.replace(\"\\n\",\" \") \n",
    "    x = x.strip() \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "db914162-6134-4fe7-a087-873edd501494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletData(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(TripletData, self).__init__()\n",
    "        self.data = [txt for txt in Path(path).glob('*.txt')]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class custom_collate_metric_learning(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"tanapatentlm/patentdeberta_large_spec_128_pwi\")\n",
    "        self.chunk_size = 512\n",
    "    \n",
    "    def clean_text(self, t):\n",
    "        x = re.sub(\"\\d+.\",\"\", t) \n",
    "        x = x.replace(\"\\n\",\" \") \n",
    "        x = x.strip() \n",
    "        return x \n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        b = len(batch) \n",
    "        input_ids, attn_masks, labels = [], [], [] \n",
    "        ids = 0 \n",
    "        for idx, txt_file in enumerate(batch):\n",
    "            with txt_file.open(\"r\", encoding=\"utf8\") as f:\n",
    "                data = f.read()\n",
    "            triplet = data.split('\\n\\n\\n') \n",
    "            q,p,n = triplet \n",
    "            \n",
    "            q_ttl = re.search(\"<TTL>([\\s\\S]*?)<IPC>\", q).group(1) \n",
    "            q_ipc = re.search(\"<IPC>([\\s\\S]*?)<ABST>\", q).group(1)\n",
    "            q_abst = re.search(\"<ABST>([\\s\\S]*?)<CLMS>\", q).group(1) \n",
    "            q_clms = re.search(\"<CLMS>([\\s\\S]*?)<DESC>\", q).group(1) \n",
    "            q_ttl = q_ttl.lower() # convert title to lower case \n",
    "            q_ipc = q_ipc[:3] # get first three characters \n",
    "            # get first claim as long as it is not canceled \n",
    "            q_ind_clms = q_clms.split('\\n\\n') \n",
    "            selected_q_clm = q_ind_clms[0] \n",
    "            for q_ind_clm in q_ind_clms:\n",
    "                if '(canceled)' in q_ind_clm:\n",
    "                    continue\n",
    "                else:\n",
    "                    selected_q_clm = q_ind_clm\n",
    "                    break \n",
    "            selected_q_clm = self.clean_text(selected_q_clm)\n",
    "            q_text_input = q_ipc + \" \" + q_ttl + self.tokenizer.sep_token + q_abst + tokenizer.sep_token + selected_q_clm\n",
    "            encoded_q = self.tokenizer(q_text_input, return_tensors='pt', max_length=self.chunk_size, padding='max_length', truncation=True)\n",
    "            input_ids.append(encoded_q['input_ids'])  \n",
    "            attn_masks.append(encoded_q['attention_mask']) \n",
    "            labels.append(ids*2)  \n",
    "            \n",
    "            p_ttl = re.search(\"<TTL>([\\s\\S]*?)<IPC>\", p).group(1)  \n",
    "            p_ipc = re.search(\"<IPC>([\\s\\S]*?)<ABST>\", p).group(1) \n",
    "            p_abst = re.search(\"<ABST>([\\s\\S]*?)<CLMS>\", p).group(1) \n",
    "            p_clms = re.search(\"<CLMS>([\\s\\S]*?)<DESC>\", p).group(1) \n",
    "            p_ttl = p_ttl.lower() \n",
    "            p_ipc = p_ipc[:3] \n",
    "            p_ind_clms = p_clms.split('\\n\\n')\n",
    "            selected_p_clm = p_ind_clms[0] \n",
    "            for p_ind_clm in p_ind_clms:\n",
    "                if '(canceled)' in p_ind_clm:\n",
    "                    continue\n",
    "                else:\n",
    "                    selected_p_clm = p_ind_clm\n",
    "                    break \n",
    "            selected_p_clm = self.clean_text(selected_p_clm) \n",
    "            p_text_input = p_ipc + \" \" + p_ttl + self.tokenizer.sep_token + p_abst + tokenizer.sep_token + selected_p_clm \n",
    "            encoded_p = self.tokenizer(p_text_input, return_tensors='pt', max_length=self.chunk_size, padding='max_length', truncation=True) \n",
    "            input_ids.append(encoded_p['input_ids']) \n",
    "            attn_masks.append(encoded_p['attention_mask']) \n",
    "            labels.append(ids*2) \n",
    "            \n",
    "            n_ttl = re.search(\"<TTL>([\\s\\S]*?)<IPC>\", n).group(1) \n",
    "            n_ipc = re.search(\"<IPC>([\\s\\S]*?)<ABST>\", n).group(1) \n",
    "            n_abst = re.search(\"<ABST>([\\s\\S]*?)<CLMS>\", n).group(1) \n",
    "            n_clms = re.search(\"<CLMS>([\\s\\S]*?)<DESC>\", n).group(1) \n",
    "            n_ttl = n_ttl.lower() \n",
    "            n_ipc = n_ipc[:3] \n",
    "            n_ind_clms = n_clms.split('\\n\\n') \n",
    "            selected_n_clm = n_ind_clms[0] \n",
    "            for n_ind_clm in n_ind_clms:\n",
    "                if '(canceled)' in n_ind_clm:\n",
    "                    continue \n",
    "                else:\n",
    "                    selected_n_clm = n_ind_clm\n",
    "                    break \n",
    "            selected_n_clm = self.clean_text(selected_n_clm) \n",
    "            n_text_input = n_ipc + \" \" + n_ttl + self.tokenizer.sep_token + n_abst + tokenizer.sep_token + selected_n_clm \n",
    "            encoded_n = self.tokenizer(n_text_input, return_tensors='pt', max_length=self.chunk_size, padding='max_length', truncation=True) \n",
    "            input_ids.append(encoded_n['input_ids']) \n",
    "            attn_masks.append(encoded_n['attention_mask']) \n",
    "            labels.append(ids*2+1) \n",
    "        input_ids = torch.stack(input_ids, dim=0).squeeze(dim=1)  \n",
    "        attn_masks = torch.stack(attn_masks, dim=0).squeeze(dim=1)  \n",
    "        labels = torch.tensor(labels, dtype=int) \n",
    "        return input_ids, attn_masks, labels \n",
    "            \n",
    "            \n",
    "class custom_collate(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"tanapatentlm/patentdeberta_large_spec_128_pwi\")\n",
    "        self.chunk_size = 512\n",
    "    \n",
    "    def clean_text(self, t):\n",
    "        x = re.sub(\"\\d+.\",\"\", t) \n",
    "        x = x.replace(\"\\n\",\" \") \n",
    "        x = x.strip() \n",
    "        return x \n",
    "\n",
    "    def __call__(self, batch):\n",
    "        b = len(batch) \n",
    "        qb_input_ids, qb_attn_masks = torch.zeros((b, self.chunk_size),dtype=int), torch.zeros((b, self.chunk_size),dtype=int)\n",
    "        pb_input_ids, pb_attn_masks = torch.zeros((b, self.chunk_size),dtype=int), torch.zeros((b, self.chunk_size),dtype=int)\n",
    "        nb_input_ids, nb_attn_masks = torch.zeros((b, self.chunk_size),dtype=int), torch.zeros((b, self.chunk_size),dtype=int)\n",
    "        for idx, txt_file in enumerate(batch):\n",
    "            with txt_file.open(\"r\", encoding=\"utf8\") as f:\n",
    "                data = f.read()\n",
    "            triplet = data.split('\\n\\n\\n') \n",
    "            q,p,n = triplet \n",
    "            \n",
    "            q_ttl = re.search(\"<TTL>([\\s\\S]*?)<IPC>\", q).group(1) \n",
    "            q_ipc = re.search(\"<IPC>([\\s\\S]*?)<ABST>\", q).group(1)\n",
    "            q_abst = re.search(\"<ABST>([\\s\\S]*?)<CLMS>\", q).group(1) \n",
    "            q_clms = re.search(\"<CLMS>([\\s\\S]*?)<DESC>\", q).group(1) \n",
    "            q_ttl = q_ttl.lower() # convert title to lower case \n",
    "            q_ipc = q_ipc[:3] # get first three characters \n",
    "            # get first claim as long as it is not canceled \n",
    "            q_ind_clms = q_clms.split('\\n\\n') \n",
    "            selected_q_clm = q_ind_clms[0] \n",
    "            for q_ind_clm in q_ind_clms:\n",
    "                if '(canceled)' in q_ind_clm:\n",
    "                    continue\n",
    "                else:\n",
    "                    selected_q_clm = q_ind_clm\n",
    "                    break \n",
    "            selected_q_clm = self.clean_text(selected_q_clm)\n",
    "            q_text_input = q_ipc + \" \" + q_ttl + self.tokenizer.sep_token + q_abst + tokenizer.sep_token + selected_q_clm  \n",
    "            encoded_q = self.tokenizer(q_text_input, return_tensors='pt', max_length=self.chunk_size, padding='max_length', truncation=True)\n",
    "            \n",
    "            p_ttl = re.search(\"<TTL>([\\s\\S]*?)<IPC>\", p).group(1)  \n",
    "            p_ipc = re.search(\"<IPC>([\\s\\S]*?)<ABST>\", p).group(1) \n",
    "            p_abst = re.search(\"<ABST>([\\s\\S]*?)<CLMS>\", p).group(1) \n",
    "            p_clms = re.search(\"<CLMS>([\\s\\S]*?)<DESC>\", p).group(1) \n",
    "            p_ttl = p_ttl.lower() \n",
    "            p_ipc = p_ipc[:3] \n",
    "            p_ind_clms = p_clms.split('\\n\\n')\n",
    "            selected_p_clm = p_ind_clms[0] \n",
    "            for p_ind_clm in p_ind_clms:\n",
    "                if '(canceled)' in p_ind_clm:\n",
    "                    continue\n",
    "                else:\n",
    "                    selected_p_clm = p_ind_clm\n",
    "                    break \n",
    "            selected_p_clm = self.clean_text(selected_p_clm) \n",
    "            p_text_input = p_ipc + \" \" + p_ttl + self.tokenizer.sep_token + p_abst + tokenizer.sep_token + selected_p_clm \n",
    "            encoded_p = self.tokenizer(p_text_input, return_tensors='pt', max_length=self.chunk_size, padding='max_length', truncation=True) \n",
    "            \n",
    "            n_ttl = re.search(\"<TTL>([\\s\\S]*?)<IPC>\", n).group(1) \n",
    "            n_ipc = re.search(\"<IPC>([\\s\\S]*?)<ABST>\", n).group(1) \n",
    "            n_abst = re.search(\"<ABST>([\\s\\S]*?)<CLMS>\", n).group(1) \n",
    "            n_clms = re.search(\"<CLMS>([\\s\\S]*?)<DESC>\", n).group(1) \n",
    "            n_ttl = n_ttl.lower() \n",
    "            n_ipc = n_ipc[:3] \n",
    "            n_ind_clms = n_clms.split('\\n\\n') \n",
    "            selected_n_clm = n_ind_clms[0] \n",
    "            for n_ind_clm in n_ind_clms:\n",
    "                if '(canceled)' in n_ind_clm:\n",
    "                    continue \n",
    "                else:\n",
    "                    selected_n_clm = n_ind_clm\n",
    "                    break \n",
    "            selected_n_clm = self.clean_text(selected_n_clm) \n",
    "            n_text_input = n_ipc + \" \" + n_ttl + self.tokenizer.sep_token + n_abst + tokenizer.sep_token + selected_n_clm \n",
    "            encoded_n = self.tokenizer(n_text_input, return_tensors='pt', max_length=self.chunk_size, padding='max_length', truncation=True) \n",
    "            \n",
    "            qb_input_ids[idx] = encoded_q['input_ids'] \n",
    "            qb_attn_masks[idx] = encoded_q['attention_mask']\n",
    "            \n",
    "            pb_input_ids[idx] = encoded_p['input_ids'] \n",
    "            pb_attn_masks[idx] = encoded_p['attention_mask'] \n",
    "            \n",
    "            nb_input_ids[idx] = encoded_n['input_ids'] \n",
    "            nb_attn_masks[idx] = encoded_n['attention_mask'] \n",
    "        return qb_input_ids, qb_attn_masks, pb_input_ids, pb_attn_masks, nb_input_ids, nb_attn_masks     \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4ed3a3a9-9e8c-4efb-af1e-a3b012dee3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TripletData(\"../storage/train_spec\")\n",
    "collate = custom_collate()\n",
    "train_dataloader = DataLoader(train_set, batch_size=8, collate_fn=collate, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f65d3a7c-e0d0-4891-a711-9a0a7f0db0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00,  7.57it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tanapatentlm/patentdeberta_large_spec_128_pwi\") \n",
    "\n",
    "cnt = 0 \n",
    "\n",
    "for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "    q_input_id, q_attn_mask, p_input_id, p_attn_mask, n_input_id, n_attn_mask = batch \n",
    "    cnt += 1 \n",
    "    if cnt == 5:\n",
    "        break \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e1597ec9-83b0-4c3c-991e-df024ee347af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]G06 link system[SEP]In a link system in which a plurality of MFPs and a server apparatus is connected so as to enable data communication over a network, a remote connection driver of the MFP starts an application on the server apparatus by remote control and at the time, on the basis of an operation start signal transmitted from the MFP, a device setting manager of the server apparatus identifies as a remote operation host apparatus the MFP having transmitted the operation start signal, and automatically sets the MFP as the MFP by which the application is used.[SEP]A link system in which a plurality of image forming apparatuses and a server apparatus are connected so as to enable data communication over a network, the image forming apparatus having a remote operating section adapted to make an application on the server apparatus available by remote control, and the server apparatus having: a remote operation host apparatus identifying section for identifying a remote operation host apparatus that is an image forming apparatus which is currently performing a remote operation, with the remote operating section, among the plurality of image forming apparatuses connected over a network; and an automatic setting section adapted to automatically set an image forming apparatus to be used for printing a print job of an application, wherein the remote operating section of the image forming apparatus starts an application on the server apparatus by remote control, the image forming apparatus transmits over a network an operation start signal indicative of onset of the remote control, and the server apparatus receives the operation start signal transmitted from the image forming apparatus, based on which signal: the remote operation host apparatus identifying section identifies as a remote operation host apparatus the image forming apparatus having transmitted the operation start signal; and based on an identification result of the remote operation host apparatus identifying section, the automatic setting section automatically sets as the remote operation host apparatus an image forming apparatus to be used for printing by the application.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(q_input_id[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f643a-e823-4a21-884a-c09e92a32beb",
   "metadata": {},
   "source": [
    "A system, method and program product for providing a voice response unit (VRU) proxy. A system is provided that includes: a graphical user interface (GUI) for dynamically displaying information from a VRU and for receiving data from a user; a system for initiating a call with the VRU; and a VRU interface system for transmitting data from the user to the VRU, and for using speech recognition to capture broadcasts from the VRU for display within the GUI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e880abc-1a5b-459e-aadc-48478bf7bdb3",
   "metadata": {},
   "source": [
    "A voice response unit (VRU) proxy system, comprising: a graphical user interface (GUI) for dynamically displaying information from a VRU and for receiving data from a user; a system for initiating a call with the VRU; and a VRU interface system for transmitting data from the user to the VRU, and for using speech recognition to capture broadcasts from the VRU for display within the GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "985264d8-8885-4655-9408-f9b73586f5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]H04 facsimile communication system and image processing apparatus[SEP]Leakage of communication information concerning facsimile communication is prevented. A facsimile server manages communication information concerning facsimile communication. An image processing apparatus connects itself to the facsimile server to acquire communication information. Only a part of communication information is displayed, and a user selects a transmission destination. The image processing apparatus sends the designated transmission destination and image data to the facsimile server and requests facsimile communication. The facsimile server sends image data to the designated transmission destination.[SEP]A facsimile communication system, wherein an image processing apparatus and a server having a facsimile communication function are communicatively connected, the server has a management section managing communication information concerning facsimile communication, and the image processing apparatus has a transmission requesting section requesting the server for facsimile communication referring to the communication information.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(p_input_id[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9ff3c27e-3875-4614-bb0a-c86464906277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]G03 image forming system, an apparatus, and method for controlling the same[SEP]When a printing process on a first apparatus is interrupted due to an error and the printing process is resumed by a second apparatus, it is determined whether a predetermined function (for example, stapling) is selected for the printing process according to the printing mode. If the predetermined function is selected for the printing process, the second printing apparatus prints the number of copies including the copy interrupted in the first machine according to the print setting. If the predetermined function is not selected for the printing process, the second printing apparatus prints the number of copies excluding the interrupted copy and the unprinted pages in the interrupted copy according to the print setting.[SEP]An image forming system including a first apparatus and a second apparatus, the first apparatus reads an image of an original document, the read image data is transmitted to the second apparatus, the image forming system capable of controlling a printing mode in which printing of a plurality of copies of the read image data is performed using the first apparatus and the second apparatus, the image forming system comprising:  a determining unit configured to determine whether a predetermined function is selected on a printing process according to the printing mode in a case that an error occurs in one of the first apparatus and the second apparatus is printing one of the plurality of copies; and  a control unit configured to control the other of the first apparatus and the second apparatus to print page which is not printed in the one of the plurality of copies without printing page which is printed by the one of the first apparatus and the second apparatus in the one of the plurality of copies in a case that the determining unit determines that the predetermined function is not selected, and to control the other of the first apparatus and the second apparatus to print page which is not printed in the one of the plurality of copies with printing page which is printed by the one of the first apparatus and the second apparatus in the one of the plurality of copies in a case that the determining unit determines that the predetermined function is selected.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(n_input_id[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cecf12a-8398-432e-a966-c0b2eb1e1ae1",
   "metadata": {},
   "source": [
    "There is provided a display device of high display quality having a circuit substrate, on which electronic parts are correctly mounted in a manner to afford visually recognizing presence and absence of misalignment of a solder resist opening with ease.\\nThe display device comprises a display panel and a circuit substrate connected to the display panel, the circuit substrate comprises an insulating substrate, a conductive layer, an insulating layer to cover a part of the conductive layer, a plating layer applied to that portion of the conductive layer, which is exposed from the insulating layer, and a misalignment detection pattern for detection of misalignment between the conductive layer and the insulating layer, and the misalignment detection pattern comprises a pattern covered by the insulating layer in a manner to prevent adherence of a plating material to the conductive layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da540e9c-ee9f-4135-b245-de1885de333c",
   "metadata": {},
   "source": [
    "A display device comprising a display panel and a circuit substrate connected to the display panel, wherein the display panel includes an array substrate, the circuit substrate is separately formed and positioned different from the array substrate and comprises an insulating substrate, a conductive layer, and an insulating layer to cover a part of the conductive layer, the conductive layer of the circuit substrate includes a wiring pattern, wiring terminals, and a meshy pattern, the wiring terminals are exposed from the insulating layer and are applied with a plating layer to a surface exposed from the insulating layer, a misalignment detection pattern for detecting the misalignment between the conductive layer and the insulating layer is formed on the circuit substrate, the misalignment detection pattern includes a first layer formed of the conductive layer, a second layer formed of the insulating layer having an opening, the first layer is connected with the meshy pattern and is supplied with a predetermined potential, if the misalignment between the conductive layer and the insulating layer is smaller than a predetermined misalignment tolerance, the first layer is wholly covered by the second layer in a manner to prevent adherence of a plating material to the first layer, and if the misalignment between the conductive layer and the insulating layer is larger than the predetermined misalignment tolerance, a part of the first layer is exposed from the second layer at the opening in a manner to be applied with plating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669335dd-f0c9-48c8-ab87-029fc9eeff27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
