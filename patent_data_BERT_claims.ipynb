{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT+LSTM model is used for processing the claims information from patent documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.functional as f \n",
    "from torch.optim import AdamW \n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler  \n",
    "from sklearn.model_selection import train_test_split \n",
    "import time \n",
    "import datetime \n",
    "import re \n",
    "import math \n",
    "import os \n",
    "import json \n",
    "from tqdm import tqdm \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BigBirdModel, BigBirdTokenizer, get_linear_schedule_with_warmup, BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sample files = 1471\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('sample_data') \n",
    "print(\"number of sample files = {}\".format(len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['patent1id', 'patent2id', 'fullpatent1id', 'fullpatent2id', 'patent1abstract', 'patent2abstract', 'patent1claims', 'patent2claims'])\n"
     ]
    }
   ],
   "source": [
    "with open('sample_data/'+files[0]) as f: \n",
    "    data = json.load(f) \n",
    "    print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem occured with file 12001939_20030108880.json\n",
      "Extra data: line 89 column 2 (char 14144)\n",
      "problem occured with file 12126663_20080250190.json\n",
      "Extra data: line 85 column 2 (char 20614)\n",
      "problem occured with file 12001631_5382185.json\n",
      "Extra data: line 106 column 2 (char 29769)\n",
      "problem occured with file 12001003_4589656.json\n",
      "Extra data: line 114 column 2 (char 25639)\n",
      "problem occured with file 12002104_6609036.json\n",
      "Extra data: line 191 column 2 (char 37994)\n",
      "problem occured with file 12002154_20010029974.json\n",
      "Extra data: line 114 column 2 (char 36254)\n",
      "problem occured with file 12001021_20060183900.json\n",
      "Extra data: line 395 column 2 (char 126007)\n",
      "problem occured with file 12002137_7338160.json\n",
      "Extra data: line 77 column 2 (char 15328)\n",
      "problem occured with file 12000641_20070108429.json\n",
      "Extra data: line 125 column 2 (char 30036)\n",
      "problem occured with file 12001962_20040263061.json\n",
      "Extra data: line 104 column 2 (char 31536)\n",
      "problem occured with file 12000959_20050276053.json\n",
      "Extra data: line 100 column 2 (char 14009)\n",
      "problem occured with file 12000891_5700250.json\n",
      "Extra data: line 85 column 2 (char 17949)\n",
      "problem occured with file 12001163_6758760.json\n",
      "Extra data: line 75 column 2 (char 13246)\n",
      "problem occured with file 12001571_5082007.json\n",
      "Extra data: line 97 column 2 (char 26012)\n",
      "problem occured with file 12001281_20020105352.json\n",
      "Extra data: line 54 column 2 (char 13608)\n",
      "problem occured with file 12000822_20020020426.json\n",
      "Extra data: line 427 column 2 (char 86260)\n",
      "problem occured with file 12001889_20040014955.json\n",
      "Extra data: line 125 column 2 (char 28214)\n",
      "problem occured with file 12001586_6720860.json\n",
      "Extra data: line 67 column 2 (char 15161)\n",
      "problem occured with file 12001281_7342405.json\n",
      "Extra data: line 87 column 2 (char 16915)\n",
      "problem occured with file 12001351_20060043011.json\n",
      "Extra data: line 89 column 2 (char 16951)\n",
      "problem occured with file 12000618_20080055227.json\n",
      "Extra data: line 141 column 2 (char 41587)\n",
      "problem occured with file 12001260_20050042489.json\n",
      "Extra data: line 62 column 2 (char 10708)\n",
      "problem occured with file 12001247_20050147269.json\n",
      "Extra data: line 130 column 2 (char 26054)\n",
      "problem occured with file 12001872_6631331.json\n",
      "Extra data: line 47 column 2 (char 9841)\n",
      "problem occured with file 12001872_20060039593.json\n",
      "Extra data: line 96 column 2 (char 18428)\n",
      "problem occured with file 12001771_5369739.json\n",
      "Extra data: line 129 column 2 (char 25852)\n",
      "problem occured with file 12002187_20070001278.json\n",
      "Extra data: line 225 column 2 (char 43657)\n",
      "problem occured with file 12001473_20060020343.json\n",
      "Extra data: line 87 column 2 (char 30683)\n",
      "problem occured with file 12000491_3751333.json\n",
      "Extra data: line 37 column 2 (char 5139)\n",
      "problem occured with file 12001571_5323787.json\n",
      "Extra data: line 100 column 2 (char 24451)\n",
      "problem occured with file 12126713_5849602.json\n",
      "Extra data: line 62 column 2 (char 13916)\n",
      "problem occured with file 12000662_20040094050.json\n",
      "Extra data: line 253 column 2 (char 63125)\n",
      "problem occured with file 12000414_6913295.json\n",
      "Extra data: line 73 column 2 (char 14197)\n",
      "problem occured with file 12002225_6337507.json\n",
      "Extra data: line 38 column 2 (char 9905)\n",
      "problem occured with file 12000811_20020033664.json\n",
      "Extra data: line 53 column 2 (char 11167)\n",
      "problem occured with file 12002187_7071545.json\n",
      "Extra data: line 67 column 2 (char 15417)\n",
      "problem occured with file 12001983_20050147559.json\n",
      "Extra data: line 164 column 2 (char 26382)\n",
      "problem occured with file 12002288_4925446.json\n",
      "Extra data: line 116 column 2 (char 26287)\n",
      "problem occured with file 12001473_20070299519.json\n",
      "Extra data: line 64 column 2 (char 26742)\n",
      "problem occured with file 12001916_20060141014.json\n",
      "Extra data: line 87 column 2 (char 19433)\n",
      "problem occured with file 12002288_6547776.json\n",
      "Extra data: line 111 column 2 (char 17817)\n",
      "problem occured with file 12001351_20030186955.json\n",
      "Extra data: line 54 column 2 (char 11672)\n",
      "problem occured with file 12000872_20050284529.json\n",
      "Extra data: line 46 column 2 (char 10372)\n",
      "problem occured with file 12001247_6585075.json\n",
      "Extra data: line 135 column 2 (char 25191)\n",
      "problem occured with file 12002054_20080048308.json\n",
      "Extra data: line 100 column 2 (char 30749)\n",
      "problem occured with file 12001025_5994629.json\n",
      "Extra data: line 86 column 2 (char 24674)\n",
      "problem occured with file 12126622_6357846.json\n",
      "Extra data: line 75 column 2 (char 46227)\n",
      "problem occured with file 12126837_20070036292.json\n",
      "Extra data: line 81 column 2 (char 24839)\n",
      "problem occured with file 12001116_6409058.json\n",
      "Extra data: line 85 column 2 (char 16751)\n",
      "problem occured with file 12002080_20040217009.json\n",
      "Extra data: line 37 column 2 (char 5669)\n",
      "problem occured with file 12002039_6146335.json\n",
      "Extra data: line 102 column 2 (char 23330)\n",
      "problem occured with file 12001351_5888428.json\n",
      "Extra data: line 49 column 2 (char 10233)\n",
      "problem occured with file 12000414_6347640.json\n",
      "Extra data: line 73 column 2 (char 11635)\n",
      "problem occured with file 12002317_7898994.json\n",
      "Extra data: line 50 column 2 (char 10255)\n",
      "problem occured with file 12001493_5678566.json\n",
      "Extra data: line 105 column 2 (char 29338)\n",
      "problem occured with file 12000491_4140911.json\n",
      "Extra data: line 47 column 2 (char 8712)\n",
      "problem occured with file 12001771_5473391.json\n",
      "Extra data: line 89 column 2 (char 22250)\n",
      "problem occured with file 12000618_20070046608.json\n",
      "Extra data: line 130 column 2 (char 46161)\n",
      "problem occured with file 12001351_6749758.json\n",
      "Extra data: line 88 column 2 (char 20584)\n",
      "problem occured with file 12126833_6139373.json\n",
      "Extra data: line 77 column 2 (char 17980)\n",
      "problem occured with file 12000222_20050232963.json\n",
      "Extra data: line 133 column 2 (char 23226)\n",
      "problem occured with file 12001351_20040129644.json\n",
      "Extra data: line 112 column 2 (char 21449)\n",
      "problem occured with file 12001473_20040199166.json\n",
      "Extra data: line 83 column 2 (char 29909)\n",
      "problem occured with file 12001281_20020145437.json\n",
      "Extra data: line 120 column 2 (char 22040)\n",
      "problem occured with file 12000564_20030118876.json\n",
      "Extra data: line 89 column 2 (char 25011)\n",
      "problem occured with file 12001260_20050215728.json\n",
      "Extra data: line 86 column 2 (char 13249)\n",
      "problem occured with file 12001645_5902475.json\n",
      "Extra data: line 99 column 2 (char 23880)\n",
      "problem occured with file 12000959_20070274093.json\n",
      "Extra data: line 68 column 2 (char 13218)\n",
      "problem occured with file 12001771_5982553.json\n",
      "Extra data: line 164 column 2 (char 44312)\n",
      "problem occured with file 12000618_20090121998.json\n",
      "Extra data: line 107 column 2 (char 40566)\n",
      "problem occured with file 12001247_20040215053.json\n",
      "Extra data: line 106 column 2 (char 23494)\n",
      "problem occured with file 12001962_5813753.json\n",
      "Extra data: line 72 column 2 (char 12944)\n",
      "problem occured with file 12000222_20060165635.json\n",
      "Extra data: line 117 column 2 (char 21304)\n",
      "problem occured with file 12002080_20050189231.json\n",
      "Extra data: line 125 column 2 (char 28935)\n",
      "problem occured with file 12001062_20080254336.json\n",
      "Extra data: line 102 column 2 (char 25058)\n",
      "problem occured with file 12001916_6322801.json\n",
      "Extra data: line 103 column 2 (char 19378)\n",
      "problem occured with file 12000414_5529086.json\n",
      "Extra data: line 89 column 2 (char 17655)\n",
      "problem occured with file 12001771_5798743.json\n",
      "Extra data: line 104 column 2 (char 23344)\n",
      "problem occured with file 12002054_7071545.json\n",
      "Extra data: line 60 column 2 (char 18140)\n",
      "problem occured with file 12001586_20070028111.json\n",
      "Extra data: line 129 column 2 (char 20858)\n",
      "problem occured with file 12000811_20060103324.json\n",
      "Extra data: line 101 column 2 (char 21074)\n",
      "problem occured with file 12001889_20060100187.json\n",
      "Extra data: line 75 column 2 (char 12971)\n",
      "problem occured with file 12000822_20040168698.json\n",
      "Extra data: line 83 column 2 (char 17035)\n",
      "problem occured with file 12000546_5461557.json\n",
      "Extra data: line 74 column 2 (char 19642)\n",
      "problem occured with file 12001442_20060188116.json\n",
      "Extra data: line 131 column 2 (char 23036)\n",
      "problem occured with file 12000564_20040144579.json\n",
      "Extra data: line 34 column 2 (char 9830)\n",
      "problem occured with file 12001813_4082011.json\n",
      "Extra data: line 53 column 2 (char 27090)\n",
      "problem occured with file 12000990_5345388.json\n",
      "Extra data: line 91 column 2 (char 29753)\n",
      "problem occured with file 12002312_6592578.json\n",
      "Extra data: line 161 column 2 (char 43847)\n",
      "problem occured with file 12000242_20040232509.json\n",
      "Extra data: line 140 column 2 (char 21415)\n",
      "problem occured with file 12001473_7678151.json\n",
      "Extra data: line 80 column 2 (char 34321)\n",
      "problem occured with file 12002288_20040158194.json\n",
      "Extra data: line 225 column 2 (char 46023)\n",
      "problem occured with file 12002097_20020058350.json\n",
      "Extra data: line 74 column 2 (char 12198)\n",
      "problem occured with file 12001493_20050047677.json\n",
      "Extra data: line 115 column 2 (char 27526)\n",
      "problem occured with file 12000891_5242410.json\n",
      "Extra data: line 89 column 2 (char 19854)\n",
      "problem occured with file 12126622_6328395.json\n",
      "Extra data: line 96 column 2 (char 53015)\n",
      "problem occured with file 12000811_20070262707.json\n",
      "Extra data: line 74 column 2 (char 12850)\n",
      "problem occured with file 12000018_6189895.json\n",
      "Extra data: line 56 column 2 (char 13126)\n",
      "problem occured with file 12000879_6631668.json\n",
      "Extra data: line 69 column 2 (char 15549)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem occured with file 12001062_20060257718.json\n",
      "Extra data: line 49 column 2 (char 14449)\n",
      "problem occured with file 12126762_6777660.json\n",
      "Extra data: line 80 column 2 (char 16719)\n",
      "problem occured with file 12000298_6851645.json\n",
      "Extra data: line 120 column 2 (char 29592)\n",
      "problem occured with file 12126762_6587142.json\n",
      "Extra data: line 75 column 2 (char 12556)\n",
      "problem occured with file 12000716_20030156467.json\n",
      "Extra data: line 172 column 2 (char 34440)\n",
      "problem occured with file 12002154_3879229.json\n",
      "Extra data: line 47 column 2 (char 31372)\n"
     ]
    }
   ],
   "source": [
    "sample_train = [] \n",
    "\n",
    "cnt = 0 \n",
    "\n",
    "for idx, file in enumerate(files): \n",
    "    filename = 'sample_data/' + file\n",
    "    if '.json' not in filename: \n",
    "        continue\n",
    "    try: \n",
    "        with open(filename) as f:  \n",
    "            data = json.load(f) \n",
    "            sample_train.append(data)\n",
    "    except Exception as e: \n",
    "        print(\"problem occured with file {}\".format(file))\n",
    "        print(e)\n",
    "        cnt += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of problematic files = 105/1471\n"
     ]
    }
   ],
   "source": [
    "print(\"number of problematic files = {}/{}\".format(cnt, len(files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(s, overlap, chunk_size): \n",
    "    total = [] \n",
    "    partial = [] \n",
    "    if len(s.split()) // (chunk_size - overlap) > 0: \n",
    "        n = len(s.split()) // (chunk_size - overlap) \n",
    "    else: \n",
    "        n = 1 \n",
    "    for w in range(n): \n",
    "        if w == 0: \n",
    "            partial = s.split()[:chunk_size]\n",
    "            total.append(\" \".join(partial)) \n",
    "        else: \n",
    "            partial = s.split()[w*(chunk_size - overlap):w*(chunk_size - overlap) + chunk_size] \n",
    "            total.append(\" \".join(partial))  \n",
    "    return total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(in_string,num_chunks):\n",
    "    chunk_size = len(in_string)//num_chunks\n",
    "    if len(in_string) % num_chunks: chunk_size += 1\n",
    "    iterator = iter(in_string)\n",
    "    for _ in range(num_chunks):\n",
    "        accumulator = list()\n",
    "        for _ in range(chunk_size):\n",
    "            try: accumulator.append(next(iterator))\n",
    "            except StopIteration: break\n",
    "        yield ''.join(accumulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define tokenizer ### \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_parts(s, n): \n",
    "    splitted_ids, splitted_masks = [], [] \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = s, \n",
    "        add_special_tokens = True, \n",
    "        pad_to_max_length = False,  \n",
    "        return_attention_mask = True \n",
    "    ) \n",
    "    input_id = encoded_dict['input_ids'] \n",
    "    attention_mask = encoded_dict['attention_mask'] \n",
    "    \n",
    "    ### split into n chunks ### \n",
    "    splitted_input_id = np.array_split(np.asarray(input_id), n) \n",
    "    splitted_attention_mask = np.array_split(np.asarray(attention_mask), n) \n",
    "    \n",
    "    return splitted_input_id, splitted_attention_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1365 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1959 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 1365/1365 [02:19<00:00,  9.75it/s]\n"
     ]
    }
   ],
   "source": [
    "splitted_ids1, splitted_ids2 = [], [] \n",
    "splitted_masks1, splitted_masks2 = [], []   \n",
    "labels = [] \n",
    "longcnts = 0 \n",
    "for i in tqdm(range(len(sample_train)), position=0, leave=True):  \n",
    "\n",
    "    text = sample_train[i]['patent1claims']\n",
    "    text = ' '.join(text)\n",
    "    splitted_id, splitted_attention_mask = split_into_parts(text, 50)\n",
    "    splitted_id1, splitted_mask1 = [], []  \n",
    "    for j in range(len(splitted_id)): \n",
    "        if len(splitted_id[j]) < 100: \n",
    "            splitted_id1.append(np.concatenate([splitted_id[j], np.zeros((100-len(splitted_id[j])))]))\n",
    "            splitted_mask1.append(np.concatenate([splitted_attention_mask[j], np.zeros((100-len(splitted_attention_mask[j])))]))\n",
    "        elif len(splitted_id[j]) >= 100: \n",
    "            splitted_id1.append(splitted_id[j][:100]) \n",
    "            splitted_mask1.append(splitted_attention_mask[j][:100])\n",
    "    splitted_ids1.append(splitted_id1) \n",
    "    splitted_masks1.append(splitted_mask1)\n",
    "        \n",
    "    text2 = sample_train[i]['patent2claims']\n",
    "    text2 = ' '.join(text2) \n",
    "    splitted_id, splitted_attention_mask = split_into_parts(text2, 50)\n",
    "    splitted_id2, splitted_mask2 = [], [] \n",
    "    for j in range(len(splitted_id)): \n",
    "        if len(splitted_id[j]) < 100: \n",
    "            splitted_id2.append(np.concatenate([splitted_id[j], np.zeros((100-len(splitted_id[j])))])) \n",
    "            splitted_mask2.append(np.concatenate([splitted_attention_mask[j], np.zeros((100-len(splitted_attention_mask[j])))]))\n",
    "        elif len(splitted_id[j]) >= 100:\n",
    "            splitted_id2.append(splitted_id[j][:100])  \n",
    "            splitted_mask2.append(splitted_attention_mask[j][:100])  \n",
    "    \n",
    "    \n",
    "    splitted_ids2.append(splitted_id2) \n",
    "    splitted_masks2.append(splitted_mask2)\n",
    "    \n",
    "    ### currently all data are positive samples ### \n",
    "    labels.append(1.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1365, 50, 100]),\n",
       " torch.Size([1365, 50, 100]),\n",
       " torch.Size([1365, 50, 100]),\n",
       " torch.Size([1365, 50, 100]),\n",
       " torch.Size([1365, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### shape should be (batch, 50, 100) ### \n",
    "\n",
    "splitted_ids1 = torch.tensor(splitted_ids1, dtype=int) \n",
    "splitted_ids2 = torch.tensor(splitted_ids2, dtype=int) \n",
    "splitted_masks1 = torch.tensor(splitted_masks1, dtype=int) \n",
    "splitted_masks2 = torch.tensor(splitted_masks2, dtype=int)\n",
    "labels = torch.tensor(labels, dtype=float)\n",
    "labels = torch.reshape(labels, (-1,1))\n",
    "\n",
    "splitted_ids1.shape, splitted_ids2.shape, splitted_masks1.shape, splitted_masks2.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1228, 50, 100]),\n",
       " torch.Size([137, 50, 100]),\n",
       " torch.Size([1228, 50, 100]),\n",
       " torch.Size([137, 50, 100]),\n",
       " torch.Size([1228, 50, 100]),\n",
       " torch.Size([137, 50, 100]),\n",
       " torch.Size([1228, 50, 100]),\n",
       " torch.Size([137, 50, 100]),\n",
       " torch.Size([1228, 1]),\n",
       " torch.Size([137, 1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_claims1_ids, val_claims1_ids, train_claims1_attention_masks, val_claims1_attention_masks = train_test_split(splitted_ids1, \n",
    "                                                                                                                  splitted_masks1, \n",
    "                                                                                                                  random_state=888, \n",
    "                                                                                                                  test_size=0.1)\n",
    "\n",
    "train_claims2_ids, val_claims2_ids, train_claims2_attention_masks, val_claims2_attention_masks = train_test_split(splitted_ids2,\n",
    "                                                                                                                  splitted_masks2,\n",
    "                                                                                                                  random_state=888, \n",
    "                                                                                                                  test_size=0.1)\n",
    "\n",
    "_, _, train_labels, val_labels = train_test_split(splitted_ids1, \n",
    "                                                  labels, \n",
    "                                                  random_state=888, \n",
    "                                                  test_size=0.1)\n",
    "\n",
    "\n",
    "train_claims1_ids.shape, val_claims1_ids.shape, train_claims1_attention_masks.shape, val_claims1_attention_masks.shape, train_claims2_ids.shape, val_claims2_ids.shape, train_claims2_attention_masks.shape, val_claims2_attention_masks.shape, train_labels.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "train_data = TensorDataset(train_claims1_ids, \n",
    "                           train_claims1_attention_masks, \n",
    "                           train_claims2_ids, \n",
    "                           train_claims2_attention_masks, \n",
    "                           train_labels)\n",
    "\n",
    "train_sampler = RandomSampler(train_data) \n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = TensorDataset(val_claims1_ids, \n",
    "                                val_claims1_attention_masks, \n",
    "                                val_claims2_ids, \n",
    "                                val_claims2_attention_masks,  \n",
    "                                val_labels) \n",
    "\n",
    "val_sampler = SequentialSampler(validation_data) \n",
    "\n",
    "val_dataloader = DataLoader(validation_data, sampler = val_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claims model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_LSTM(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(BERT_LSTM, self).__init__() \n",
    "        self.bert1 = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "        self.bert2 = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "        self.lstm1 = nn.LSTM(768, 256, batch_first=True, bidirectional=True) \n",
    "        self.lstm2 = nn.LSTM(768, 256, batch_first=True, bidirectional=True) \n",
    "        self.fc1 = nn.Linear(1024, 256) \n",
    "        self.batchnorm1 = nn.BatchNorm1d(256) \n",
    "        self.fc2 = nn.Linear(256, 64) \n",
    "        self.batchnorm2 = nn.BatchNorm1d(64) \n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.activation = nn.Sigmoid() \n",
    "        self.seq_len = 50\n",
    "        \n",
    "    def forward(self, ids1, masks1, ids2, masks2): \n",
    "        first_seq = [] \n",
    "        batch_size = ids1.shape[0]\n",
    "        for i in range(self.seq_len): \n",
    "            outputs1 = self.bert1(input_ids = ids1[:,i,:], \n",
    "                                  attention_mask = masks1[:,i,:]) \n",
    "            pooler1 = outputs1.pooler_output\n",
    "            pooler1 = pooler1.flatten()\n",
    "            first_seq.append(pooler1)\n",
    "        first_seq = torch.stack(first_seq) \n",
    "        first_seq = torch.reshape(first_seq, (batch_size, self.seq_len, 768))\n",
    "        # print(first_seq.shape)\n",
    "        \n",
    "        second_seq = [] \n",
    "        for i in range(self.seq_len): \n",
    "            outputs2 = self.bert2(input_ids = ids2[:,i,:], \n",
    "                                  attention_mask = masks2[:,i,:])\n",
    "            pooler2 = outputs2.pooler_output\n",
    "            pooler2 = pooler2.flatten() \n",
    "            second_seq.append(pooler2) \n",
    "        second_seq = torch.stack(second_seq) \n",
    "        second_seq = torch.reshape(second_seq, (batch_size, self.seq_len, 768))\n",
    "        # print(second_seq.shape)\n",
    "        \n",
    "        lstm1_output, (h, c) = self.lstm1(first_seq) \n",
    "        lstm2_output, (h, c) = self.lstm2(second_seq) \n",
    "        \n",
    "        lstm1_output = lstm1_output[:,-1,:] \n",
    "        lstm2_output = lstm2_output[:,-1,:] \n",
    "        \n",
    "        # print(lstm1_output.shape, lstm2_output.shape) \n",
    "        \n",
    "        hidden = torch.cat((lstm1_output, lstm2_output), axis=1) \n",
    "        # print(\"hidden shape = {}\".format(hidden.shape)) \n",
    "        fc1 = self.fc1(hidden) \n",
    "        bn1 = self.batchnorm1(fc1) \n",
    "        fc2 = self.fc2(bn1)\n",
    "        bn2 = self.batchnorm2(fc2) \n",
    "        fc3 = self.activation(self.fc3(bn2)) \n",
    "        return fc3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT_LSTM(\n",
       "  (bert1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bert2): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lstm1): LSTM(768, 256, batch_first=True, bidirectional=True)\n",
       "  (lstm2): LSTM(768, 256, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (batchnorm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERT_LSTM() \n",
    "model.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([[0.4926],\n",
      "        [0.5562]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "device = torch.device('cuda')\n",
    "for batch in train_dataloader: \n",
    "    if cnt > 0: \n",
    "        break \n",
    "    batch = tuple(t.to(device) for t in batch) \n",
    "    \n",
    "    b_ids1, b_masks1, b_ids2, b_masks2, b_labels = batch  \n",
    "    outputs = model(ids1 = b_ids1, \n",
    "                    masks1 = b_masks1, \n",
    "                    ids2 = b_ids2,  \n",
    "                    masks2 = b_masks2) \n",
    "    \n",
    "    print(outputs)\n",
    "    \n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):  \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten() \n",
    "    labels_flat = labels.flatten() \n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat) \n",
    "\n",
    "def format_time(elapsed):  \n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping: \n",
    "    ''' if validation loss does not decrease anymore, we stop training '''\n",
    "    def __init__(self, patience, verbose, delta, path): \n",
    "        self.patience = patience \n",
    "        self.verbose = verbose \n",
    "        self.counter = 0 \n",
    "        self.best_score = None \n",
    "        self.early_stop = False \n",
    "        self.val_loss_min = np.Inf \n",
    "        self.delta = delta \n",
    "        self.path = path \n",
    "    \n",
    "    def __call__(self, val_loss, model): \n",
    "        score = -val_loss \n",
    "        if self.best_score is None:  \n",
    "            self.best_score = score \n",
    "            self.save_checkpoint(val_loss, model) \n",
    "        elif score < self.best_score + self.delta: \n",
    "            self.counter += 1 \n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience: \n",
    "                self.early_stop = True \n",
    "        else: \n",
    "            self.best_score = score \n",
    "            self.save_checkpoint(val_loss, model) \n",
    "            self.counter = 0 \n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model): \n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1 / 2 =====\n",
      "Training...\n",
      "  Batch    20  of    614.    Elapsed: 0:01:20.\n",
      "  current average loss = 0.7439177662134171\n",
      "  Batch    40  of    614.    Elapsed: 0:02:31.\n",
      "  current average loss = 0.7469433724880219\n",
      "  Batch    60  of    614.    Elapsed: 0:03:41.\n",
      "  current average loss = 0.7452892124652862\n",
      "  Batch    80  of    614.    Elapsed: 0:04:55.\n",
      "  current average loss = 0.7434857599437237\n",
      "  Batch   100  of    614.    Elapsed: 0:06:04.\n",
      "  current average loss = 0.739684824347496\n",
      "  Batch   120  of    614.    Elapsed: 0:07:13.\n",
      "  current average loss = 0.7373640323678653\n",
      "  Batch   140  of    614.    Elapsed: 0:08:22.\n",
      "  current average loss = 0.7356224200555257\n",
      "  Batch   160  of    614.    Elapsed: 0:09:38.\n",
      "  current average loss = 0.7345329008996486\n",
      "  Batch   180  of    614.    Elapsed: 0:10:52.\n",
      "  current average loss = 0.7349275218115913\n",
      "  Batch   200  of    614.    Elapsed: 0:12:15.\n",
      "  current average loss = 0.733034462928772\n",
      "  Batch   220  of    614.    Elapsed: 0:13:26.\n",
      "  current average loss = 0.7313368808139454\n",
      "  Batch   240  of    614.    Elapsed: 0:14:36.\n",
      "  current average loss = 0.7294267368813355\n",
      "  Batch   260  of    614.    Elapsed: 0:16:01.\n",
      "  current average loss = 0.7282351491542963\n",
      "  Batch   280  of    614.    Elapsed: 0:17:32.\n",
      "  current average loss = 0.7270108861582619\n",
      "  Batch   300  of    614.    Elapsed: 0:18:58.\n",
      "  current average loss = 0.7256037998199463\n",
      "  Batch   320  of    614.    Elapsed: 0:20:25.\n",
      "  current average loss = 0.7243734933435917\n",
      "  Batch   340  of    614.    Elapsed: 0:21:50.\n",
      "  current average loss = 0.7231458400978762\n",
      "  Batch   360  of    614.    Elapsed: 0:23:07.\n",
      "  current average loss = 0.7221870028310352\n",
      "  Batch   380  of    614.    Elapsed: 0:24:24.\n",
      "  current average loss = 0.7212956505386453\n",
      "  Batch   400  of    614.    Elapsed: 0:25:37.\n",
      "  current average loss = 0.720501424074173\n",
      "  Batch   420  of    614.    Elapsed: 0:26:49.\n",
      "  current average loss = 0.7196297076486405\n",
      "  Batch   440  of    614.    Elapsed: 0:28:10.\n",
      "  current average loss = 0.7187920247966593\n",
      "  Batch   460  of    614.    Elapsed: 0:29:29.\n",
      "  current average loss = 0.7179053136835928\n",
      "  Batch   480  of    614.    Elapsed: 0:30:43.\n",
      "  current average loss = 0.7172407707820336\n",
      "  Batch   500  of    614.    Elapsed: 0:31:59.\n",
      "  current average loss = 0.7165335173606873\n",
      "  Batch   520  of    614.    Elapsed: 0:33:25.\n",
      "  current average loss = 0.7157505632593082\n",
      "  Batch   540  of    614.    Elapsed: 0:34:51.\n",
      "  current average loss = 0.7150343876194071\n",
      "  Batch   560  of    614.    Elapsed: 0:36:20.\n",
      "  current average loss = 0.714403534574168\n",
      "  Batch   580  of    614.    Elapsed: 0:37:50.\n",
      "  current average loss = 0.713747058551887\n",
      "  Batch   600  of    614.    Elapsed: 0:39:16.\n",
      "  current average loss = 0.7131060583392779\n",
      "\n",
      " Average Training Loss: 0.7126694423562152\n",
      " Training epoch took: 0:40:16\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6816882059194039\n",
      "Validation loss decreased (inf --> 0.681688).  Saving model ...\n",
      "\n",
      "===== Epoch 2 / 2 =====\n",
      "Training...\n",
      "  Batch    20  of    614.    Elapsed: 0:01:26.\n",
      "  current average loss = 0.6928835153579712\n",
      "  Batch    40  of    614.    Elapsed: 0:02:58.\n",
      "  current average loss = 0.6926795348525048\n",
      "  Batch    60  of    614.    Elapsed: 0:04:22.\n",
      "  current average loss = 0.6925129900376003\n",
      "  Batch    80  of    614.    Elapsed: 0:05:48.\n",
      "  current average loss = 0.6921690449118614\n",
      "  Batch   100  of    614.    Elapsed: 0:07:14.\n",
      "  current average loss = 0.693065065741539\n",
      "  Batch   120  of    614.    Elapsed: 0:08:45.\n",
      "  current average loss = 0.692796541750431\n",
      "  Batch   140  of    614.    Elapsed: 0:10:15.\n",
      "  current average loss = 0.6924582856042044\n",
      "  Batch   160  of    614.    Elapsed: 0:11:42.\n",
      "  current average loss = 0.692145574092865\n",
      "  Batch   180  of    614.    Elapsed: 0:13:09.\n",
      "  current average loss = 0.691884391175376\n",
      "  Batch   200  of    614.    Elapsed: 0:14:38.\n",
      "  current average loss = 0.6923964643478393\n",
      "  Batch   220  of    614.    Elapsed: 0:16:03.\n",
      "  current average loss = 0.6928300234404478\n",
      "  Batch   240  of    614.    Elapsed: 0:17:29.\n",
      "  current average loss = 0.6926051979263623\n",
      "  Batch   260  of    614.    Elapsed: 0:18:55.\n",
      "  current average loss = 0.693372053366441\n",
      "  Batch   280  of    614.    Elapsed: 0:20:21.\n",
      "  current average loss = 0.6932801297732762\n",
      "  Batch   300  of    614.    Elapsed: 0:21:47.\n",
      "  current average loss = 0.6934893866380055\n",
      "  Batch   320  of    614.    Elapsed: 0:23:17.\n",
      "  current average loss = 0.6932658061385155\n",
      "  Batch   340  of    614.    Elapsed: 0:24:47.\n",
      "  current average loss = 0.6930616801275926\n",
      "  Batch   360  of    614.    Elapsed: 0:26:16.\n",
      "  current average loss = 0.6929672494530678\n",
      "  Batch   380  of    614.    Elapsed: 0:27:43.\n",
      "  current average loss = 0.692767177757464\n",
      "  Batch   400  of    614.    Elapsed: 0:29:11.\n",
      "  current average loss = 0.692669760286808\n",
      "  Batch   420  of    614.    Elapsed: 0:30:39.\n",
      "  current average loss = 0.692506378037589\n",
      "  Batch   440  of    614.    Elapsed: 0:32:07.\n",
      "  current average loss = 0.6923049128868363\n",
      "  Batch   460  of    614.    Elapsed: 0:33:32.\n",
      "  current average loss = 0.6920854067024977\n",
      "  Batch   480  of    614.    Elapsed: 0:34:59.\n",
      "  current average loss = 0.6918631734947364\n",
      "  Batch   500  of    614.    Elapsed: 0:36:28.\n",
      "  current average loss = 0.6916840574741363\n",
      "  Batch   520  of    614.    Elapsed: 0:37:43.\n",
      "  current average loss = 0.6915634171320841\n",
      "  Batch   540  of    614.    Elapsed: 0:38:56.\n",
      "  current average loss = 0.6915521512428919\n",
      "  Batch   560  of    614.    Elapsed: 0:40:09.\n",
      "  current average loss = 0.691406182625464\n",
      "  Batch   580  of    614.    Elapsed: 0:41:21.\n",
      "  current average loss = 0.691250584454372\n",
      "  Batch   600  of    614.    Elapsed: 0:42:32.\n",
      "  current average loss = 0.6910732146104177\n",
      "\n",
      " Average Training Loss: 0.6910157171639246\n",
      " Training epoch took: 0:43:27\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6448690398879673\n",
      "Validation loss decreased (0.681688 --> 0.644869).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs \n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "train_losses, val_losses = [], [] \n",
    "### binary crossentropy loss ### \n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "### early stopping ### \n",
    "early_stopping = EarlyStopping(patience = 3, verbose = True, delta=0, path=\"BERT_CLAIMS.pt\")\n",
    "tolerance = True \n",
    "\n",
    "### initialize gradient ###\n",
    "model.zero_grad() \n",
    "\n",
    "for epoch_i in range(0, epochs):  \n",
    "    ### Training ### \n",
    "    print(\"\")\n",
    "    print(\"===== Epoch {:} / {:} =====\".format(epoch_i + 1, epochs))\n",
    "    print(\"Training...\")\n",
    "    t0 = time.time() \n",
    "    total_loss = 0 \n",
    "    model.train() \n",
    "    for step, batch in enumerate(train_dataloader): \n",
    "        if step%20 == 0 and not step == 0: \n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print('  current average loss = {}'.format(total_loss / step))\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        \n",
    "        b_ids1, b_masks1, b_ids2, b_masks2, b_labels = batch \n",
    "        outputs = model(ids1 = b_ids1, \n",
    "                        masks1 = b_masks1, \n",
    "                        ids2 = b_ids2, \n",
    "                        masks2 = b_masks2) \n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs.float(), b_labels.float()) \n",
    "        total_loss += loss.item() \n",
    "        loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "        optimizer.step() \n",
    "        scheduler.step() \n",
    "        model.zero_grad() \n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader) \n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\" Average Training Loss: {}\".format(avg_train_loss)) \n",
    "    print(\" Training epoch took: {:}\".format(format_time(time.time() - t0))) \n",
    "    ### Validation ### \n",
    "    print(\"\")\n",
    "    print(\"Running Validation\") \n",
    "    t0 = time.time() \n",
    "    model.eval() \n",
    "    \n",
    "    eval_loss = 0 \n",
    "    \n",
    "    for batch in val_dataloader: \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        b_ids1, b_masks1, b_ids2, b_masks2, b_labels = batch \n",
    "        with torch.no_grad(): \n",
    "            outputs = model(ids1 = b_ids1, \n",
    "                            masks1 = b_masks1, \n",
    "                            ids2 = b_ids2, \n",
    "                            masks2 = b_masks2)\n",
    "        \n",
    "        loss = criterion(outputs.float(), b_labels.float())\n",
    "        eval_loss += loss.item() \n",
    "        \n",
    "    avg_val_loss = eval_loss / len(val_dataloader)  \n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(\" Average validation loss: {}\".format(avg_val_loss))  \n",
    "    if tolerance == True:  \n",
    "        early_stopping(avg_val_loss, model) \n",
    "    elif tolerance == False: \n",
    "        if avg_val_loss == np.min(val_losses): \n",
    "            print(\"saving best checkpoint after early stopping!\") \n",
    "            torch.save(model.state_dict(), \"BERT_CLAIMS_\" + str(epoch_i))\n",
    "    \n",
    "    if early_stopping.early_stop: \n",
    "        print(\"We are out of patience\") \n",
    "        tolerance = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
