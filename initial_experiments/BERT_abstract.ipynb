{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline with Bert  \n",
    "\n",
    "I will eventually use dense retreival based models. This is simply a baseline. \n",
    "\n",
    "This BERT model is used for processing the abstract information from patent documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.functional as f \n",
    "from torch.optim import AdamW \n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "from sklearn.model_selection import train_test_split \n",
    "import time \n",
    "import datetime \n",
    "import re \n",
    "import math \n",
    "import sklearn \n",
    "import os\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import BertModel, BertTokenizer, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sample files = 12508\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('sample_data2')\n",
    "print(\"number of sample files = {}\".format(len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['patent1id', 'patent2id', 'fullpatent1id', 'fullpatent2id', 'patent1abstract', 'patent2abstract', 'patent1claims', 'patent2claims'])\n"
     ]
    }
   ],
   "source": [
    "with open('sample_data2/'+files[0]) as f: \n",
    "    data = json.load(f) \n",
    "    print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files: \n",
    "    if file == 'labels.json': \n",
    "        with open('sample_data2/'+file) as f: \n",
    "            labels = json.load(f)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few problematic json files. For now let's only use samples that we can parse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_abstracts = [] \n",
    "train_labels = [] \n",
    "problematic = 0 \n",
    "for idx, file in enumerate(files): \n",
    "    filename = 'sample_data2/' + file\n",
    "    if '.json' not in filename or 'labels' in filename: \n",
    "        continue \n",
    "    try: \n",
    "        with open(filename) as f:  \n",
    "            data = json.load(f) \n",
    "            train_abstracts.append(data) \n",
    "            train_labels.append(labels[file])\n",
    "    except Exception as e: \n",
    "        problematic += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid json files = 11617\n",
      "Number of problematic json files = 889\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of valid json files = {}\".format(len(train_abstracts))) \n",
    "print(\"Number of problematic json files = {}\".format(problematic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to not using some json files, we have slightly more positive samples than negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11617"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones = 54.69570457088749%\n",
      "zeros = 45.30429542911251%\n"
     ]
    }
   ],
   "source": [
    "ones, zeros = 0, 0 \n",
    "for num in train_labels:  \n",
    "    ones += (num==1) \n",
    "    zeros += (num==0) \n",
    "    \n",
    "print(\"ones = {}%\".format(ones / len(train_labels)*100)) \n",
    "print(\"zeros = {}%\".format(zeros / len(train_labels) * 100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the features we have, we are going to use the patent1abstract, patent2abstract and patent1claims and patent2claims. \n",
    "\n",
    "We will build separate models for each pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['patent1id', 'patent2id', 'fullpatent1id', 'fullpatent2id', 'patent1abstract', 'patent2abstract', 'patent1claims', 'patent2claims'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_abstracts[0].keys() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "\n",
    "def BERT_Tokenizer(text, MAX_LEN=512): \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = text,  \n",
    "        add_special_tokens = True, \n",
    "        pad_to_max_length = False, \n",
    "        return_attention_mask = True \n",
    "    )\n",
    "    input_id = encoded_dict['input_ids'] \n",
    "    attention_mask = encoded_dict['attention_mask'] \n",
    "    \n",
    "    # get the first 512 tokens\n",
    "    text_len = len(input_id)\n",
    "    if len(input_id) > MAX_LEN: \n",
    "        input_id = input_id[:MAX_LEN] \n",
    "        attention_mask = attention_mask[:MAX_LEN] \n",
    "    else: \n",
    "        input_id = input_id + [0]*(MAX_LEN - len(input_id)) \n",
    "        attention_mask = attention_mask + [0]*(MAX_LEN - len(attention_mask)) \n",
    "    \n",
    "    return input_id, attention_mask, text_len \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 1907/11617 [00:14<01:17, 125.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 11617/11617 [01:20<00:00, 144.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# store tokenzied abstract and claims \n",
    "abstract1_input_ids, abstract2_input_ids = [], [] \n",
    "abstract1_attention_masks, abstract2_attention_masks = [], [] \n",
    "abstract1_lengths, abstract2_lengths = [], [] \n",
    "labels = []  \n",
    "\n",
    "for i in tqdm(range(len(train_abstracts)), position=0, leave=True): \n",
    "    a1 = train_abstracts[i]['patent1abstract'] \n",
    "    a2 = train_abstracts[i]['patent2abstract'] \n",
    "               \n",
    "    a1_input_id, a1_attention_mask, a1_length = BERT_Tokenizer(a1)\n",
    "    abstract1_input_ids.append(a1_input_id) \n",
    "    abstract1_attention_masks.append(a1_attention_mask)\n",
    "    abstract1_lengths.append(a1_length)   \n",
    "    \n",
    "    a2_input_id, a2_attention_mask, a2_length = BERT_Tokenizer(a2) \n",
    "    abstract2_input_ids.append(a2_input_id)\n",
    "    abstract2_attention_masks.append(a2_attention_mask) \n",
    "    abstract2_lengths.append(a2_length) \n",
    "     \n",
    "    labels.append(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stats(arr):\n",
    "    print(\"mean tokens = {}\".format(np.mean(arr))) \n",
    "    print(\"min tokens = {}\".format(np.min(arr))) \n",
    "    print(\"max tokens = {}\".format(np.max(arr))) \n",
    "    print(\"number of data with more than 512 tokens = {}\".format(np.sum(i > 512 for i in arr))) \n",
    "    print(\"percentage of data with more than 512 tokens = {:.2f}%\".format(np.sum(i > 512 for i in arr) * 100 / len(arr)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tokens = 134.10966686752172\n",
      "min tokens = 14\n",
      "max tokens = 445\n",
      "number of data with more than 512 tokens = 0\n",
      "percentage of data with more than 512 tokens = 0.00%\n",
      "\n",
      "mean tokens = 151.96479297581132\n",
      "min tokens = 9\n",
      "max tokens = 600\n",
      "number of data with more than 512 tokens = 5\n",
      "percentage of data with more than 512 tokens = 0.04%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "token_stats(abstract1_lengths) \n",
    "\n",
    "token_stats(abstract2_lengths) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11617, 512]),\n",
       " torch.Size([11617, 512]),\n",
       " torch.Size([11617, 512]),\n",
       " torch.Size([11617, 512]),\n",
       " torch.Size([11617, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract1_input_ids = torch.tensor(abstract1_input_ids, dtype=int)\n",
    "abstract1_attention_masks = torch.tensor(abstract1_attention_masks, dtype=int) \n",
    "\n",
    "abstract2_input_ids = torch.tensor(abstract2_input_ids, dtype=int)\n",
    "abstract2_attention_masks = torch.tensor(abstract2_attention_masks, dtype=int) \n",
    "\n",
    "labels = torch.tensor(labels, dtype=float)\n",
    "labels = torch.reshape(labels, (-1,1))\n",
    "\n",
    "abstract1_input_ids.shape, abstract1_attention_masks.shape, abstract2_input_ids.shape, abstract2_attention_masks.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Train dataset size===\n",
      "torch.Size([9293, 512])\n",
      "torch.Size([9293, 512])\n",
      "torch.Size([9293, 512])\n",
      "torch.Size([9293, 512])\n",
      "torch.Size([9293, 1])\n",
      "\n",
      "===Validation dataset size===\n",
      "torch.Size([1162, 512])\n",
      "torch.Size([1162, 512])\n",
      "torch.Size([1162, 512])\n",
      "torch.Size([1162, 512])\n",
      "torch.Size([1162, 1])\n",
      "\n",
      "===Test dataset size===\n",
      "torch.Size([1162, 512])\n",
      "torch.Size([1162, 512])\n",
      "torch.Size([1162, 512])\n",
      "torch.Size([1162, 512])\n",
      "torch.Size([1162, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### obtain train set ### \n",
    "train_abstract1_input_ids, other_abstract1_input_ids, train_abstract1_attention_masks, other_abstract1_attention_masks = train_test_split(abstract1_input_ids, \n",
    "                                                                                                                                          abstract1_attention_masks, \n",
    "                                                                                                                                          random_state = 888, \n",
    "                                                                                                                                          test_size = 0.2, \n",
    "                                                                                                                                          stratify = labels) \n",
    "\n",
    "train_abstract2_input_ids, other_abstract2_input_ids, train_abstract2_attention_masks, other_abstract2_attention_masks = train_test_split(abstract2_input_ids,\n",
    "                                                                                                                                          abstract2_attention_masks, \n",
    "                                                                                                                                          random_state = 888, \n",
    "                                                                                                                                          test_size = 0.2, \n",
    "                                                                                                                                          stratify = labels)\n",
    "\n",
    "\n",
    "\n",
    "_, _, train_labels, other_labels = train_test_split(abstract1_input_ids, \n",
    "                                                    labels, \n",
    "                                                    random_state = 888, \n",
    "                                                    test_size = 0.2,\n",
    "                                                    stratify = labels)\n",
    "\n",
    "### obtain validation and test set ### \n",
    "val_abstract1_input_ids, test_abstract1_input_ids, val_abstract1_attention_masks, test_abstract1_attention_masks = train_test_split(other_abstract1_input_ids, \n",
    "                                                                                                                                    other_abstract1_attention_masks, \n",
    "                                                                                                                                    random_state = 999, \n",
    "                                                                                                                                    test_size = 0.5, \n",
    "                                                                                                                                    stratify = other_labels)  \n",
    "\n",
    "val_abstract2_input_ids, test_abstract2_input_ids, val_abstract2_attention_masks, test_abstract2_attention_masks = train_test_split(other_abstract2_input_ids, \n",
    "                                                                                                                                    other_abstract2_attention_masks, \n",
    "                                                                                                                                    random_state = 999, \n",
    "                                                                                                                                    test_size = 0.5, \n",
    "                                                                                                                                    stratify = other_labels)\n",
    "\n",
    "_, _, val_labels, test_labels = train_test_split(other_abstract1_input_ids, \n",
    "                                                 other_labels, \n",
    "                                                 random_state = 999, \n",
    "                                                 test_size = 0.5, \n",
    "                                                 stratify = other_labels)\n",
    "\n",
    "print(\"===Train dataset size===\") \n",
    "print(train_abstract1_input_ids.shape) \n",
    "print(train_abstract1_attention_masks.shape)\n",
    "print(train_abstract2_input_ids.shape) \n",
    "print(train_abstract2_attention_masks.shape) \n",
    "print(train_labels.shape) \n",
    "print() \n",
    "print(\"===Validation dataset size===\")\n",
    "print(val_abstract1_input_ids.shape) \n",
    "print(val_abstract1_attention_masks.shape) \n",
    "print(val_abstract2_input_ids.shape) \n",
    "print(val_abstract2_attention_masks.shape) \n",
    "print(val_labels.shape)\n",
    "print() \n",
    "print(\"===Test dataset size===\") \n",
    "print(test_abstract1_input_ids.shape) \n",
    "print(test_abstract1_attention_masks.shape) \n",
    "print(test_abstract2_input_ids.shape) \n",
    "print(test_abstract2_attention_masks.shape) \n",
    "print(test_labels.shape) \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_data = TensorDataset(train_abstract1_input_ids, \n",
    "                           train_abstract1_attention_masks, \n",
    "                           train_abstract2_input_ids,  \n",
    "                           train_abstract2_attention_masks, \n",
    "                           train_labels) \n",
    "\n",
    "train_sampler = RandomSampler(train_data) \n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = TensorDataset(val_abstract1_input_ids, \n",
    "                                val_abstract1_attention_masks, \n",
    "                                val_abstract2_input_ids, \n",
    "                                val_abstract2_attention_masks, \n",
    "                                val_labels) \n",
    "\n",
    "val_sampler = SequentialSampler(validation_data)\n",
    "\n",
    "val_dataloader = DataLoader(validation_data, sampler = val_sampler, batch_size = batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_abstract1_input_ids, \n",
    "                          test_abstract1_attention_masks, \n",
    "                          test_abstract2_input_ids, \n",
    "                          test_abstract2_attention_masks, \n",
    "                          test_labels) \n",
    "\n",
    "test_sampler = SequentialSampler(test_data) \n",
    "\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model that takes in abstract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_ABSTRACT(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(BERT_ABSTRACT, self).__init__()\n",
    "        self.bert1 = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "        self.bert2 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.fc1 = nn.Linear(768*2, 768) \n",
    "        self.batchnorm1 = nn.BatchNorm1d(768)\n",
    "        self.fc2 = nn.Linear(768, 256) \n",
    "        self.batchnorm2 = nn.BatchNorm1d(256) \n",
    "        self.fc3 = nn.Linear(256, 1) \n",
    "        self.activation = nn.Sigmoid() \n",
    "    \n",
    "    def forward(self, ids1, masks1, ids2, masks2): \n",
    "        outputs1 = self.bert1(input_ids = ids1, \n",
    "                              attention_mask = masks1) \n",
    "        pooler1 = outputs1.pooler_output \n",
    "        \n",
    "        outputs2 = self.bert2(input_ids = ids2, \n",
    "                              attention_mask = masks2) \n",
    "        pooler2 = outputs2.pooler_output \n",
    "                \n",
    "        x = torch.cat((pooler1, pooler2), 1) \n",
    "        fc1 = self.fc1(x) \n",
    "        bn1 = self.batchnorm1(fc1) \n",
    "        fc2 = self.fc2(bn1) \n",
    "        bn2 = self.batchnorm2(fc2) \n",
    "        fc3 = self.activation(self.fc3(bn2)) \n",
    "        return fc3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT_ABSTRACT(\n",
       "  (bert1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bert2): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (batchnorm1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (batchnorm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERT_ABSTRACT() \n",
    "\n",
    "model.cuda() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels): \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten() \n",
    "    labels_flat = labels.flatten() \n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed): \n",
    "    elapsed_rounded = int(round(elapsed)) \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping: \n",
    "    ''' if validation loss does not decrease anymore, we stop training '''\n",
    "    def __init__(self, patience, verbose, delta, path): \n",
    "        self.patience = patience \n",
    "        self.verbose = verbose \n",
    "        self.counter = 0 \n",
    "        self.best_score = None \n",
    "        self.early_stop = False \n",
    "        self.val_loss_min = np.Inf \n",
    "        self.delta = delta \n",
    "        self.path = path \n",
    "    \n",
    "    def __call__(self, val_loss, model): \n",
    "        score = -val_loss \n",
    "        if self.best_score is None:  \n",
    "            self.best_score = score \n",
    "            self.save_checkpoint(val_loss, model) \n",
    "        elif score < self.best_score + self.delta: \n",
    "            self.counter += 1 \n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience: \n",
    "                self.early_stop = True \n",
    "        else: \n",
    "            self.best_score = score \n",
    "            self.save_checkpoint(val_loss, model) \n",
    "            self.counter = 0 \n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model): \n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.7262326896190643\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.666527958959341\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.5981549933552742\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:48.\n",
      "  current average loss = 0.562524575414136\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:59.\n",
      "  current average loss = 0.5681340754404665\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:11.\n",
      "  current average loss = 0.531388616344581\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:23.\n",
      "  current average loss = 0.49414504298142026\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:35.\n",
      "  current average loss = 0.4639268481754698\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:47.\n",
      "  current average loss = 0.4677402284099824\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:59.\n",
      "  current average loss = 0.46610093872062863\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:11.\n",
      "  current average loss = 0.43912386034869333\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:23.\n",
      "  current average loss = 0.4290471513716814\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:34.\n",
      "  current average loss = 0.41289606843537724\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:46.\n",
      "  current average loss = 0.40832169315544886\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:59.\n",
      "  current average loss = 0.40130752239221085\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:11.\n",
      "  current average loss = 0.3933291587192798\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:22.\n",
      "  current average loss = 0.37621326523915155\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:34.\n",
      "  current average loss = 0.3621016437750465\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:46.\n",
      "  current average loss = 0.3518722672655713\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:58.\n",
      "  current average loss = 0.35085728887352163\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:10.\n",
      "  current average loss = 0.3447210545152692\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:21.\n",
      "  current average loss = 0.3369720824277134\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:33.\n",
      "  current average loss = 0.3252626670732001\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:45.\n",
      "  current average loss = 0.3221973324310966\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:57.\n",
      "  current average loss = 0.31283707479783335\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:08.\n",
      "  current average loss = 0.3120729444351477\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:20.\n",
      "  current average loss = 0.3032713474172245\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:31.\n",
      "  current average loss = 0.30620714926626535\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:43.\n",
      "  current average loss = 0.30157577369958244\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:54.\n",
      "  current average loss = 0.2956542585392405\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:06.\n",
      "  current average loss = 0.29057551672458726\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:17.\n",
      "  current average loss = 0.2832465226309068\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:29.\n",
      "  current average loss = 0.27672049787616937\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:41.\n",
      "  current average loss = 0.27573484490062317\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:52.\n",
      "  current average loss = 0.27119822520288706\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:04.\n",
      "  current average loss = 0.2647938121541099\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:16.\n",
      "  current average loss = 0.26313571538532315\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:27.\n",
      "  current average loss = 0.2636587976381538\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:39.\n",
      "  current average loss = 0.2589828461092204\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:51.\n",
      "  current average loss = 0.2533838814611227\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:02.\n",
      "  current average loss = 0.25268450773650464\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:14.\n",
      "  current average loss = 0.24788572713614918\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:26.\n",
      "  current average loss = 0.2431585161634017\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:37.\n",
      "  current average loss = 0.24115879244048052\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:49.\n",
      "  current average loss = 0.23714781930653972\n",
      "  Batch   920  of  1,162.    Elapsed: 0:09:00.\n",
      "  current average loss = 0.23605266208067643\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:12.\n",
      "  current average loss = 0.23327539431546107\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:24.\n",
      "  current average loss = 0.23011626435960958\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:35.\n",
      "  current average loss = 0.2300862618275367\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:47.\n",
      "  current average loss = 0.22839345187376603\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:09:59.\n",
      "  current average loss = 0.2259100755720283\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:11.\n",
      "  current average loss = 0.2237485944902381\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:22.\n",
      "  current average loss = 0.22028989332526558\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:34.\n",
      "  current average loss = 0.21725606586742624\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:46.\n",
      "  current average loss = 0.21549031616490208\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:58.\n",
      "  current average loss = 0.21508541710546913\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:10.\n",
      "  current average loss = 0.21416547153583676\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:21.\n",
      "  current average loss = 0.21318145438595695\n",
      "\n",
      " Average Training Loss: 0.21281677787184652\n",
      " Training epoch took: 0:11:22\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.05166765551636161\n",
      "Validation loss decreased (inf --> 0.051668).  Saving model ...\n",
      "\n",
      "===== Epoch 2 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.035964097260148264\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.02797859927377431\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.029799089538573752\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.02569323501775216\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:59.\n",
      "  current average loss = 0.03905911600071704\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.044252513660224695\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.054709633397370845\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:34.\n",
      "  current average loss = 0.05418567772903771\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:46.\n",
      "  current average loss = 0.0640506978172602\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:57.\n",
      "  current average loss = 0.06418571290065302\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:09.\n",
      "  current average loss = 0.06530316634757169\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:21.\n",
      "  current average loss = 0.06393886511480863\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:32.\n",
      "  current average loss = 0.06244355234599565\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:44.\n",
      "  current average loss = 0.0631359159386193\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:56.\n",
      "  current average loss = 0.06370152609742945\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:07.\n",
      "  current average loss = 0.060906095006157554\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:19.\n",
      "  current average loss = 0.07842459365612527\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:31.\n",
      "  current average loss = 0.0835334375875795\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:42.\n",
      "  current average loss = 0.083286508645075\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:54.\n",
      "  current average loss = 0.08365082758522475\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:06.\n",
      "  current average loss = 0.08017017520594111\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:17.\n",
      "  current average loss = 0.08223210794240003\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:29.\n",
      "  current average loss = 0.0791030830520195\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:40.\n",
      "  current average loss = 0.07612015759023052\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:52.\n",
      "  current average loss = 0.07311907881966909\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:04.\n",
      "  current average loss = 0.0719084617136944\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:15.\n",
      "  current average loss = 0.0693719130180802\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:27.\n",
      "  current average loss = 0.06893131037153839\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:39.\n",
      "  current average loss = 0.06930068300503552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   600  of  1,162.    Elapsed: 0:05:51.\n",
      "  current average loss = 0.06964760588331652\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:02.\n",
      "  current average loss = 0.06749831867719261\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:14.\n",
      "  current average loss = 0.06548746799232959\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:26.\n",
      "  current average loss = 0.06526498341977871\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:37.\n",
      "  current average loss = 0.06483544745890814\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:49.\n",
      "  current average loss = 0.06323480840219417\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:00.\n",
      "  current average loss = 0.06165212252013185\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:12.\n",
      "  current average loss = 0.06407891766988591\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:24.\n",
      "  current average loss = 0.06254369773692145\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:35.\n",
      "  current average loss = 0.06298933324171976\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:47.\n",
      "  current average loss = 0.0630273288729586\n",
      "  Batch   820  of  1,162.    Elapsed: 0:07:59.\n",
      "  current average loss = 0.06267548097464719\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:10.\n",
      "  current average loss = 0.06484402633155004\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:22.\n",
      "  current average loss = 0.06344415165260935\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:33.\n",
      "  current average loss = 0.06218799687176215\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:45.\n",
      "  current average loss = 0.06092283169236099\n",
      "  Batch   920  of  1,162.    Elapsed: 0:08:56.\n",
      "  current average loss = 0.06055723080674437\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:08.\n",
      "  current average loss = 0.060249952706037724\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:20.\n",
      "  current average loss = 0.05995014361698547\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:31.\n",
      "  current average loss = 0.05882628489189486\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:43.\n",
      "  current average loss = 0.05774624764551117\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:09:54.\n",
      "  current average loss = 0.057274739921986645\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:06.\n",
      "  current average loss = 0.057051647373219355\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:18.\n",
      "  current average loss = 0.056068506608535934\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:29.\n",
      "  current average loss = 0.05622156753264857\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:41.\n",
      "  current average loss = 0.05852805735170312\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:52.\n",
      "  current average loss = 0.05823762666373763\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:04.\n",
      "  current average loss = 0.05730407364771644\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:15.\n",
      "  current average loss = 0.057013259391219595\n",
      "\n",
      " Average Training Loss: 0.05691654429688203\n",
      " Training epoch took: 0:11:16\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.0476502428562476\n",
      "Validation loss decreased (0.051668 --> 0.047650).  Saving model ...\n",
      "\n",
      "===== Epoch 3 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.002012192660185974\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.034493603684313714\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.04099861719199301\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.03215814706118181\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:58.\n",
      "  current average loss = 0.034444708841183454\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.028881254404526165\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.02567113162331225\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:33.\n",
      "  current average loss = 0.022852314049259803\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:45.\n",
      "  current average loss = 0.020460212176840287\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:57.\n",
      "  current average loss = 0.018788500830414705\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:08.\n",
      "  current average loss = 0.022353775945918062\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:20.\n",
      "  current average loss = 0.02070997046572908\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:32.\n",
      "  current average loss = 0.023015822524254104\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:45.\n",
      "  current average loss = 0.0292796556308271\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:57.\n",
      "  current average loss = 0.03052616744224603\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:09.\n",
      "  current average loss = 0.03147317608004414\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:21.\n",
      "  current average loss = 0.034320018302027806\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:32.\n",
      "  current average loss = 0.03443986119026603\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:44.\n",
      "  current average loss = 0.036893529194965496\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:55.\n",
      "  current average loss = 0.03734848229891213\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:07.\n",
      "  current average loss = 0.035735062674540956\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:19.\n",
      "  current average loss = 0.03658486546653215\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:31.\n",
      "  current average loss = 0.039601001307212\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:42.\n",
      "  current average loss = 0.038092125283355927\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:54.\n",
      "  current average loss = 0.036657280267347235\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:06.\n",
      "  current average loss = 0.03539924392880493\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:17.\n",
      "  current average loss = 0.03596886370069115\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:29.\n",
      "  current average loss = 0.03485152966775006\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:41.\n",
      "  current average loss = 0.03662936444406083\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:52.\n",
      "  current average loss = 0.03550066095385167\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:04.\n",
      "  current average loss = 0.03871889966107002\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:16.\n",
      "  current average loss = 0.03993997595392784\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:27.\n",
      "  current average loss = 0.04055605541420157\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:39.\n",
      "  current average loss = 0.039821236740360345\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:51.\n",
      "  current average loss = 0.0399356475112914\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:02.\n",
      "  current average loss = 0.040111792570456196\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:14.\n",
      "  current average loss = 0.0402013315581457\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:26.\n",
      "  current average loss = 0.04157023570249198\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:37.\n",
      "  current average loss = 0.041736399723110176\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:49.\n",
      "  current average loss = 0.040776365671395066\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:00.\n",
      "  current average loss = 0.05511745284708989\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:12.\n",
      "  current average loss = 0.0538806392838307\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:23.\n",
      "  current average loss = 0.0527179603344552\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:35.\n",
      "  current average loss = 0.05191535270520366\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:47.\n",
      "  current average loss = 0.05355153440311066\n",
      "  Batch   920  of  1,162.    Elapsed: 0:08:58.\n",
      "  current average loss = 0.05239923430546459\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:10.\n",
      "  current average loss = 0.05143120622328075\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:21.\n",
      "  current average loss = 0.05231749590192294\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:33.\n",
      "  current average loss = 0.055221312152151564\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:45.\n",
      "  current average loss = 0.054612053111912244\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:09:56.\n",
      "  current average loss = 0.053575737974500166\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:08.\n",
      "  current average loss = 0.05265473488726247\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:20.\n",
      "  current average loss = 0.05171412720022279\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:32.\n",
      "  current average loss = 0.05080085713608679\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:44.\n",
      "  current average loss = 0.051886552093284\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:55.\n",
      "  current average loss = 0.05100761546697023\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:07.\n",
      "  current average loss = 0.05015443206140621\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:19.\n",
      "  current average loss = 0.049355802485124826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average Training Loss: 0.04927102315934506\n",
      " Training epoch took: 0:11:20\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.030593444885534938\n",
      "Validation loss decreased (0.047650 --> 0.030593).  Saving model ...\n",
      "\n",
      "===== Epoch 4 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.002363210971634544\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.0014687390937069722\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.019924480448692824\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.015353811389513795\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:58.\n",
      "  current average loss = 0.012976176631673298\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.010881660276603118\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.009496695172934519\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:33.\n",
      "  current average loss = 0.008401083638068485\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:45.\n",
      "  current average loss = 0.007639188697107279\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:57.\n",
      "  current average loss = 0.006926867798938474\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:08.\n",
      "  current average loss = 0.00649016858315008\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:20.\n",
      "  current average loss = 0.011656145305096288\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:31.\n",
      "  current average loss = 0.0140139329293351\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:43.\n",
      "  current average loss = 0.013231342964406525\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:55.\n",
      "  current average loss = 0.012434209748304663\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:06.\n",
      "  current average loss = 0.01826488056403832\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:18.\n",
      "  current average loss = 0.01727747807446362\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:30.\n",
      "  current average loss = 0.01640600999883443\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:42.\n",
      "  current average loss = 0.015653090347031568\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:53.\n",
      "  current average loss = 0.014946674975030874\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:05.\n",
      "  current average loss = 0.014352131753228258\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:17.\n",
      "  current average loss = 0.013756454123834125\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:28.\n",
      "  current average loss = 0.013329900832935022\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:40.\n",
      "  current average loss = 0.017569441087910795\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:52.\n",
      "  current average loss = 0.018327945207842276\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:05.\n",
      "  current average loss = 0.017742657197941998\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:17.\n",
      "  current average loss = 0.01714083666371085\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:30.\n",
      "  current average loss = 0.01654695086893168\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:42.\n",
      "  current average loss = 0.01602530623495113\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:54.\n",
      "  current average loss = 0.017117292818617593\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:06.\n",
      "  current average loss = 0.016588542855104957\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:17.\n",
      "  current average loss = 0.01773234315195964\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:30.\n",
      "  current average loss = 0.01778218086933934\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:41.\n",
      "  current average loss = 0.017332534379837693\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:53.\n",
      "  current average loss = 0.016900451902420174\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:05.\n",
      "  current average loss = 0.024643021768828374\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:16.\n",
      "  current average loss = 0.024421139945718245\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:28.\n",
      "  current average loss = 0.023792186335913356\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:40.\n",
      "  current average loss = 0.023196144726234005\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:52.\n",
      "  current average loss = 0.02322664212694235\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:03.\n",
      "  current average loss = 0.022706435997710418\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:15.\n",
      "  current average loss = 0.022193009266376455\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:27.\n",
      "  current average loss = 0.026081676809424734\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:38.\n",
      "  current average loss = 0.02559382608293907\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:50.\n",
      "  current average loss = 0.025085560405148297\n",
      "  Batch   920  of  1,162.    Elapsed: 0:09:02.\n",
      "  current average loss = 0.024603063452996534\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:14.\n",
      "  current average loss = 0.0241218145386967\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:25.\n",
      "  current average loss = 0.02551621269866852\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:37.\n",
      "  current average loss = 0.02609906731863053\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:49.\n",
      "  current average loss = 0.026314437577277205\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:10:01.\n",
      "  current average loss = 0.02689305280901845\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:13.\n",
      "  current average loss = 0.027414106181360975\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:25.\n",
      "  current average loss = 0.026906511601068044\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:37.\n",
      "  current average loss = 0.028222521369555883\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:48.\n",
      "  current average loss = 0.027749591695760915\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:11:00.\n",
      "  current average loss = 0.029428103195498677\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:12.\n",
      "  current average loss = 0.031177564144119266\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:23.\n",
      "  current average loss = 0.03408404478642798\n",
      "\n",
      " Average Training Loss: 0.03475253378308076\n",
      " Training epoch took: 0:11:24\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.08369415163537743\n",
      "EarlyStopping counter: 1 out of 5\n",
      "\n",
      "===== Epoch 5 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.06189923981291941\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.05003864580762638\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.03354419272491214\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.027138636769473125\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:59.\n",
      "  current average loss = 0.022184389142494182\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.027617945053195096\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.035579200876041536\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:34.\n",
      "  current average loss = 0.03120618937590507\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:45.\n",
      "  current average loss = 0.03140248869445309\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:58.\n",
      "  current average loss = 0.035167392461780765\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:10.\n",
      "  current average loss = 0.03521877820954945\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:23.\n",
      "  current average loss = 0.036364326599724946\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:34.\n",
      "  current average loss = 0.036593717583995246\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:46.\n",
      "  current average loss = 0.03400881989772253\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:58.\n",
      "  current average loss = 0.03181440596916824\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:10.\n",
      "  current average loss = 0.03519843187958145\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:21.\n",
      "  current average loss = 0.03321614449976765\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:33.\n",
      "  current average loss = 0.03142808999772468\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:45.\n",
      "  current average loss = 0.029895823022865156\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:57.\n",
      "  current average loss = 0.0304690920405983\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:08.\n",
      "  current average loss = 0.02965561227708479\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:20.\n",
      "  current average loss = 0.0284066844468808\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:32.\n",
      "  current average loss = 0.032980441552247254\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:43.\n",
      "  current average loss = 0.03165491296607949\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:55.\n",
      "  current average loss = 0.03997916890937631\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:07.\n",
      "  current average loss = 0.03848083915669262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   540  of  1,162.    Elapsed: 0:05:18.\n",
      "  current average loss = 0.03714387672528203\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:30.\n",
      "  current average loss = 0.03587670198070165\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:41.\n",
      "  current average loss = 0.035992713647600136\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:53.\n",
      "  current average loss = 0.03486601804560754\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:05.\n",
      "  current average loss = 0.03692736558259639\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:16.\n",
      "  current average loss = 0.03635582602938996\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:28.\n",
      "  current average loss = 0.03534444551561054\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:40.\n",
      "  current average loss = 0.03434967876499767\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:51.\n",
      "  current average loss = 0.033432409936867774\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:03.\n",
      "  current average loss = 0.03255331281458464\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:15.\n",
      "  current average loss = 0.0318331809331232\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:27.\n",
      "  current average loss = 0.031038474000254276\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:38.\n",
      "  current average loss = 0.031203401951912523\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:50.\n",
      "  current average loss = 0.030446103811909778\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:02.\n",
      "  current average loss = 0.029714570270649658\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:13.\n",
      "  current average loss = 0.030542694622304873\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:25.\n",
      "  current average loss = 0.029856603928043155\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:37.\n",
      "  current average loss = 0.029810975610126373\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:49.\n",
      "  current average loss = 0.029153964510385575\n",
      "  Batch   920  of  1,162.    Elapsed: 0:09:00.\n",
      "  current average loss = 0.029862234737667043\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:12.\n",
      "  current average loss = 0.029276513660632653\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:24.\n",
      "  current average loss = 0.028682853628709874\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:36.\n",
      "  current average loss = 0.028815516224258132\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:48.\n",
      "  current average loss = 0.028246798649788615\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:09:59.\n",
      "  current average loss = 0.028730258296445755\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:11.\n",
      "  current average loss = 0.028922999941211787\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:23.\n",
      "  current average loss = 0.02838489687135494\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:34.\n",
      "  current average loss = 0.02789014804440268\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:46.\n",
      "  current average loss = 0.027419891645462154\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:58.\n",
      "  current average loss = 0.026957620984284503\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:10.\n",
      "  current average loss = 0.02716065661533752\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:21.\n",
      "  current average loss = 0.026735657782188386\n",
      "\n",
      " Average Training Loss: 0.026689780398861777\n",
      " Training epoch took: 0:11:22\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.029471783343841994\n",
      "Validation loss decreased (0.030593 --> 0.029472).  Saving model ...\n",
      "\n",
      "===== Epoch 6 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.0013011610224111791\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.045409542678612524\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.030760419210097704\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.023256607227006044\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:59.\n",
      "  current average loss = 0.03333800407168383\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.03422672068155255\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.029853590709931918\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:34.\n",
      "  current average loss = 0.026158789430081698\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:45.\n",
      "  current average loss = 0.02340040491174275\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:57.\n",
      "  current average loss = 0.02120634350872024\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:09.\n",
      "  current average loss = 0.019488029709211995\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:20.\n",
      "  current average loss = 0.017892836399933762\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:32.\n",
      "  current average loss = 0.01657500881179448\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:44.\n",
      "  current average loss = 0.01869946901599308\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:56.\n",
      "  current average loss = 0.023247347976882035\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:07.\n",
      "  current average loss = 0.021875784529797215\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:19.\n",
      "  current average loss = 0.02284161423933309\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:31.\n",
      "  current average loss = 0.02162171050216936\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:42.\n",
      "  current average loss = 0.020521079588558533\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:54.\n",
      "  current average loss = 0.01954959812072275\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:06.\n",
      "  current average loss = 0.02199325975104676\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:17.\n",
      "  current average loss = 0.023591059405616347\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:29.\n",
      "  current average loss = 0.022597820192148625\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:41.\n",
      "  current average loss = 0.021725501546855715\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:53.\n",
      "  current average loss = 0.023808623248765797\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:04.\n",
      "  current average loss = 0.022960294936414216\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:16.\n",
      "  current average loss = 0.022119171320905986\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:28.\n",
      "  current average loss = 0.02137648140037527\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:40.\n",
      "  current average loss = 0.020684243519814304\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:52.\n",
      "  current average loss = 0.020038186471671604\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:03.\n",
      "  current average loss = 0.021003144165196053\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:15.\n",
      "  current average loss = 0.02040375231892\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:27.\n",
      "  current average loss = 0.019791130100922986\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:38.\n",
      "  current average loss = 0.020225599249075084\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:50.\n",
      "  current average loss = 0.019675896250321654\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:02.\n",
      "  current average loss = 0.02025098942196261\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:13.\n",
      "  current average loss = 0.01971388477919833\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:25.\n",
      "  current average loss = 0.020958854474546523\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:37.\n",
      "  current average loss = 0.022105974214168666\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:48.\n",
      "  current average loss = 0.021609215032395924\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:00.\n",
      "  current average loss = 0.021102239151188282\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:11.\n",
      "  current average loss = 0.02224824379549903\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:23.\n",
      "  current average loss = 0.022864094298915387\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:34.\n",
      "  current average loss = 0.02502646876475141\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:46.\n",
      "  current average loss = 0.02535318519988727\n",
      "  Batch   920  of  1,162.    Elapsed: 0:08:58.\n",
      "  current average loss = 0.02767498799888254\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:09.\n",
      "  current average loss = 0.02709366717948255\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:21.\n",
      "  current average loss = 0.027887261624111186\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:32.\n",
      "  current average loss = 0.028978584014679173\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:44.\n",
      "  current average loss = 0.030001100454282094\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:09:56.\n",
      "  current average loss = 0.029528196959367335\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:07.\n",
      "  current average loss = 0.030357654868504808\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:19.\n",
      "  current average loss = 0.03171929928963385\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:30.\n",
      "  current average loss = 0.03195172263155941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:42.\n",
      "  current average loss = 0.03285400705019542\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:54.\n",
      "  current average loss = 0.03227090139710234\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:05.\n",
      "  current average loss = 0.03173501910487335\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:17.\n",
      "  current average loss = 0.031215887022175717\n",
      "\n",
      " Average Training Loss: 0.031162212660901648\n",
      " Training epoch took: 0:11:18\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.03269706651796185\n",
      "EarlyStopping counter: 1 out of 5\n",
      "\n",
      "===== Epoch 7 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.052518612281028255\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.026965064010528294\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.01818835353239289\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:48.\n",
      "  current average loss = 0.044765587386405056\n",
      "  Batch   100  of  1,162.    Elapsed: 0:01:00.\n",
      "  current average loss = 0.05261268277237832\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:12.\n",
      "  current average loss = 0.04396583199797609\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:24.\n",
      "  current average loss = 0.03777594186253737\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:36.\n",
      "  current average loss = 0.043615604600620375\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:47.\n",
      "  current average loss = 0.03885009402789213\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:59.\n",
      "  current average loss = 0.03499885657603272\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:11.\n",
      "  current average loss = 0.03187139057015429\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:23.\n",
      "  current average loss = 0.02927351846595722\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:34.\n",
      "  current average loss = 0.02975435172091308\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:46.\n",
      "  current average loss = 0.027643285350399893\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:58.\n",
      "  current average loss = 0.02591416180588567\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:09.\n",
      "  current average loss = 0.024339144501956865\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:21.\n",
      "  current average loss = 0.022941915580046202\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:32.\n",
      "  current average loss = 0.021703100567482477\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:44.\n",
      "  current average loss = 0.020585048004895006\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:56.\n",
      "  current average loss = 0.019611999260878293\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:07.\n",
      "  current average loss = 0.01873326852935623\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:19.\n",
      "  current average loss = 0.01795427011400286\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:31.\n",
      "  current average loss = 0.01717972586750717\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:42.\n",
      "  current average loss = 0.016472469380269673\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:54.\n",
      "  current average loss = 0.015837213688772864\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:05.\n",
      "  current average loss = 0.015270901267819261\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:17.\n",
      "  current average loss = 0.014713261713040993\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:29.\n",
      "  current average loss = 0.01419382412423959\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:41.\n",
      "  current average loss = 0.0157032767894642\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:52.\n",
      "  current average loss = 0.017980356624914293\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:04.\n",
      "  current average loss = 0.020777321466733067\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:16.\n",
      "  current average loss = 0.021452478739561798\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:27.\n",
      "  current average loss = 0.02204711347671326\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:39.\n",
      "  current average loss = 0.02142218896236192\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:51.\n",
      "  current average loss = 0.02084015824391827\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:02.\n",
      "  current average loss = 0.020268001529936127\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:14.\n",
      "  current average loss = 0.01973590223671476\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:26.\n",
      "  current average loss = 0.019246636696388913\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:38.\n",
      "  current average loss = 0.021074087462397983\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:49.\n",
      "  current average loss = 0.020604701083349254\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:01.\n",
      "  current average loss = 0.020842515092247195\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:12.\n",
      "  current average loss = 0.02037388398559599\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:24.\n",
      "  current average loss = 0.01990349831271117\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:36.\n",
      "  current average loss = 0.019496081384177574\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:47.\n",
      "  current average loss = 0.021195330234088233\n",
      "  Batch   920  of  1,162.    Elapsed: 0:08:59.\n",
      "  current average loss = 0.021521530617357475\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:10.\n",
      "  current average loss = 0.021099942472011073\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:22.\n",
      "  current average loss = 0.022115829206604095\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:34.\n",
      "  current average loss = 0.021668296799259912\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:45.\n",
      "  current average loss = 0.021259788925867724\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:09:57.\n",
      "  current average loss = 0.020870536888340514\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:09.\n",
      "  current average loss = 0.02047326344486814\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:20.\n",
      "  current average loss = 0.02013326225373158\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:32.\n",
      "  current average loss = 0.019779145650089605\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:44.\n",
      "  current average loss = 0.019544234780174852\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:55.\n",
      "  current average loss = 0.019211976941564655\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:07.\n",
      "  current average loss = 0.019488587515094656\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:18.\n",
      "  current average loss = 0.01916066875435145\n",
      "\n",
      " Average Training Loss: 0.019127740751489222\n",
      " Training epoch took: 0:11:19\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.0036573021530622936\n",
      "Validation loss decreased (0.029472 --> 0.003657).  Saving model ...\n",
      "\n",
      "===== Epoch 8 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.0008905866859777234\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.0007339299208865668\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.0007704085124563184\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:46.\n",
      "  current average loss = 0.0006976921947511982\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:58.\n",
      "  current average loss = 0.00837817456985249\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.007156785391115742\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:21.\n",
      "  current average loss = 0.006173747832683927\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:33.\n",
      "  current average loss = 0.005463818405900156\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:45.\n",
      "  current average loss = 0.004947166440016796\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:57.\n",
      "  current average loss = 0.00450127667048946\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:09.\n",
      "  current average loss = 0.004110283041588694\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:21.\n",
      "  current average loss = 0.003774262636234956\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:32.\n",
      "  current average loss = 0.0037008656579770945\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:44.\n",
      "  current average loss = 0.0035372877819668247\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:56.\n",
      "  current average loss = 0.00690877322279448\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:07.\n",
      "  current average loss = 0.009833194826340019\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:19.\n",
      "  current average loss = 0.009314439281678567\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:31.\n",
      "  current average loss = 0.008834202296256737\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:43.\n",
      "  current average loss = 0.008394548271477512\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:54.\n",
      "  current average loss = 0.008012860386115222\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:06.\n",
      "  current average loss = 0.007635778610166913\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:18.\n",
      "  current average loss = 0.007330054904876575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   460  of  1,162.    Elapsed: 0:04:29.\n",
      "  current average loss = 0.007049908156425219\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:41.\n",
      "  current average loss = 0.006776880938754933\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:52.\n",
      "  current average loss = 0.006539673189368841\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:04.\n",
      "  current average loss = 0.006333878383915922\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:16.\n",
      "  current average loss = 0.006132239254617178\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:27.\n",
      "  current average loss = 0.005952666493662459\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:39.\n",
      "  current average loss = 0.008303337414015694\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:51.\n",
      "  current average loss = 0.008065149123438383\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:02.\n",
      "  current average loss = 0.00884274687460682\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:14.\n",
      "  current average loss = 0.008593598214651976\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:25.\n",
      "  current average loss = 0.008347975001865135\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:37.\n",
      "  current average loss = 0.01105503470573895\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:49.\n",
      "  current average loss = 0.01075245811982378\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:01.\n",
      "  current average loss = 0.012930910098248836\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:12.\n",
      "  current average loss = 0.012601611535287272\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:24.\n",
      "  current average loss = 0.012279706464581869\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:36.\n",
      "  current average loss = 0.013031881221566448\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:47.\n",
      "  current average loss = 0.012709966770020458\n",
      "  Batch   820  of  1,162.    Elapsed: 0:07:59.\n",
      "  current average loss = 0.012403900491021776\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:11.\n",
      "  current average loss = 0.012110378940267147\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:22.\n",
      "  current average loss = 0.012817182209143041\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:34.\n",
      "  current average loss = 0.012854259771803682\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:46.\n",
      "  current average loss = 0.012571914050789322\n",
      "  Batch   920  of  1,162.    Elapsed: 0:08:57.\n",
      "  current average loss = 0.012325205084707013\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:09.\n",
      "  current average loss = 0.012078616860629426\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:21.\n",
      "  current average loss = 0.013351499261222699\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:32.\n",
      "  current average loss = 0.013089775228860274\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:44.\n",
      "  current average loss = 0.012829234685436404\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:09:56.\n",
      "  current average loss = 0.012591371510503298\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:07.\n",
      "  current average loss = 0.012362293160273033\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:19.\n",
      "  current average loss = 0.012144996380048158\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:30.\n",
      "  current average loss = 0.011938398587377016\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:42.\n",
      "  current average loss = 0.011728631308432217\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:54.\n",
      "  current average loss = 0.011531209363171716\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:05.\n",
      "  current average loss = 0.011338007223478023\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:17.\n",
      "  current average loss = 0.01115310856344281\n",
      "\n",
      " Average Training Loss: 0.011135274300671592\n",
      " Training epoch took: 0:11:18\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.02680450434656313\n",
      "EarlyStopping counter: 1 out of 5\n",
      "\n",
      "===== Epoch 9 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.00035294181129756906\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.0008841533550025815\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.0009082499385347849\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.0007932128167567499\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:58.\n",
      "  current average loss = 0.008011943112021526\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.006809383332167348\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:21.\n",
      "  current average loss = 0.005856888774685233\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:33.\n",
      "  current average loss = 0.00945415114271242\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:44.\n",
      "  current average loss = 0.008507440325144621\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:56.\n",
      "  current average loss = 0.012364396386597037\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:08.\n",
      "  current average loss = 0.011342290646486865\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:19.\n",
      "  current average loss = 0.01045534679741896\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:31.\n",
      "  current average loss = 0.009653809346572801\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:42.\n",
      "  current average loss = 0.008975545269302236\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:54.\n",
      "  current average loss = 0.008408527627409512\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:06.\n",
      "  current average loss = 0.00793496353158929\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:17.\n",
      "  current average loss = 0.007523370770726455\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:29.\n",
      "  current average loss = 0.007147514060362584\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:41.\n",
      "  current average loss = 0.006808425873142417\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:52.\n",
      "  current average loss = 0.006503383831901033\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:04.\n",
      "  current average loss = 0.006210751462289128\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:15.\n",
      "  current average loss = 0.005947579658571943\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:27.\n",
      "  current average loss = 0.005733270074881108\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:39.\n",
      "  current average loss = 0.005510309776614974\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:50.\n",
      "  current average loss = 0.005296037588492254\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:02.\n",
      "  current average loss = 0.0050947637593043715\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:14.\n",
      "  current average loss = 0.00492744230242242\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:25.\n",
      "  current average loss = 0.004772750832009771\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:37.\n",
      "  current average loss = 0.0057782157696308414\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:49.\n",
      "  current average loss = 0.007316637830919035\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:00.\n",
      "  current average loss = 0.007086162028314623\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:13.\n",
      "  current average loss = 0.006875945279300311\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:24.\n",
      "  current average loss = 0.007722132354629621\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:36.\n",
      "  current average loss = 0.007497806042078991\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:47.\n",
      "  current average loss = 0.007292410343818899\n",
      "  Batch   720  of  1,162.    Elapsed: 0:06:59.\n",
      "  current average loss = 0.007108186426166993\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:11.\n",
      "  current average loss = 0.006932617664494341\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:23.\n",
      "  current average loss = 0.007891912780688938\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:35.\n",
      "  current average loss = 0.023724768939592585\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:46.\n",
      "  current average loss = 0.023146427517571767\n",
      "  Batch   820  of  1,162.    Elapsed: 0:07:58.\n",
      "  current average loss = 0.02259259347729936\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:10.\n",
      "  current average loss = 0.022062246916279837\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:23.\n",
      "  current average loss = 0.021574279009588165\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:35.\n",
      "  current average loss = 0.022202209758406158\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:47.\n",
      "  current average loss = 0.02172429047941389\n",
      "  Batch   920  of  1,162.    Elapsed: 0:08:58.\n",
      "  current average loss = 0.02125978586011681\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:10.\n",
      "  current average loss = 0.02081423442209005\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:22.\n",
      "  current average loss = 0.020403820694736532\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:33.\n",
      "  current average loss = 0.020008650394278824\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:45.\n",
      "  current average loss = 0.019628357264152782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,020  of  1,162.    Elapsed: 0:09:57.\n",
      "  current average loss = 0.02039548667125638\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:09.\n",
      "  current average loss = 0.02175986127763412\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:20.\n",
      "  current average loss = 0.02376159373431688\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:32.\n",
      "  current average loss = 0.02572059610248716\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:44.\n",
      "  current average loss = 0.025262846882945125\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:55.\n",
      "  current average loss = 0.02481313384349418\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:07.\n",
      "  current average loss = 0.024379609019822928\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:19.\n",
      "  current average loss = 0.023971196907820746\n",
      "\n",
      " Average Training Loss: 0.023930675499047326\n",
      " Training epoch took: 0:11:20\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.010507357160567813\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\n",
      "===== Epoch 10 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.00029852713658442556\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.00016884815896958116\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.0002873852278526101\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.00040697903933306634\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:58.\n",
      "  current average loss = 0.0065868965422566815\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.005604875741439249\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:21.\n",
      "  current average loss = 0.00481373886756923\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:33.\n",
      "  current average loss = 0.004250935275747736\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:45.\n",
      "  current average loss = 0.01592054366339918\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:56.\n",
      "  current average loss = 0.014383269247135786\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:08.\n",
      "  current average loss = 0.019319617867426007\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:20.\n",
      "  current average loss = 0.020421998840679124\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:31.\n",
      "  current average loss = 0.01889091459844875\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:43.\n",
      "  current average loss = 0.01756235356290209\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:55.\n",
      "  current average loss = 0.018674344704334468\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:06.\n",
      "  current average loss = 0.01771132206863335\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:18.\n",
      "  current average loss = 0.020554167472906382\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:30.\n",
      "  current average loss = 0.022809388126040313\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:42.\n",
      "  current average loss = 0.021631469688994803\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:53.\n",
      "  current average loss = 0.02189776057338918\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:05.\n",
      "  current average loss = 0.020886966549679052\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:17.\n",
      "  current average loss = 0.024805450448086866\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:29.\n",
      "  current average loss = 0.025142253056831914\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:40.\n",
      "  current average loss = 0.024100153603532702\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:52.\n",
      "  current average loss = 0.02313854782242993\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:04.\n",
      "  current average loss = 0.022579563448619392\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:16.\n",
      "  current average loss = 0.021745709911564333\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:27.\n",
      "  current average loss = 0.02102837685691676\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:39.\n",
      "  current average loss = 0.02229394000317288\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:51.\n",
      "  current average loss = 0.021579385544608462\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:03.\n",
      "  current average loss = 0.020908342731658626\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:14.\n",
      "  current average loss = 0.022373034823082706\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:26.\n",
      "  current average loss = 0.021704266933928143\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:38.\n",
      "  current average loss = 0.02108618767612728\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:50.\n",
      "  current average loss = 0.02145806906076424\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:02.\n",
      "  current average loss = 0.021780745015165495\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:13.\n",
      "  current average loss = 0.021211825673774116\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:25.\n",
      "  current average loss = 0.03711727956103794\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:36.\n",
      "  current average loss = 0.05220463298615542\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:48.\n",
      "  current average loss = 0.05169514138979551\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:00.\n",
      "  current average loss = 0.050442209313053314\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:11.\n",
      "  current average loss = 0.04925636649886141\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:23.\n",
      "  current average loss = 0.04813337949044118\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:35.\n",
      "  current average loss = 0.04705270662129496\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:46.\n",
      "  current average loss = 0.046013291921095614\n",
      "  Batch   920  of  1,162.    Elapsed: 0:08:58.\n",
      "  current average loss = 0.045013899046774275\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:09.\n",
      "  current average loss = 0.04472935264611443\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:21.\n",
      "  current average loss = 0.04446709777128923\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:32.\n",
      "  current average loss = 0.04356668592617717\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:44.\n",
      "  current average loss = 0.042709996406265535\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:09:56.\n",
      "  current average loss = 0.04187949596535521\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:07.\n",
      "  current average loss = 0.04108603282161618\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:19.\n",
      "  current average loss = 0.04032089654070147\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:30.\n",
      "  current average loss = 0.04076773323049997\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:42.\n",
      "  current average loss = 0.04013822275436527\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:54.\n",
      "  current average loss = 0.04241599555072944\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:05.\n",
      "  current average loss = 0.04167264190998255\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:17.\n",
      "  current average loss = 0.04095930960134053\n",
      "\n",
      " Average Training Loss: 0.040889169405191175\n",
      " Training epoch took: 0:11:18\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.010939692094951494\n",
      "EarlyStopping counter: 3 out of 5\n",
      "\n",
      "===== Epoch 11 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.0001366288549206729\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.05762081067718441\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.03863461599518511\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.029207149583052684\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:59.\n",
      "  current average loss = 0.023375775809308833\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.019616181364035392\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.021183214792609664\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:34.\n",
      "  current average loss = 0.022392826214598927\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:46.\n",
      "  current average loss = 0.019937049953318818\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:57.\n",
      "  current average loss = 0.017978235281341313\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:09.\n",
      "  current average loss = 0.01638997922673918\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:21.\n",
      "  current average loss = 0.017671080294770054\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:32.\n",
      "  current average loss = 0.02458640278796819\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:44.\n",
      "  current average loss = 0.02586812558401976\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:57.\n",
      "  current average loss = 0.03073310072052512\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:08.\n",
      "  current average loss = 0.031201968953042326\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:20.\n",
      "  current average loss = 0.029405848090993912\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:32.\n",
      "  current average loss = 0.027800611110963144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   380  of  1,162.    Elapsed: 0:03:44.\n",
      "  current average loss = 0.0263800332566201\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:55.\n",
      "  current average loss = 0.025065911042583763\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:07.\n",
      "  current average loss = 0.025299992082860476\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:19.\n",
      "  current average loss = 0.026440418276512324\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:30.\n",
      "  current average loss = 0.02840290960490516\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:42.\n",
      "  current average loss = 0.029215570353013\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:54.\n",
      "  current average loss = 0.028085537500043983\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:05.\n",
      "  current average loss = 0.028525670528902827\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:17.\n",
      "  current average loss = 0.02747361038676567\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:29.\n",
      "  current average loss = 0.026502650652907633\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:40.\n",
      "  current average loss = 0.028344153112558074\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:52.\n",
      "  current average loss = 0.028778107319869642\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:03.\n",
      "  current average loss = 0.02785502130441658\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:15.\n",
      "  current average loss = 0.027003901972573095\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:26.\n",
      "  current average loss = 0.027112063705651956\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:39.\n",
      "  current average loss = 0.02631706828631212\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:51.\n",
      "  current average loss = 0.025574625523597662\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:03.\n",
      "  current average loss = 0.026497559470260827\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:15.\n",
      "  current average loss = 0.025789762161781176\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:27.\n",
      "  current average loss = 0.025119104315631018\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:39.\n",
      "  current average loss = 0.024477933844483837\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:50.\n",
      "  current average loss = 0.025360603234564964\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:02.\n",
      "  current average loss = 0.02476051820192632\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:14.\n",
      "  current average loss = 0.024178072726894585\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:25.\n",
      "  current average loss = 0.023623530701709087\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:37.\n",
      "  current average loss = 0.023771876239282556\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:48.\n",
      "  current average loss = 0.023923812155993953\n",
      "  Batch   920  of  1,162.    Elapsed: 0:09:00.\n",
      "  current average loss = 0.023414495754118386\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:12.\n",
      "  current average loss = 0.022942311691128738\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:23.\n",
      "  current average loss = 0.02248068594935191\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:35.\n",
      "  current average loss = 0.02203262121107964\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:47.\n",
      "  current average loss = 0.0222011814956752\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:09:59.\n",
      "  current average loss = 0.02176690360753723\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:10.\n",
      "  current average loss = 0.021924086591315346\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:22.\n",
      "  current average loss = 0.02151186791677618\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:34.\n",
      "  current average loss = 0.02112598088513201\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:45.\n",
      "  current average loss = 0.02075266866336051\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:57.\n",
      "  current average loss = 0.021178396694297216\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:08.\n",
      "  current average loss = 0.02080756165692406\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:20.\n",
      "  current average loss = 0.02045889431819456\n",
      "\n",
      " Average Training Loss: 0.020424323587353833\n",
      " Training epoch took: 0:11:21\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.0121503926882207\n",
      "EarlyStopping counter: 4 out of 5\n",
      "\n",
      "===== Epoch 12 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.019689611608566793\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.010126419816910471\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.007053259566871853\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.0053574257755357735\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:58.\n",
      "  current average loss = 0.004497454279916155\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.0038454706183936576\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.007628554565400236\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:33.\n",
      "  current average loss = 0.006708859988068383\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:45.\n",
      "  current average loss = 0.009539927367963703\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:57.\n",
      "  current average loss = 0.008646438758531758\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:08.\n",
      "  current average loss = 0.00786630128515898\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:20.\n",
      "  current average loss = 0.013242358764719124\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:31.\n",
      "  current average loss = 0.012286744050870678\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:43.\n",
      "  current average loss = 0.01144129658414751\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:55.\n",
      "  current average loss = 0.010718284866501336\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:06.\n",
      "  current average loss = 0.010067074042629187\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:18.\n",
      "  current average loss = 0.012254993834428393\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:29.\n",
      "  current average loss = 0.011623288500848617\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:41.\n",
      "  current average loss = 0.011043903118986452\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:53.\n",
      "  current average loss = 0.010497602195086699\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:04.\n",
      "  current average loss = 0.01001820406104154\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:16.\n",
      "  current average loss = 0.010898765807067435\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:28.\n",
      "  current average loss = 0.010428233775471456\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:39.\n",
      "  current average loss = 0.010028508564314602\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:51.\n",
      "  current average loss = 0.009648497830205315\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:03.\n",
      "  current average loss = 0.009300576055878633\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:15.\n",
      "  current average loss = 0.008959665435225327\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:27.\n",
      "  current average loss = 0.008650962312076363\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:39.\n",
      "  current average loss = 0.008387123642627566\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:50.\n",
      "  current average loss = 0.008121693068829549\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:02.\n",
      "  current average loss = 0.007868853428148316\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:14.\n",
      "  current average loss = 0.0076306931602770686\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:25.\n",
      "  current average loss = 0.007406252723405666\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:37.\n",
      "  current average loss = 0.0072025766381559\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:48.\n",
      "  current average loss = 0.007008844810601431\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:00.\n",
      "  current average loss = 0.007655750694901384\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:12.\n",
      "  current average loss = 0.00824826773607706\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:23.\n",
      "  current average loss = 0.00805557694058441\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:35.\n",
      "  current average loss = 0.007854934547738897\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:47.\n",
      "  current average loss = 0.008410739285748434\n",
      "  Batch   820  of  1,162.    Elapsed: 0:07:58.\n",
      "  current average loss = 0.008219607809131897\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:10.\n",
      "  current average loss = 0.008033598453533165\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:22.\n",
      "  current average loss = 0.00852468622042143\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:34.\n",
      "  current average loss = 0.008337748667234477\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:45.\n",
      "  current average loss = 0.009959654651858045\n",
      "  Batch   920  of  1,162.    Elapsed: 0:08:57.\n",
      "  current average loss = 0.01294243238838261\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:09.\n",
      "  current average loss = 0.013298209384220777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   960  of  1,162.    Elapsed: 0:09:20.\n",
      "  current average loss = 0.013023293807001105\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:32.\n",
      "  current average loss = 0.012758925954753931\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:44.\n",
      "  current average loss = 0.013096657257173547\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:09:56.\n",
      "  current average loss = 0.013421076164953503\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:07.\n",
      "  current average loss = 0.01316409985548957\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:19.\n",
      "  current average loss = 0.012920447450590618\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:31.\n",
      "  current average loss = 0.0132546812855734\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:42.\n",
      "  current average loss = 0.013021626989965708\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:54.\n",
      "  current average loss = 0.01279939415543479\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:05.\n",
      "  current average loss = 0.012579567658630858\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:17.\n",
      "  current average loss = 0.012876894467587104\n",
      "\n",
      " Average Training Loss: 0.013574015235274628\n",
      " Training epoch took: 0:11:18\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.00019583739260288483\n",
      "Validation loss decreased (0.003657 --> 0.000196).  Saving model ...\n",
      "\n",
      "===== Epoch 13 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.0002700680159819058\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.00026743216119200496\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.014547488774144312\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.01100620479816854\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:58.\n",
      "  current average loss = 0.008927716732279123\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.007482519935154149\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.0065282191232402705\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:33.\n",
      "  current average loss = 0.005808828770662444\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:45.\n",
      "  current average loss = 0.005196168592116616\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:56.\n",
      "  current average loss = 0.004755684231294026\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:08.\n",
      "  current average loss = 0.004364929590132182\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:20.\n",
      "  current average loss = 0.006831486892492459\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:31.\n",
      "  current average loss = 0.006310443201232374\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:43.\n",
      "  current average loss = 0.0058650963514773075\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:06.\n",
      "  current average loss = 0.01872072585353434\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:17.\n",
      "  current average loss = 0.018337054480162106\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:29.\n",
      "  current average loss = 0.017690210314103387\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:41.\n",
      "  current average loss = 0.017096195766683406\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:52.\n",
      "  current average loss = 0.016534820489871056\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:04.\n",
      "  current average loss = 0.01602531810708001\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:16.\n",
      "  current average loss = 0.015538127094026067\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:27.\n",
      "  current average loss = 0.015075393638472757\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:39.\n",
      "  current average loss = 0.0149771591636529\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:51.\n",
      "  current average loss = 0.015442051920483826\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:02.\n",
      "  current average loss = 0.015021649377543946\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:14.\n",
      "  current average loss = 0.014627994095943954\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:26.\n",
      "  current average loss = 0.015044765952122487\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:37.\n",
      "  current average loss = 0.014668229052224783\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:49.\n",
      "  current average loss = 0.014307185766841002\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:01.\n",
      "  current average loss = 0.013968333781669971\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:13.\n",
      "  current average loss = 0.013637288782873909\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:25.\n",
      "  current average loss = 0.013326275617726762\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:38.\n",
      "  current average loss = 0.013028812873151129\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:50.\n",
      "  current average loss = 0.012753368820394953\n",
      "  Batch   920  of  1,162.    Elapsed: 0:09:02.\n",
      "  current average loss = 0.013727015655765947\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:15.\n",
      "  current average loss = 0.013454215928582597\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:27.\n",
      "  current average loss = 0.013189133336251402\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:39.\n",
      "  current average loss = 0.012927868380903507\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:51.\n",
      "  current average loss = 0.012674194768526945\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:10:03.\n",
      "  current average loss = 0.012996516174584874\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:14.\n",
      "  current average loss = 0.013312560101946775\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:26.\n",
      "  current average loss = 0.013611076699792536\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:38.\n",
      "  current average loss = 0.013371834376388636\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:49.\n",
      "  current average loss = 0.013138009390836177\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:11:01.\n",
      "  current average loss = 0.012904326467215808\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:13.\n",
      "  current average loss = 0.013420634203847284\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:24.\n",
      "  current average loss = 0.013193224374238552\n",
      "\n",
      " Average Training Loss: 0.013170522019168027\n",
      " Training epoch took: 0:11:25\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 5.896243161347129e-06\n",
      "Validation loss decreased (0.000196 --> 0.000006).  Saving model ...\n",
      "\n",
      "===== Epoch 14 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.00020982747652169563\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.00022176085623755172\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.0003295571276453302\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.000307296434468185\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:58.\n",
      "  current average loss = 0.0002948889880394745\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.0002540665634469974\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.00034583253706281476\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:34.\n",
      "  current average loss = 0.00033616151233921697\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:45.\n",
      "  current average loss = 0.0003460170847723197\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:57.\n",
      "  current average loss = 0.00461534442457122\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:09.\n",
      "  current average loss = 0.004214854639879853\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:21.\n",
      "  current average loss = 0.0038929497270290864\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:33.\n",
      "  current average loss = 0.005809255067110432\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:44.\n",
      "  current average loss = 0.005417114833860483\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:56.\n",
      "  current average loss = 0.005078742793591004\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:07.\n",
      "  current average loss = 0.0047777202460524745\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:19.\n",
      "  current average loss = 0.006215320511160084\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:31.\n",
      "  current average loss = 0.008220612121944271\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:42.\n",
      "  current average loss = 0.007810694445852936\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:54.\n",
      "  current average loss = 0.007421311929848287\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:05.\n",
      "  current average loss = 0.007071074819065378\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:17.\n",
      "  current average loss = 0.006767506682735752\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:29.\n",
      "  current average loss = 0.0065039381057856544\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:41.\n",
      "  current average loss = 0.006251696494135335\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:53.\n",
      "  current average loss = 0.006022840548353315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   520  of  1,162.    Elapsed: 0:05:04.\n",
      "  current average loss = 0.008524770576304026\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:16.\n",
      "  current average loss = 0.008218199910128494\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:28.\n",
      "  current average loss = 0.007950173180854746\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:40.\n",
      "  current average loss = 0.007700503125703296\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:51.\n",
      "  current average loss = 0.007451981170569771\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:03.\n",
      "  current average loss = 0.008581037132132\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:15.\n",
      "  current average loss = 0.008319484578390046\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:27.\n",
      "  current average loss = 0.008069066983123959\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:38.\n",
      "  current average loss = 0.007833611876181206\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:50.\n",
      "  current average loss = 0.008849911760198828\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:02.\n",
      "  current average loss = 0.008615100544139094\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:14.\n",
      "  current average loss = 0.0083934492208731\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:26.\n",
      "  current average loss = 0.0081900171404477\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:37.\n",
      "  current average loss = 0.008744793733562845\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:49.\n",
      "  current average loss = 0.008526683555372401\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:01.\n",
      "  current average loss = 0.009565162605362366\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:12.\n",
      "  current average loss = 0.009349536753034775\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:24.\n",
      "  current average loss = 0.009144448715429914\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:36.\n",
      "  current average loss = 0.008945946243284262\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:48.\n",
      "  current average loss = 0.00875275037951806\n",
      "  Batch   920  of  1,162.    Elapsed: 0:08:59.\n",
      "  current average loss = 0.009486177701326672\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:11.\n",
      "  current average loss = 0.009292609304242205\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:23.\n",
      "  current average loss = 0.009105943604626236\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:34.\n",
      "  current average loss = 0.008927737018884947\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:46.\n",
      "  current average loss = 0.008765014687200505\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:09:57.\n",
      "  current average loss = 0.008603794022734763\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:09.\n",
      "  current average loss = 0.009604450850566118\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:21.\n",
      "  current average loss = 0.009427926344787995\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:32.\n",
      "  current average loss = 0.009254340803371763\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:44.\n",
      "  current average loss = 0.010141223029751865\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:56.\n",
      "  current average loss = 0.009960989892008396\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:07.\n",
      "  current average loss = 0.009793022744887624\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:19.\n",
      "  current average loss = 0.009628011580261631\n",
      "\n",
      " Average Training Loss: 0.010358173225995563\n",
      " Training epoch took: 0:11:20\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.012322429364354003\n",
      "EarlyStopping counter: 1 out of 5\n",
      "\n",
      "===== Epoch 15 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.0007758669885845393\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.021583582674037417\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.038471653792133034\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:48.\n",
      "  current average loss = 0.02886158637542664\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:59.\n",
      "  current average loss = 0.03497564398983684\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:11.\n",
      "  current average loss = 0.02916149953806979\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:23.\n",
      "  current average loss = 0.025004146025970286\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:35.\n",
      "  current average loss = 0.021924423951632476\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:46.\n",
      "  current average loss = 0.019513173089560434\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:58.\n",
      "  current average loss = 0.020450048262391364\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:10.\n",
      "  current average loss = 0.01865214268539174\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:21.\n",
      "  current average loss = 0.0194850296085086\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:33.\n",
      "  current average loss = 0.01804971022293365\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:45.\n",
      "  current average loss = 0.01677697370853894\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:57.\n",
      "  current average loss = 0.015692881347857793\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:09.\n",
      "  current average loss = 0.014714084688895568\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:20.\n",
      "  current average loss = 0.0138791720353202\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:32.\n",
      "  current average loss = 0.013110134215072834\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:44.\n",
      "  current average loss = 0.012424016891712074\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:55.\n",
      "  current average loss = 0.011827378543163292\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:07.\n",
      "  current average loss = 0.011287209133003611\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:19.\n",
      "  current average loss = 0.0107967620538619\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:30.\n",
      "  current average loss = 0.012161030368657353\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:42.\n",
      "  current average loss = 0.011657047428119445\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:54.\n",
      "  current average loss = 0.011193154186815605\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:05.\n",
      "  current average loss = 0.010776827065840695\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:17.\n",
      "  current average loss = 0.012485639682995393\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:29.\n",
      "  current average loss = 0.012047231396309753\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:40.\n",
      "  current average loss = 0.01163385444792178\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:52.\n",
      "  current average loss = 0.011259033143419306\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:04.\n",
      "  current average loss = 0.010897498522244713\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:16.\n",
      "  current average loss = 0.011449978942142901\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:28.\n",
      "  current average loss = 0.011109307699428318\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:39.\n",
      "  current average loss = 0.012023556033605168\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:51.\n",
      "  current average loss = 0.011700225938528694\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:02.\n",
      "  current average loss = 0.011376892477303516\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:14.\n",
      "  current average loss = 0.011075348025548238\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:26.\n",
      "  current average loss = 0.010790050447780617\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:38.\n",
      "  current average loss = 0.010523667719667684\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:50.\n",
      "  current average loss = 0.01027067632726954\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:02.\n",
      "  current average loss = 0.010032067523933628\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:14.\n",
      "  current average loss = 0.009808041915867163\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:26.\n",
      "  current average loss = 0.009589045708143068\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:38.\n",
      "  current average loss = 0.009372228263109215\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:49.\n",
      "  current average loss = 0.009801942539395819\n",
      "  Batch   920  of  1,162.    Elapsed: 0:09:01.\n",
      "  current average loss = 0.01021487133834544\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:13.\n",
      "  current average loss = 0.010005132966610633\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:25.\n",
      "  current average loss = 0.00979820653721634\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:37.\n",
      "  current average loss = 0.009603429002780044\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:49.\n",
      "  current average loss = 0.009420865941176316\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:10:00.\n",
      "  current average loss = 0.009239956841384792\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:12.\n",
      "  current average loss = 0.009074090107927847\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:24.\n",
      "  current average loss = 0.008903763864174689\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:36.\n",
      "  current average loss = 0.009530041904731997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:48.\n",
      "  current average loss = 0.009878912882902362\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:11:00.\n",
      "  current average loss = 0.009711309753453636\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:11.\n",
      "  current average loss = 0.009549373819706974\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:23.\n",
      "  current average loss = 0.009877252290060557\n",
      "\n",
      " Average Training Loss: 0.009860393974982602\n",
      " Training epoch took: 0:11:24\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 6.5437065301597465e-06\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\n",
      "===== Epoch 16 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.00034180175823905756\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.0002660633364456544\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.0003852702946782453\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.0003727822004108816\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:58.\n",
      "  current average loss = 0.00034222997169422345\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.00032123281334766793\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.0002855415152786074\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:33.\n",
      "  current average loss = 0.0002941370664679255\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:45.\n",
      "  current average loss = 0.0002689128478228516\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:58.\n",
      "  current average loss = 0.00026299576827170767\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:10.\n",
      "  current average loss = 0.00024403689530872312\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:23.\n",
      "  current average loss = 0.00024044055139948265\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:35.\n",
      "  current average loss = 0.0002253465934477215\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:48.\n",
      "  current average loss = 0.00022262793336307951\n",
      "  Batch   300  of  1,162.    Elapsed: 0:03:00.\n",
      "  current average loss = 0.00023245432929835868\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:14.\n",
      "  current average loss = 0.00022266380195823388\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:26.\n",
      "  current average loss = 0.0002445531638269425\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:39.\n",
      "  current average loss = 0.0002436975481363864\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:50.\n",
      "  current average loss = 0.0002334095735660101\n",
      "  Batch   400  of  1,162.    Elapsed: 0:04:02.\n",
      "  current average loss = 0.0016718246790227908\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:14.\n",
      "  current average loss = 0.003624227743609611\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:26.\n",
      "  current average loss = 0.0034874601813989804\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:38.\n",
      "  current average loss = 0.003338997226717434\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:50.\n",
      "  current average loss = 0.003225638118453015\n",
      "  Batch   500  of  1,162.    Elapsed: 0:05:01.\n",
      "  current average loss = 0.004237579526608102\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:13.\n",
      "  current average loss = 0.004081421515458317\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:25.\n",
      "  current average loss = 0.0039432756440382485\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:37.\n",
      "  current average loss = 0.0038089586060344517\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:49.\n",
      "  current average loss = 0.0036892393484190076\n",
      "  Batch   600  of  1,162.    Elapsed: 0:06:00.\n",
      "  current average loss = 0.004524025150950687\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:12.\n",
      "  current average loss = 0.004379464501438461\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:23.\n",
      "  current average loss = 0.00425307068511902\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:35.\n",
      "  current average loss = 0.004129070714463462\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:47.\n",
      "  current average loss = 0.004018117743509004\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:59.\n",
      "  current average loss = 0.005930904569960051\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:11.\n",
      "  current average loss = 0.005779060851850993\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:22.\n",
      "  current average loss = 0.005628723822000106\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:34.\n",
      "  current average loss = 0.005485668196525831\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:46.\n",
      "  current average loss = 0.0053467100676233515\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:57.\n",
      "  current average loss = 0.005221936222193762\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:09.\n",
      "  current average loss = 0.0058003207412241405\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:21.\n",
      "  current average loss = 0.006339756265333427\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:34.\n",
      "  current average loss = 0.006193868539264052\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:46.\n",
      "  current average loss = 0.007013699586227192\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:59.\n",
      "  current average loss = 0.006862526738758561\n",
      "  Batch   920  of  1,162.    Elapsed: 0:09:11.\n",
      "  current average loss = 0.006727623966190406\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:23.\n",
      "  current average loss = 0.007188169449149635\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:35.\n",
      "  current average loss = 0.007044408428258346\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:46.\n",
      "  current average loss = 0.006904490737102323\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:58.\n",
      "  current average loss = 0.006770692905717055\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:10:10.\n",
      "  current average loss = 0.006646931864667424\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.014271003628147127\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.018973447550589148\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.014351902465057265\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:59.\n",
      "  current average loss = 0.011495888527448983\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:10.\n",
      "  current average loss = 0.019011665159524207\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.016336405813061512\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:34.\n",
      "  current average loss = 0.014334417069445849\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:45.\n",
      "  current average loss = 0.012773353090672243\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:57.\n",
      "  current average loss = 0.011541751405649734\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:09.\n",
      "  current average loss = 0.01053676022108024\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:21.\n",
      "  current average loss = 0.012026256150878349\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:34.\n",
      "  current average loss = 0.011105565142433377\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:45.\n",
      "  current average loss = 0.01033406650385079\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:57.\n",
      "  current average loss = 0.00964960697541566\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:09.\n",
      "  current average loss = 0.009048932568070888\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:20.\n",
      "  current average loss = 0.008526644115379407\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:32.\n",
      "  current average loss = 0.008064103341061469\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:44.\n",
      "  current average loss = 0.007662159590705492\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:56.\n",
      "  current average loss = 0.009689218860795563\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:07.\n",
      "  current average loss = 0.009244918821223705\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:19.\n",
      "  current average loss = 0.008851940707902564\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:31.\n",
      "  current average loss = 0.008486423191351625\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:43.\n",
      "  current average loss = 0.00931271245785581\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:55.\n",
      "  current average loss = 0.008962668035322849\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:06.\n",
      "  current average loss = 0.009707935007950343\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:18.\n",
      "  current average loss = 0.012506199865344288\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:30.\n",
      "  current average loss = 0.012060894183197491\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:42.\n",
      "  current average loss = 0.011656436359895836\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:53.\n",
      "  current average loss = 0.01129134361105533\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:05.\n",
      "  current average loss = 0.010929918191418657\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:17.\n",
      "  current average loss = 0.010595503286316088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   660  of  1,162.    Elapsed: 0:06:29.\n",
      "  current average loss = 0.010292473753990332\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:40.\n",
      "  current average loss = 0.010821355544400583\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:52.\n",
      "  current average loss = 0.010514017510744144\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:04.\n",
      "  current average loss = 0.010247400097532259\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:16.\n",
      "  current average loss = 0.009982998208661106\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:27.\n",
      "  current average loss = 0.011579225282144391\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:39.\n",
      "  current average loss = 0.012003019176508955\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:51.\n",
      "  current average loss = 0.011704411512463793\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:02.\n",
      "  current average loss = 0.011426221074471417\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:14.\n",
      "  current average loss = 0.011168188460488065\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:26.\n",
      "  current average loss = 0.010913251774268194\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:39.\n",
      "  current average loss = 0.01068079097332822\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:51.\n",
      "  current average loss = 0.011081115207536274\n",
      "  Batch   920  of  1,162.    Elapsed: 0:09:04.\n",
      "  current average loss = 0.010844844919749434\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:15.\n",
      "  current average loss = 0.010625787888589644\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:27.\n",
      "  current average loss = 0.011262926586660171\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:39.\n",
      "  current average loss = 0.011037121606935329\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:51.\n",
      "  current average loss = 0.010822897096035605\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:10:02.\n",
      "  current average loss = 0.010622807678678596\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:14.\n",
      "  current average loss = 0.01041934328380515\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:25.\n",
      "  current average loss = 0.010230873935808432\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:37.\n",
      "  current average loss = 0.010041719319600557\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:49.\n",
      "  current average loss = 0.009862447989063619\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:11:01.\n",
      "  current average loss = 0.009689617200130587\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:12.\n",
      "  current average loss = 0.009520066113735556\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:24.\n",
      "  current average loss = 0.009359118577709496\n",
      "\n",
      " Average Training Loss: 0.010072456295450779\n",
      " Training epoch took: 0:11:25\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.013048174439842264\n",
      "EarlyStopping counter: 4 out of 5\n",
      "\n",
      "===== Epoch 18 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 5.383995330134894e-05\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:24.\n",
      "  current average loss = 5.7922149538569555e-05\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.00014180108972633814\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.00019468274741711865\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:59.\n",
      "  current average loss = 0.0002685412029558165\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:11.\n",
      "  current average loss = 0.0003021823525780102\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:23.\n",
      "  current average loss = 0.0003048598186146429\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:35.\n",
      "  current average loss = 0.0003073845607104886\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:47.\n",
      "  current average loss = 0.0002765458373905479\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:59.\n",
      "  current average loss = 0.0002682083220503273\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:10.\n",
      "  current average loss = 0.000269491199973841\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:22.\n",
      "  current average loss = 0.0002732910544080388\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:34.\n",
      "  current average loss = 0.0035175837501406283\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:46.\n",
      "  current average loss = 0.0032840146022305055\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:57.\n",
      "  current average loss = 0.007775563259890532\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:09.\n",
      "  current average loss = 0.007299791710176961\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:21.\n",
      "  current average loss = 0.006888623954211974\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:33.\n",
      "  current average loss = 0.0065294479308229195\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:44.\n",
      "  current average loss = 0.006201805105498848\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:56.\n",
      "  current average loss = 0.008704090895254098\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:08.\n",
      "  current average loss = 0.008312745257724768\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:20.\n",
      "  current average loss = 0.00822136148786179\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:32.\n",
      "  current average loss = 0.007866972431307318\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:44.\n",
      "  current average loss = 0.007552450535534187\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:56.\n",
      "  current average loss = 0.008373971108157206\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:07.\n",
      "  current average loss = 0.008055056899241957\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:19.\n",
      "  current average loss = 0.0077680846382840495\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:31.\n",
      "  current average loss = 0.007496793058029994\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:43.\n",
      "  current average loss = 0.009030290663108566\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:55.\n",
      "  current average loss = 0.008730648722754211\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:06.\n",
      "  current average loss = 0.009379465054853096\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:18.\n",
      "  current average loss = 0.00909199372216527\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:30.\n",
      "  current average loss = 0.010119081817814022\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:42.\n",
      "  current average loss = 0.009850498694316577\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:53.\n",
      "  current average loss = 0.010381796877191748\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:05.\n",
      "  current average loss = 0.011286430514369132\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:17.\n",
      "  current average loss = 0.010986121564015957\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:28.\n",
      "  current average loss = 0.010712024679232001\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:40.\n",
      "  current average loss = 0.010444686904475385\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:52.\n",
      "  current average loss = 0.010195060349331726\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:03.\n",
      "  current average loss = 0.00996294618208087\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:15.\n",
      "  current average loss = 0.010739035981851651\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:27.\n",
      "  current average loss = 0.010490932736412783\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:39.\n",
      "  current average loss = 0.010265160393347178\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:50.\n",
      "  current average loss = 0.010046668360384451\n",
      "  Batch   920  of  1,162.    Elapsed: 0:09:02.\n",
      "  current average loss = 0.009832739219948124\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:14.\n",
      "  current average loss = 0.010530366907297632\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:25.\n",
      "  current average loss = 0.010312137592712035\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:37.\n",
      "  current average loss = 0.010102316854273707\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:49.\n",
      "  current average loss = 0.010462476460685594\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:10:01.\n",
      "  current average loss = 0.010261036217442284\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:12.\n",
      "  current average loss = 0.010069414506538539\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:24.\n",
      "  current average loss = 0.009888420390202587\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:36.\n",
      "  current average loss = 0.009705721136318207\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:47.\n",
      "  current average loss = 0.009529769017871672\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:10:59.\n",
      "  current average loss = 0.009867931963914498\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:11.\n",
      "  current average loss = 0.009697782354346261\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:23.\n",
      "  current average loss = 0.009535913070429215\n",
      "\n",
      " Average Training Loss: 0.009519503269431583\n",
      " Training epoch took: 0:11:24\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.020684956835117965\n",
      "EarlyStopping counter: 5 out of 5\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 19 / 20 =====\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.00043506043746788237\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.0005474787837542294\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.00043201228010332217\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.0003652511722044949\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:59.\n",
      "  current average loss = 0.00039837346826288923\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:11.\n",
      "  current average loss = 0.005028221692424495\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.0043318224232783515\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:34.\n",
      "  current average loss = 0.003828149919266366\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:46.\n",
      "  current average loss = 0.0034519453189975342\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:58.\n",
      "  current average loss = 0.0031482697103092506\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:09.\n",
      "  current average loss = 0.002866736969714752\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:21.\n",
      "  current average loss = 0.0026421884984413188\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:33.\n",
      "  current average loss = 0.0024409609988509827\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:45.\n",
      "  current average loss = 0.004268393541956884\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:57.\n",
      "  current average loss = 0.004011524017034655\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:08.\n",
      "  current average loss = 0.0037714640537080514\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:20.\n",
      "  current average loss = 0.003575270214408774\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:32.\n",
      "  current average loss = 0.0033779001137163945\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:45.\n",
      "  current average loss = 0.0032015414161442753\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:57.\n",
      "  current average loss = 0.004450595714826164\n",
      "  Batch   420  of  1,162.    Elapsed: 0:04:10.\n",
      "  current average loss = 0.004252328215935921\n",
      "  Batch   440  of  1,162.    Elapsed: 0:04:21.\n",
      "  current average loss = 0.004062651624248304\n",
      "  Batch   460  of  1,162.    Elapsed: 0:04:33.\n",
      "  current average loss = 0.003900054285776254\n",
      "  Batch   480  of  1,162.    Elapsed: 0:04:45.\n",
      "  current average loss = 0.0037445317191285928\n",
      "  Batch   500  of  1,162.    Elapsed: 0:04:57.\n",
      "  current average loss = 0.003595852757677335\n",
      "  Batch   520  of  1,162.    Elapsed: 0:05:08.\n",
      "  current average loss = 0.0034693664560522235\n",
      "  Batch   540  of  1,162.    Elapsed: 0:05:20.\n",
      "  current average loss = 0.0033515091459462468\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:32.\n",
      "  current average loss = 0.003237477932258465\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:43.\n",
      "  current average loss = 0.0031320006934032365\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:55.\n",
      "  current average loss = 0.003028536416075364\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:06.\n",
      "  current average loss = 0.002932362351898262\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:18.\n",
      "  current average loss = 0.0028503286391684314\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:30.\n",
      "  current average loss = 0.00277386414637273\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:41.\n",
      "  current average loss = 0.0026983655000298555\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:53.\n",
      "  current average loss = 0.002633475840933995\n",
      "  Batch   720  of  1,162.    Elapsed: 0:07:05.\n",
      "  current average loss = 0.0025652080373010084\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:16.\n",
      "  current average loss = 0.0032709365850867264\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:28.\n",
      "  current average loss = 0.00318576732343708\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:40.\n",
      "  current average loss = 0.0031058983481327047\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:51.\n",
      "  current average loss = 0.0030386976439702095\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:03.\n",
      "  current average loss = 0.002969138509503709\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:15.\n",
      "  current average loss = 0.0029056124030313684\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:27.\n",
      "  current average loss = 0.0028389406844376236\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:38.\n",
      "  current average loss = 0.002789915251767509\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:50.\n",
      "  current average loss = 0.002734018343871109\n",
      "  Batch   920  of  1,162.    Elapsed: 0:09:02.\n",
      "  current average loss = 0.0026795595443818753\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:14.\n",
      "  current average loss = 0.002623044055392353\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:26.\n",
      "  current average loss = 0.002577503551299524\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:38.\n",
      "  current average loss = 0.0025284569378260704\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:50.\n",
      "  current average loss = 0.0030377559573631745\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:10:02.\n",
      "  current average loss = 0.002984119149478789\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:14.\n",
      "  current average loss = 0.0029321306502421425\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:26.\n",
      "  current average loss = 0.0028778601874406367\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:37.\n",
      "  current average loss = 0.002828104373152853\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:49.\n",
      "  current average loss = 0.0027798779392295293\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:11:01.\n",
      "  current average loss = 0.002733417179791431\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:13.\n",
      "  current average loss = 0.0026884016107023476\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:25.\n",
      "  current average loss = 0.002644818120428052\n",
      "\n",
      " Average Training Loss: 0.002640268911603569\n",
      " Training epoch took: 0:11:26\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.016245036606723922\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 20 / 20 =====\n",
      "Training...\n",
      "  Batch    20  of  1,162.    Elapsed: 0:00:12.\n",
      "  current average loss = 5.972993888008205e-05\n",
      "  Batch    40  of  1,162.    Elapsed: 0:00:24.\n",
      "  current average loss = 5.144930364622269e-05\n",
      "  Batch    60  of  1,162.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.00014062935028202143\n",
      "  Batch    80  of  1,162.    Elapsed: 0:00:47.\n",
      "  current average loss = 0.00011856034836768003\n",
      "  Batch   100  of  1,162.    Elapsed: 0:00:59.\n",
      "  current average loss = 0.00012800277189398913\n",
      "  Batch   120  of  1,162.    Elapsed: 0:01:11.\n",
      "  current average loss = 0.00016073395833018366\n",
      "  Batch   140  of  1,162.    Elapsed: 0:01:22.\n",
      "  current average loss = 0.00021486058076821532\n",
      "  Batch   160  of  1,162.    Elapsed: 0:01:34.\n",
      "  current average loss = 0.0002254786225684313\n",
      "  Batch   180  of  1,162.    Elapsed: 0:01:46.\n",
      "  current average loss = 0.0002204624979065607\n",
      "  Batch   200  of  1,162.    Elapsed: 0:01:58.\n",
      "  current average loss = 0.00026322482385353395\n",
      "  Batch   220  of  1,162.    Elapsed: 0:02:09.\n",
      "  current average loss = 0.0053318474184317725\n",
      "  Batch   240  of  1,162.    Elapsed: 0:02:21.\n",
      "  current average loss = 0.00490208562395272\n",
      "  Batch   260  of  1,162.    Elapsed: 0:02:33.\n",
      "  current average loss = 0.004537662158189557\n",
      "  Batch   280  of  1,162.    Elapsed: 0:02:44.\n",
      "  current average loss = 0.004224565211131497\n",
      "  Batch   300  of  1,162.    Elapsed: 0:02:56.\n",
      "  current average loss = 0.00396402488552288\n",
      "  Batch   320  of  1,162.    Elapsed: 0:03:07.\n",
      "  current average loss = 0.003742511115878244\n",
      "  Batch   340  of  1,162.    Elapsed: 0:03:19.\n",
      "  current average loss = 0.003547839821283774\n",
      "  Batch   360  of  1,162.    Elapsed: 0:03:31.\n",
      "  current average loss = 0.0033535373100148846\n",
      "  Batch   380  of  1,162.    Elapsed: 0:03:42.\n",
      "  current average loss = 0.0031865212704632853\n",
      "  Batch   400  of  1,162.    Elapsed: 0:03:54.\n",
      "  current average loss = 0.0030361955305826882\n",
      "  Batch   560  of  1,162.    Elapsed: 0:05:26.\n",
      "  current average loss = 0.0022719398751399516\n",
      "  Batch   580  of  1,162.    Elapsed: 0:05:38.\n",
      "  current average loss = 0.0022000325560205857\n",
      "  Batch   600  of  1,162.    Elapsed: 0:05:51.\n",
      "  current average loss = 0.0030720025442498885\n",
      "  Batch   620  of  1,162.    Elapsed: 0:06:03.\n",
      "  current average loss = 0.0038812005016442557\n",
      "  Batch   640  of  1,162.    Elapsed: 0:06:14.\n",
      "  current average loss = 0.0037800621470800875\n",
      "  Batch   660  of  1,162.    Elapsed: 0:06:26.\n",
      "  current average loss = 0.005806958321037462\n",
      "  Batch   680  of  1,162.    Elapsed: 0:06:37.\n",
      "  current average loss = 0.0068885190309577425\n",
      "  Batch   700  of  1,162.    Elapsed: 0:06:49.\n",
      "  current average loss = 0.006707427054575257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   720  of  1,162.    Elapsed: 0:07:01.\n",
      "  current average loss = 0.006531902171020457\n",
      "  Batch   740  of  1,162.    Elapsed: 0:07:12.\n",
      "  current average loss = 0.006360295473856345\n",
      "  Batch   760  of  1,162.    Elapsed: 0:07:24.\n",
      "  current average loss = 0.006197233598526671\n",
      "  Batch   780  of  1,162.    Elapsed: 0:07:36.\n",
      "  current average loss = 0.006757093791102656\n",
      "  Batch   800  of  1,162.    Elapsed: 0:07:48.\n",
      "  current average loss = 0.007294281734649672\n",
      "  Batch   820  of  1,162.    Elapsed: 0:08:00.\n",
      "  current average loss = 0.007132337742812402\n",
      "  Batch   840  of  1,162.    Elapsed: 0:08:12.\n",
      "  current average loss = 0.006966077380130219\n",
      "  Batch   860  of  1,162.    Elapsed: 0:08:25.\n",
      "  current average loss = 0.006818968677697446\n",
      "  Batch   880  of  1,162.    Elapsed: 0:08:37.\n",
      "  current average loss = 0.00667063917617651\n",
      "  Batch   900  of  1,162.    Elapsed: 0:08:50.\n",
      "  current average loss = 0.00715093294879523\n",
      "  Batch   920  of  1,162.    Elapsed: 0:09:02.\n",
      "  current average loss = 0.007002229618047767\n",
      "  Batch   940  of  1,162.    Elapsed: 0:09:14.\n",
      "  current average loss = 0.0068594399546311295\n",
      "  Batch   960  of  1,162.    Elapsed: 0:09:27.\n",
      "  current average loss = 0.006719942891688468\n",
      "  Batch   980  of  1,162.    Elapsed: 0:09:39.\n",
      "  current average loss = 0.006586060429956793\n",
      "  Batch 1,000  of  1,162.    Elapsed: 0:09:51.\n",
      "  current average loss = 0.0064601165239476475\n",
      "  Batch 1,020  of  1,162.    Elapsed: 0:10:02.\n",
      "  current average loss = 0.006338804296455422\n",
      "  Batch 1,040  of  1,162.    Elapsed: 0:10:14.\n",
      "  current average loss = 0.006220629969122025\n",
      "  Batch 1,060  of  1,162.    Elapsed: 0:10:26.\n",
      "  current average loss = 0.006106908377718224\n",
      "  Batch 1,080  of  1,162.    Elapsed: 0:10:38.\n",
      "  current average loss = 0.005996759645383802\n",
      "  Batch 1,100  of  1,162.    Elapsed: 0:10:49.\n",
      "  current average loss = 0.005901586229837376\n",
      "  Batch 1,120  of  1,162.    Elapsed: 0:11:01.\n",
      "  current average loss = 0.005797076382305682\n",
      "  Batch 1,140  of  1,162.    Elapsed: 0:11:13.\n",
      "  current average loss = 0.005696239285706626\n",
      "  Batch 1,160  of  1,162.    Elapsed: 0:11:24.\n",
      "  current average loss = 0.005609071850370311\n",
      "\n",
      " Average Training Loss: 0.005599425045360988\n",
      " Training epoch took: 0:11:25\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.016059279752680563\n",
      "We are out of patience\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs \n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "train_losses, val_losses = [], [] \n",
    "### binary crossentropy loss ### \n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "### early stopping ### \n",
    "early_stopping = EarlyStopping(patience = 5, verbose = True, delta=0, path=\"BERT_ABSTRACT.pt\")\n",
    "tolerance = True \n",
    "\n",
    "### initialize gradient ###\n",
    "model.zero_grad() \n",
    "\n",
    "for epoch_i in range(0, epochs):  \n",
    "    ### Training ### \n",
    "    print(\"\")\n",
    "    print(\"===== Epoch {:} / {:} =====\".format(epoch_i + 1, epochs))\n",
    "    print(\"Training...\")\n",
    "    t0 = time.time() \n",
    "    total_loss = 0 \n",
    "    model.train() \n",
    "    for step, batch in enumerate(train_dataloader): \n",
    "        if step%20 == 0 and not step == 0: \n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print('  current average loss = {}'.format(total_loss / step))\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        \n",
    "        b_ids1, b_masks1, b_ids2, b_masks2, b_labels = batch \n",
    "        outputs = model(ids1 = b_ids1, \n",
    "                        masks1 = b_masks1, \n",
    "                        ids2 = b_ids2, \n",
    "                        masks2 = b_masks2) \n",
    "        \n",
    "\n",
    "        \n",
    "        loss = criterion(outputs.float(), b_labels.float()) \n",
    "        total_loss += loss.item() \n",
    "        loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "        optimizer.step() \n",
    "        scheduler.step() \n",
    "        model.zero_grad() \n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader) \n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\" Average Training Loss: {}\".format(avg_train_loss)) \n",
    "    print(\" Training epoch took: {:}\".format(format_time(time.time() - t0))) \n",
    "    \n",
    "    ### Validation ### \n",
    "    print(\"\")\n",
    "    print(\"Running Validation\") \n",
    "    t0 = time.time() \n",
    "    model.eval() \n",
    "    \n",
    "    eval_loss = 0 \n",
    "    \n",
    "    for batch in val_dataloader: \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        b_ids1, b_masks1, b_ids2, b_masks2, b_labels = batch \n",
    "        with torch.no_grad(): \n",
    "            outputs = model(ids1 = b_ids1, \n",
    "                            masks1 = b_masks1, \n",
    "                            ids2 = b_ids2, \n",
    "                            masks2 = b_masks2)\n",
    "        \n",
    "        loss = criterion(outputs.float(), b_labels.float())\n",
    "        eval_loss += loss.item() \n",
    "        \n",
    "    avg_val_loss = eval_loss / len(val_dataloader)  \n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(\" Average validation loss: {}\".format(avg_val_loss))  \n",
    "    if tolerance == True:  \n",
    "        early_stopping(avg_val_loss, model) \n",
    "    elif tolerance == False: \n",
    "        if avg_val_loss == np.min(val_losses): \n",
    "            print(\"saving best checkpoint after early stopping!\") \n",
    "            torch.save(model.state_dict(), \"BERT_ABSTRACT_\" + str(epoch_i))\n",
    "    \n",
    "    if early_stopping.early_stop: \n",
    "        print(\"We are out of patience\") \n",
    "        tolerance = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd7f14f97f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(val_losses, label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "We wish to obtain the average ranks of relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT_ABSTRACT(\n",
       "  (bert1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bert2): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (batchnorm1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (batchnorm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"BERT_ABSTRACT.pt\") \n",
    "model = BERT_ABSTRACT() \n",
    "model.load_state_dict(checkpoint)  \n",
    "model.cuda() \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1162/1162 [00:37<00:00, 30.66it/s]\n"
     ]
    }
   ],
   "source": [
    "classes = [] \n",
    "for i in tqdm(range(len(test_abstract1_input_ids)), position=0, leave=True): \n",
    "    ### stores (score, index) ### \n",
    "    ids1 = test_abstract1_input_ids[i] \n",
    "    ids1 = torch.reshape(ids1, (1, 512)) \n",
    "    ids1 = ids1.to(device) \n",
    "        \n",
    "    masks1 = test_abstract1_attention_masks[i] \n",
    "    masks1 = torch.reshape(masks1, (1,512)) \n",
    "    masks1 = masks1.to(device) \n",
    "        \n",
    "    ids2 = test_abstract2_input_ids[j] \n",
    "    ids2 = torch.reshape(ids2, (1,512)) \n",
    "    ids2 = ids2.to(device)  \n",
    "        \n",
    "    masks2 = test_abstract2_attention_masks[j] \n",
    "    masks2 = torch.reshape(masks2, (1,512)) \n",
    "    masks2 = masks2.to(device) \n",
    "        \n",
    "    with torch.no_grad(): \n",
    "        outputs = model(ids1 = ids1, \n",
    "                        masks1 = masks1, \n",
    "                        ids2 = ids2, \n",
    "                        masks2 = masks2)\n",
    "        \n",
    "    score = outputs.detach().cpu().numpy().flatten()[0] \n",
    "    classes.append(1 if score > 0.5 else 0) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 99.82788296041308%\n"
     ]
    }
   ],
   "source": [
    "correct = 0 \n",
    "for i in range(len(test_labels)): \n",
    "    ground_truth = test_labels[i].detach().numpy()[0]\n",
    "    if classes[i] == int(ground_truth): \n",
    "        correct += 1 \n",
    "        \n",
    "print(\"accuracy = {}%\".format(correct / len(test_labels) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
