{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import string \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from transformers import BertPreTrainedModel, BertModel, BertTokenizerFast, AdamW\n",
    "from colbert.parameters import DEVICE \n",
    "import os \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Indexes for corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-08 08:33:30.224419: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\n",
      "\n",
      "[Dec 08, 08:33:31] #> Creating directory /tf/document_similarity/ColBERT/experiments/dirty/index.py/2021-12-08_08.33.29 \n",
      "\n",
      "\n",
      "INFO: 'dirty/index.py' does not exist. Creating a new experiment\n",
      "\n",
      "\n",
      "[Dec 08, 08:33:32] #> Creating directory /tf/document_similarity/ColBERT/experiments/dirty/index.py/2021-12-08_08.33.29/logs/ \n",
      "\n",
      "\n",
      "[Dec 08, 08:33:32] {'root': 'experiments', 'experiment': 'dirty', 'run': '2021-12-08_08.33.29', 'rank': -1, 'similarity': 'cosine', 'dim': 128, 'query_maxlen': 512, 'doc_maxlen': 512, 'mask_punctuation': True, 'checkpoint': './experiments/dirty/train.py/2021-12-07_09.33.51/checkpoints/colbert-40000.dnn', 'bsize': 256, 'amp': True, 'collection': 'test_collections.tsv', 'index_root': './experiments/indexes', 'index_name': 'large_train_index', 'chunksize': 6.0} \n",
      "\n",
      "\n",
      "\n",
      "[Dec 08, 08:33:32] #> Creating directory ./experiments/indexes \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Dec 08, 08:33:32] #> Creating directory ./experiments/indexes/large_train_index \n",
      "\n",
      "\n",
      "[Dec 08, 08:33:32] [0] \t\t #> Local args.bsize = 256\n",
      "[Dec 08, 08:33:32] [0] \t\t #> args.index_root = ./experiments/indexes\n",
      "[Dec 08, 08:33:32] [0] \t\t #> self.possible_subset_sizes = [49152]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ColBERT: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[Dec 08, 08:33:57] #> Loading model checkpoint.\n",
      "[Dec 08, 08:33:57] #> Loading checkpoint ./experiments/dirty/train.py/2021-12-07_09.33.51/checkpoints/colbert-40000.dnn ..\n",
      "[Dec 08, 08:33:59] #> checkpoint['epoch'] = 0\n",
      "[Dec 08, 08:33:59] #> checkpoint['batch'] = 40000\n",
      "{\n",
      "    \"root\": \"experiments\",\n",
      "    \"experiment\": \"dirty\",\n",
      "    \"run\": \"2021-12-07_09.33.51\",\n",
      "    \"rank\": -1,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"dim\": 128,\n",
      "    \"query_maxlen\": 512,\n",
      "    \"doc_maxlen\": 512,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"checkpoint\": null,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 400000,\n",
      "    \"bsize\": 8,\n",
      "    \"accumsteps\": 4,\n",
      "    \"amp\": true,\n",
      "    \"triples\": \"train_val_combined.tsv\",\n",
      "    \"queries\": null,\n",
      "    \"collection\": null\n",
      "}\n",
      "\n",
      "\n",
      "[Dec 08, 08:39:11] [0] \t\t #> Completed batch #0 (starting at passage #0) \t\tPassages/min: 9.9k (overall),  9.9k (this encoding),  23163.9M (this saving)\n",
      "[Dec 08, 08:39:29] [0] \t\t #> Saved batch #0 to ./experiments/indexes/large_train_index/0.pt \t\t Saving Throughput = 162.9k passages per minute.\n",
      "\n",
      "[Dec 08, 08:42:09] [0] \t\t #> Completed batch #1 (starting at passage #49152) \t\tPassages/min: 9.8k (overall),  10.1k (this encoding),  18400.5M (this saving)\n",
      "[Dec 08, 08:42:09] [0] \t\t [NOTE] Done with local share.\n",
      "[Dec 08, 08:42:09] [0] \t\t #> Joining saver thread.\n",
      "[Dec 08, 08:42:16] [0] \t\t #> Saved batch #1 to ./experiments/indexes/large_train_index/1.pt \t\t Saving Throughput = 262.3k passages per minute.\n",
      "\n",
      "[Dec 08, 08:42:16] Saving (the following) metadata to ./experiments/indexes/large_train_index/metadata.json ..\n",
      "Namespace(amp=True, bsize=256, checkpoint='./experiments/dirty/train.py/2021-12-07_09.33.51/checkpoints/colbert-40000.dnn', chunksize=6.0, collection='test_collections.tsv', dim=128, doc_maxlen=512, experiment='dirty', index_name='large_train_index', index_root='./experiments/indexes', mask_punctuation=True, query_maxlen=512, rank=-1, root='experiments', run='2021-12-08_08.33.29', similarity='cosine')\n"
     ]
    }
   ],
   "source": [
    "!python -m colbert.index --amp --doc_maxlen 512 --query_maxlen 512 \\\n",
    "--mask-punctuation --bsize 256 --checkpoint ./experiments/dirty/train.py/2021-12-07_09.33.51/checkpoints/colbert-40000.dnn \\\n",
    "--collection test_collections.tsv --index_root ./experiments/indexes --index_name large_train_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS Indexing for End-To-End Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Dec 08, 08:48:51] #> Creating directory /tf/document_similarity/ColBERT/experiments/dirty/index_faiss.py/2021-12-08_08.48.51 \n",
      "\n",
      "\n",
      "INFO: 'dirty/index_faiss.py' does not exist. Creating a new experiment\n",
      "\n",
      "\n",
      "[Dec 08, 08:48:51] #> Creating directory /tf/document_similarity/ColBERT/experiments/dirty/index_faiss.py/2021-12-08_08.48.51/logs/ \n",
      "\n",
      "\n",
      "[Dec 08, 08:48:51] {'root': 'experiments', 'experiment': 'dirty', 'run': '2021-12-08_08.48.51', 'rank': -1, 'index_root': './experiments/indexes', 'index_name': 'large_train_index', 'partitions': None, 'sample': None, 'slices': 1} \n",
      "\n",
      "#> num_embeddings = 38915192\n",
      "\n",
      "\n",
      "\n",
      "[Dec 08, 08:48:51] [WARNING] \t You did not specify --partitions!\n",
      "[Dec 08, 08:48:51] [WARNING] \t Default computation chooses 65536 partitions (for 38915192 embeddings)\n",
      "\n",
      "\n",
      "\n",
      "[Dec 08, 08:48:51] #> Starting..\n",
      "[Dec 08, 08:48:51] #> Processing slice #1 of 1 (range 0..2).\n",
      "[Dec 08, 08:48:51] #> Will write to ./experiments/indexes/large_train_index/ivfpq.65536.faiss.\n",
      "[Dec 08, 08:48:51] #> Loading ./experiments/indexes/large_train_index/0.sample ...\n",
      "[Dec 08, 08:48:51] #> Loading ./experiments/indexes/large_train_index/1.sample ...\n",
      "#> Sample has shape (1945759, 128)\n",
      "[Dec 08, 08:48:57] Preparing resources for 1 GPUs.\n",
      "[Dec 08, 08:48:57] #> Training with the vectors...\n",
      "[Dec 08, 08:48:57] #> Training now (using 1 GPUs)...\n",
      "128.96438360214233\n",
      "WARNING clustering 1945759 points to 65536 centroids: please provide at least 2555904 training points\n",
      "88.1919252872467\n",
      "0.022738218307495117\n",
      "[Dec 08, 08:52:34] Done training!\n",
      "\n",
      "[Dec 08, 08:52:35] #> Indexing the vectors...\n",
      "[Dec 08, 08:52:35] #> Loading ('./experiments/indexes/large_train_index/0.pt', './experiments/indexes/large_train_index/1.pt', None) (from queue)...\n",
      "[Dec 08, 08:52:44] #> Processing a sub_collection with shape (38915192, 128)\n",
      "[Dec 08, 08:52:44] Add data with shape (38915192, 128) (offset = 0)..\n",
      "  IndexIVFPQ size 0 -> GpuIndexIVFPQ indicesOptions=0 usePrecomputed=0 useFloat16=1 reserveVecs=33554432\n",
      "33488896/38915192 (82.786 s)   Flush indexes to CPU\n",
      "38862848/38915192 (104.140 s)   Flush indexes to CPU\n",
      "add(.) time: 106.775 s \t\t--\t\t index.ntotal = 38915192\n",
      "[Dec 08, 08:54:32] Done indexing!\n",
      "[Dec 08, 08:54:32] Writing index to ./experiments/indexes/large_train_index/ivfpq.65536.faiss ...\n",
      "[Dec 08, 08:54:33] \n",
      "\n",
      "Done! All complete (for slice #1 of 1)!\n"
     ]
    }
   ],
   "source": [
    "!python -m colbert.index_faiss --index_root ./experiments/indexes --index_name large_train_index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve and ReRank\n",
    "\n",
    "Attempt for the first 100 queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A system for charging or maintaining a batter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A method for calculating a path delay in stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A preventive or therapeutic agent for diabete...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            queries\n",
       "0   1   A system for charging or maintaining a batter...\n",
       "1   2   A method for calculating a path delay in stat...\n",
       "2   3   A preventive or therapeutic agent for diabete..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_queries = pd.read_csv(\"test_queries.tsv\",sep=\"\\t\") \n",
    "\n",
    "test_queries.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test_queries = test_queries.iloc[:100,:] \n",
    "\n",
    "small_test_queries.to_csv(\"small_test_queries.tsv\", index=False, sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>passages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A method for battery charger and diagnosis wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A non transitory computer readable medium car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A method for measuring glutamyl transpeptidas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           passages\n",
       "0   1   A method for battery charger and diagnosis wi...\n",
       "1   2   A non transitory computer readable medium car...\n",
       "2   3   A method for measuring glutamyl transpeptidas..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_collections = pd.read_csv(\"test_collections.tsv\", sep=\"\\t\")  \n",
    "\n",
    "test_collections.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2), (77886, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test_queries.shape, test_collections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-08 09:31:24.717369: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\n",
      "\n",
      "[Dec 08, 09:31:26] #> Creating directory /tf/document_similarity/ColBERT/experiments/dirty/retrieve.py/2021-12-08_09.31.24 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Dec 08, 09:31:26] #> Creating directory /tf/document_similarity/ColBERT/experiments/dirty/retrieve.py/2021-12-08_09.31.24/logs/ \n",
      "\n",
      "\n",
      "[Dec 08, 09:31:26] {'root': 'experiments', 'experiment': 'dirty', 'run': '2021-12-08_09.31.24', 'rank': -1, 'similarity': 'cosine', 'dim': 128, 'query_maxlen': 512, 'doc_maxlen': 512, 'mask_punctuation': False, 'checkpoint': './experiments/dirty/train.py/2021-12-07_09.33.51/checkpoints/colbert-40000.dnn', 'bsize': 256, 'amp': True, 'queries': 'small_test_queries.tsv', 'collection': 'test_collections.tsv', 'qrels': None, 'index_root': './experiments/indexes', 'index_name': 'large_train_index', 'partitions': 65536, 'nprobe': 10, 'retrieve_only': True, 'faiss_name': None, 'faiss_depth': 1024, 'part_range': None, 'batch': True, 'depth': 1000} \n",
      "\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ColBERT: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[Dec 08, 09:31:39] #> Loading model checkpoint.\n",
      "[Dec 08, 09:31:39] #> Loading checkpoint ./experiments/dirty/train.py/2021-12-07_09.33.51/checkpoints/colbert-40000.dnn ..\n",
      "[Dec 08, 09:31:40] #> checkpoint['epoch'] = 0\n",
      "[Dec 08, 09:31:40] #> checkpoint['batch'] = 40000\n",
      "{\n",
      "    \"root\": \"experiments\",\n",
      "    \"experiment\": \"dirty\",\n",
      "    \"run\": \"2021-12-07_09.33.51\",\n",
      "    \"rank\": -1,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"dim\": 128,\n",
      "    \"query_maxlen\": 512,\n",
      "    \"doc_maxlen\": 512,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"checkpoint\": null,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 400000,\n",
      "    \"bsize\": 8,\n",
      "    \"accumsteps\": 4,\n",
      "    \"amp\": true,\n",
      "    \"triples\": \"train_val_combined.tsv\",\n",
      "    \"queries\": null,\n",
      "    \"collection\": null\n",
      "}\n",
      "\n",
      "\n",
      "[Dec 08, 09:31:40] #> Loading the queries from small_test_queries.tsv ...\n",
      "[Dec 08, 09:31:40] #> Got 100 queries. All QIDs are unique.\n",
      "\n",
      "[Dec 08, 09:31:40] #> Loading the FAISS index from ./experiments/indexes/large_train_index/ivfpq.65536.faiss ..\n",
      "[Dec 08, 09:31:57] #> Building the emb2pid mapping..\n",
      "[Dec 08, 09:31:58] len(self.emb2pid) = 38915192\n",
      "[Dec 08, 09:32:13] #> Logging ranked lists to /tf/document_similarity/ColBERT/experiments/dirty/retrieve.py/2021-12-08_09.31.24/unordered.tsv\n",
      "[Dec 08, 09:32:13] #> Embedding 100 queries in parallel...\n",
      "[Dec 08, 09:32:13] #> Starting batch retrieval...\n",
      "[Dec 08, 09:32:14] #> Search in batches with faiss. \t\t Q.size() = torch.Size([100, 512, 128]), Q_faiss.size() = torch.Size([51200, 128])\n",
      "[Dec 08, 09:32:14] #> Searching from 0 to 51200...\n",
      "[Dec 08, 09:32:41] #> Lookup the PIDs..\n",
      "[Dec 08, 09:32:41] #> Converting to a list [shape = torch.Size([100, 524288])]..\n",
      "[Dec 08, 09:32:43] #> Removing duplicates (in parallel if large enough)..\n",
      "[Dec 08, 09:32:46] #> Done with embedding_ids_to_pids().\n",
      "[Dec 08, 09:32:46] #> Logging query #0 (qid 1) now...\n",
      "\n",
      "\n",
      "\n",
      "/tf/document_similarity/ColBERT/experiments/dirty/retrieve.py/2021-12-08_09.31.24/unordered.tsv\n",
      "#> Done.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m colbert.retrieve --amp --doc_maxlen 512 --query_maxlen 512 --bsize 256 \\\n",
    "--queries small_test_queries.tsv --partitions 65536\\\n",
    "--index_root ./experiments/indexes --index_name large_train_index \\\n",
    "--collection test_collections.tsv \\\n",
    "--checkpoint ./experiments/dirty/train.py/2021-12-07_09.33.51/checkpoints/colbert-40000.dnn --batch --retrieve_only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-08 09:46:03.224803: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\n",
      "\n",
      "[Dec 08, 09:46:04] #> Creating directory /tf/document_similarity/ColBERT/experiments/dirty/rerank.py/2021-12-08_09.46.03 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Dec 08, 09:46:04] #> Creating directory /tf/document_similarity/ColBERT/experiments/dirty/rerank.py/2021-12-08_09.46.03/logs/ \n",
      "\n",
      "\n",
      "[Dec 08, 09:46:04] {'root': 'experiments', 'experiment': 'dirty', 'run': '2021-12-08_09.46.03', 'rank': -1, 'similarity': 'cosine', 'dim': 128, 'query_maxlen': 512, 'doc_maxlen': 512, 'mask_punctuation': True, 'checkpoint': './experiments/dirty/train.py/2021-12-07_09.33.51/checkpoints/colbert-40000.dnn', 'bsize': 8, 'amp': True, 'queries': 'small_test_queries.tsv', 'collection': None, 'qrels': None, 'topK': './experiments/dirty/retrieve.py/2021-12-08_09.31.24/unordered.tsv', 'shortcircuit': False, 'index_root': './experiments/indexes', 'index_name': 'large_train_index', 'partitions': None, 'step': 1, 'part_range': None, 'log_scores': True, 'batch': True, 'depth': 1000} \n",
      "\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ColBERT: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[Dec 08, 09:46:26] #> Loading model checkpoint.\n",
      "[Dec 08, 09:46:26] #> Loading checkpoint ./experiments/dirty/train.py/2021-12-07_09.33.51/checkpoints/colbert-40000.dnn ..\n",
      "[Dec 08, 09:46:27] #> checkpoint['epoch'] = 0\n",
      "[Dec 08, 09:46:27] #> checkpoint['batch'] = 40000\n",
      "{\n",
      "    \"root\": \"experiments\",\n",
      "    \"experiment\": \"dirty\",\n",
      "    \"run\": \"2021-12-07_09.33.51\",\n",
      "    \"rank\": -1,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"dim\": 128,\n",
      "    \"query_maxlen\": 512,\n",
      "    \"doc_maxlen\": 512,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"checkpoint\": null,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 400000,\n",
      "    \"bsize\": 8,\n",
      "    \"accumsteps\": 4,\n",
      "    \"amp\": true,\n",
      "    \"triples\": \"train_val_combined.tsv\",\n",
      "    \"queries\": null,\n",
      "    \"collection\": null\n",
      "}\n",
      "\n",
      "\n",
      "[Dec 08, 09:46:27] #> Loading the queries from small_test_queries.tsv ...\n",
      "[Dec 08, 09:46:27] #> Got 100 queries. All QIDs are unique.\n",
      "\n",
      "[Dec 08, 09:46:27] #> Loading the top-k PIDs per query from ./experiments/dirty/retrieve.py/2021-12-08_09.31.24/unordered.tsv ...\n",
      "\n",
      "[Dec 08, 09:46:32] #> max(Ks) = 51320 , avg(Ks) = 37955.0\n",
      "[Dec 08, 09:46:32] #> Loaded the top-k per query for 100 unique queries.\n",
      "\n",
      "[Dec 08, 09:46:32] #> Launching a separate thread to load index parts asynchronously.\n",
      "[Dec 08, 09:46:32] tensor.size() =  torch.Size([24546560, 128])\n",
      "[Dec 08, 09:46:32] |> Loading ./experiments/indexes/large_train_index/0.pt ...\n",
      "[Dec 08, 09:46:37] #> Using strides [512]..\n",
      "[Dec 08, 09:46:37] tensor.size() =  torch.Size([14369656, 128])\n",
      "[Dec 08, 09:46:37] |> Loading ./experiments/indexes/large_train_index/1.pt ...\n",
      "[Dec 08, 09:46:40] #> Using strides [512]..\n",
      "[Dec 08, 09:46:50] #> Encoding all 100 queries in batches...\n",
      "[Dec 08, 09:46:51] #> Will process 3795500 query--document pairs in total.\n",
      "[Dec 08, 09:46:51] #> Sorting by PID..\n",
      "[Dec 08, 09:46:52] #> Fetching parts 0--1 from queue..\n",
      "[Dec 08, 09:46:52] #> Filtering PIDs to the range range(0, 49152)..\n",
      "[Dec 08, 09:46:52] #> Got 2396640 query--passage pairs in this range.\n",
      "[Dec 08, 09:46:52] #> Ranking in batches the pairs #0 through #2396640...\n",
      "[Dec 08, 09:46:52] ###--> Got 2396640 query--passage pairs in this sub-range (0, 49152).\n",
      "[Dec 08, 09:46:52] ###--> Ranking in batches the pairs #0 through #2396640 in this sub-range.\n",
      "[Dec 08, 09:46:53] #> Ranking in batches of 512 query--passage pairs...\n",
      "[Dec 08, 09:46:53] #> Processing batch #0..\n",
      "[Dec 08, 09:46:55] #> Processing batch #100..\n",
      "[Dec 08, 09:46:58] #> Processing batch #200..\n",
      "[Dec 08, 09:47:01] #> Processing batch #300..\n",
      "[Dec 08, 09:47:04] #> Processing batch #400..\n",
      "[Dec 08, 09:47:07] #> Processing batch #500..\n",
      "[Dec 08, 09:47:10] #> Processing batch #600..\n",
      "[Dec 08, 09:47:12] #> Processing batch #700..\n",
      "[Dec 08, 09:47:15] #> Processing batch #800..\n",
      "[Dec 08, 09:47:18] #> Processing batch #900..\n",
      "[Dec 08, 09:47:20] #> Processing batch #1000..\n",
      "[Dec 08, 09:47:23] #> Processing batch #1100..\n",
      "[Dec 08, 09:47:25] #> Processing batch #1200..\n",
      "[Dec 08, 09:47:28] #> Processing batch #1300..\n",
      "[Dec 08, 09:47:31] #> Processing batch #1400..\n",
      "[Dec 08, 09:47:33] #> Processing batch #1500..\n",
      "[Dec 08, 09:47:36] #> Processing batch #1600..\n",
      "[Dec 08, 09:47:38] #> Processing batch #1700..\n",
      "[Dec 08, 09:47:41] #> Processing batch #1800..\n",
      "[Dec 08, 09:47:43] #> Processing batch #1900..\n",
      "[Dec 08, 09:47:46] #> Processing batch #2000..\n",
      "[Dec 08, 09:47:48] #> Processing batch #2100..\n",
      "[Dec 08, 09:47:51] #> Processing batch #2200..\n",
      "[Dec 08, 09:47:53] #> Processing batch #2300..\n",
      "[Dec 08, 09:47:56] #> Processing batch #2400..\n",
      "[Dec 08, 09:47:58] #> Processing batch #2500..\n",
      "[Dec 08, 09:48:01] #> Processing batch #2600..\n",
      "[Dec 08, 09:48:03] #> Processing batch #2700..\n",
      "[Dec 08, 09:48:06] #> Processing batch #2800..\n",
      "[Dec 08, 09:48:08] #> Processing batch #2900..\n",
      "[Dec 08, 09:48:11] #> Processing batch #3000..\n",
      "[Dec 08, 09:48:13] #> Processing batch #3100..\n",
      "[Dec 08, 09:48:16] #> Processing batch #3200..\n",
      "[Dec 08, 09:48:18] #> Processing batch #3300..\n",
      "[Dec 08, 09:48:21] #> Processing batch #3400..\n",
      "[Dec 08, 09:48:23] #> Processing batch #3500..\n",
      "[Dec 08, 09:48:26] #> Processing batch #3600..\n",
      "[Dec 08, 09:48:29] #> Processing batch #3700..\n",
      "[Dec 08, 09:48:31] #> Processing batch #3800..\n",
      "[Dec 08, 09:48:33] #> Processing batch #3900..\n",
      "[Dec 08, 09:48:36] #> Processing batch #4000..\n",
      "[Dec 08, 09:48:38] #> Processing batch #4100..\n",
      "[Dec 08, 09:48:41] #> Processing batch #4200..\n",
      "[Dec 08, 09:48:43] #> Processing batch #4300..\n",
      "[Dec 08, 09:48:46] #> Processing batch #4400..\n",
      "[Dec 08, 09:48:48] #> Processing batch #4500..\n",
      "[Dec 08, 09:48:51] #> Processing batch #4600..\n",
      "[Dec 08, 09:48:54] #> Fetching parts 1--2 from queue..\n",
      "[Dec 08, 09:48:55] #> Filtering PIDs to the range range(49152, 77887)..\n",
      "[Dec 08, 09:48:55] #> Got 1398860 query--passage pairs in this range.\n",
      "[Dec 08, 09:48:55] #> Ranking in batches the pairs #2396640 through #3795500...\n",
      "[Dec 08, 09:48:55] ###--> Got 1398860 query--passage pairs in this sub-range (0, 28735).\n",
      "[Dec 08, 09:48:55] ###--> Ranking in batches the pairs #0 through #1398860 in this sub-range.\n",
      "[Dec 08, 09:48:58] #> Ranking in batches of 512 query--passage pairs...\n",
      "[Dec 08, 09:48:58] #> Processing batch #0..\n",
      "[Dec 08, 09:49:00] #> Processing batch #100..\n",
      "[Dec 08, 09:49:03] #> Processing batch #200..\n",
      "[Dec 08, 09:49:05] #> Processing batch #300..\n",
      "[Dec 08, 09:49:08] #> Processing batch #400..\n",
      "[Dec 08, 09:49:10] #> Processing batch #500..\n",
      "[Dec 08, 09:49:13] #> Processing batch #600..\n",
      "[Dec 08, 09:49:15] #> Processing batch #700..\n",
      "[Dec 08, 09:49:18] #> Processing batch #800..\n",
      "[Dec 08, 09:49:20] #> Processing batch #900..\n",
      "[Dec 08, 09:49:23] #> Processing batch #1000..\n",
      "[Dec 08, 09:49:25] #> Processing batch #1100..\n",
      "[Dec 08, 09:49:28] #> Processing batch #1200..\n",
      "[Dec 08, 09:49:30] #> Processing batch #1300..\n",
      "[Dec 08, 09:49:33] #> Processing batch #1400..\n",
      "[Dec 08, 09:49:35] #> Processing batch #1500..\n",
      "[Dec 08, 09:49:37] #> Processing batch #1600..\n",
      "[Dec 08, 09:49:40] #> Processing batch #1700..\n",
      "[Dec 08, 09:49:42] #> Processing batch #1800..\n",
      "[Dec 08, 09:49:45] #> Processing batch #1900..\n",
      "[Dec 08, 09:49:47] #> Processing batch #2000..\n",
      "[Dec 08, 09:49:50] #> Processing batch #2100..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dec 08, 09:49:52] #> Processing batch #2200..\n",
      "[Dec 08, 09:49:54] #> Processing batch #2300..\n",
      "[Dec 08, 09:49:57] #> Processing batch #2400..\n",
      "[Dec 08, 09:49:59] #> Processing batch #2500..\n",
      "[Dec 08, 09:50:02] #> Processing batch #2600..\n",
      "[Dec 08, 09:50:04] #> Processing batch #2700..\n",
      "[Dec 08, 09:50:06] #> Logging ranked lists to /tf/document_similarity/ColBERT/experiments/dirty/rerank.py/2021-12-08_09.46.03/ranking.tsv\n",
      "[Dec 08, 09:50:06] #> Logging query #0 (qid 1) now...\n",
      "#> ( QID 1 ) 1)  10795 : 486.26324462890625      None\n",
      "#> ( QID 1 ) 2)  2853 : 485.85107421875      None\n",
      "\n",
      "\n",
      "\n",
      "/tf/document_similarity/ColBERT/experiments/dirty/rerank.py/2021-12-08_09.46.03/ranking.tsv\n",
      "[Dec 08, 09:50:07] #> Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m colbert.rerank --batch --log-scores --topk ./experiments/dirty/retrieve.py/2021-12-08_09.31.24/unordered.tsv \\\n",
    "--query_maxlen 512 --doc_maxlen 512 --mask-punctuation \\\n",
    "--checkpoint ./experiments/dirty/train.py/2021-12-07_09.33.51/checkpoints/colbert-40000.dnn \\\n",
    "--amp --queries small_test_queries.tsv \\\n",
    "--index_root ./experiments/indexes --index_name large_train_index --bsize 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: MRR (Mean Reciprocal Rank) for 100 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10795</th>\n",
       "      <th>1.1</th>\n",
       "      <th>486.26324462890625</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2853</td>\n",
       "      <td>2</td>\n",
       "      <td>485.851074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>21272</td>\n",
       "      <td>3</td>\n",
       "      <td>485.454071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>31073</td>\n",
       "      <td>4</td>\n",
       "      <td>484.933655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>28206</td>\n",
       "      <td>5</td>\n",
       "      <td>484.728210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>31792</td>\n",
       "      <td>6</td>\n",
       "      <td>484.728210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  10795  1.1  486.26324462890625\n",
       "0  1   2853    2          485.851074\n",
       "1  1  21272    3          485.454071\n",
       "2  1  31073    4          484.933655\n",
       "3  1  28206    5          484.728210\n",
       "4  1  31792    6          484.728210"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings = pd.read_csv(\"experiments/dirty/rerank.py/2021-12-08_09.46.03/ranking.tsv\", sep=\"\\t\") \n",
    "\n",
    "rankings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10795</td>\n",
       "      <td>1.1</td>\n",
       "      <td>486.26324462890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2853</td>\n",
       "      <td>2</td>\n",
       "      <td>485.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21272</td>\n",
       "      <td>3</td>\n",
       "      <td>485.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>31073</td>\n",
       "      <td>4</td>\n",
       "      <td>484.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>28206</td>\n",
       "      <td>5</td>\n",
       "      <td>484.728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1    2                   3\n",
       "0  1  10795  1.1  486.26324462890625\n",
       "1  1   2853    2             485.851\n",
       "2  1  21272    3             485.454\n",
       "3  1  31073    4             484.934\n",
       "4  1  28206    5             484.728"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings = rankings.columns.to_frame().T.append(rankings, ignore_index=True) \n",
    "\n",
    "rankings.columns = range(len(rankings.columns)) \n",
    "\n",
    "rankings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_texts = small_test_queries['queries'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load dictionary that contains (query text, similar document list) \n",
    "\n",
    "with open('query_positive_dict.pkl', 'rb') as f: \n",
    "    loaded_dict = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ranks(df, idx):\n",
    "    relevant = loaded_dict[query_texts[idx-1]] \n",
    "    rank_i = df[df[0]==idx] \n",
    "    for i, val in enumerate(rank_i[1].values): \n",
    "        if val in relevant: \n",
    "            return i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR for 100 queries = 0.212\n"
     ]
    }
   ],
   "source": [
    "### MRR calculation \n",
    "ranks = [] \n",
    "for i in range(1,small_test_queries.shape[0]+1):  \n",
    "    ranks.append(calc_ranks(rankings, i))  \n",
    "\n",
    "s = 0 \n",
    "for r in ranks: \n",
    "    if r is None: \n",
    "        s += 0 \n",
    "    else: \n",
    "        s += 1/(r+1)  \n",
    "\n",
    "s = s / len(ranks)\n",
    "\n",
    "print(\"MRR for {} queries = {:.3f}\".format(len(ranks), s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
