{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f996ee-f7e9-4714-b602-5cc902e469cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler, IterableDataset\n",
    "from pytorch_metric_learning import miners, losses\n",
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "from pytorch_lightning.callbacks import BasePredictionWriter\n",
    "from pytorch_lightning.core.saving import load_hparams_from_yaml, update_hparams\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import addict\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b460f8-330f-42f1-9ed6-7f361f0b3618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletData(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        with Path(path).open(\"r\", encoding=\"utf8\") as f:\n",
    "            for i, triplet in enumerate(f):\n",
    "                try:\n",
    "                    query, positive, negative = triplet.strip().split(\",\")\n",
    "                    data = []\n",
    "                    data.append(\"../storage/FGH_spec_ind_claim_triplet_v1.4.1s/{}.txt\".format(query))\n",
    "                    data.append(\"../storage/FGH_spec_ind_claim_triplet_v1.4.1s/{}.txt\".format(positive))\n",
    "                    data.append(\"../storage/FGH_spec_ind_claim_triplet_v1.4.1s/{}.txt\".format(negative))\n",
    "                    self.data.append(data)\n",
    "                except:\n",
    "                    continue\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85dfd4a6-44f4-4615-90b4-ccebd057153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_collate(object):\n",
    "    def __init__(self,\n",
    "                 is_test=False,\n",
    "                 plm=\"tanapatentlm/patentdeberta_base_spec_1024_pwi\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(plm)\n",
    "        # regular token -> special token\n",
    "        self.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"[IPC]\", \"[TTL]\", \"[CLMS]\", \"[ABST]\"]})\n",
    "        self.chunk_size = 512\n",
    "        self.is_test = is_test\n",
    "    def clean_text(self, t):\n",
    "        x = re.sub(\"\\d+\",\"\",t)\n",
    "        x = x.replace(\"\\n\",\" \")\n",
    "        x = x.strip()\n",
    "        return x\n",
    "    def __call__(self, batch):\n",
    "        ret = [] # for inference\n",
    "        input_ids, attn_masks, labels = [], [], [] # for training and validation\n",
    "        ids = 0\n",
    "        for idx, triplet in enumerate(batch):\n",
    "            try:\n",
    "                query_txt, positive_txt, negative_txt = triplet\n",
    "                with Path(query_txt).open(\"r\", encoding=\"utf8\") as f:\n",
    "                    q = f.read()\n",
    "                with Path(positive_txt).open(\"r\", encoding=\"utf8\") as f:\n",
    "                    p = f.read()\n",
    "                with Path(negative_txt).open(\"r\", encoding=\"utf8\") as f:\n",
    "                    n = f.read()\n",
    "\n",
    "                q_ttl = re.search(\"<TTL>([\\s\\S]*?)<IPC>\", q).group(1)\n",
    "                q_ttl = q_ttl.lower()\n",
    "                q_ipc = re.search(\"<IPC>([\\s\\S]*?)<ABST>\", q).group(1)\n",
    "                q_ipc = q_ipc[:3]\n",
    "                q_clms = re.search(\"<CLMS>([\\s\\S]*?)<DESC>\", q).group(1)\n",
    "                q_ind_clms = q_clms.split(\"\\n\\n\")\n",
    "                q_clean_ind_clms = []\n",
    "                for q_ind_clm in q_ind_clms:\n",
    "                    if \"(canceled)\" in q_ind_clm:\n",
    "                        continue\n",
    "                    else:\n",
    "                        q_clean_ind_clms.append(self.clean_text(q_ind_clm))\n",
    "                q_text_input = \"[IPC]\" + q_ipc + \"[TTL]\" + q_ttl\n",
    "                for i in range(len(q_clean_ind_clms)):\n",
    "                    q_text_input += \"[CLMS]\" + q_clean_ind_clms[i]\n",
    "                encoded_q = self.tokenizer(q_text_input, return_tensors=\"pt\", max_length=self.chunk_size, padding=\"max_length\", truncation=True)\n",
    "\n",
    "                p_ttl = re.search(\"<TTL>([\\s\\S]*?)<IPC>\", p).group(1)\n",
    "                p_ttl = p_ttl.lower()\n",
    "                p_ipc = re.search(\"<IPC>([\\s\\S]*?)<ABST>\", p).group(1)\n",
    "                p_ipc = p_ipc[:3]\n",
    "                p_clms = re.search(\"<CLMS>([\\s\\S]*?)<DESC>\", p).group(1)\n",
    "                p_ind_clms = p_clms.split(\"\\n\\n\")\n",
    "                p_clean_ind_clms = []\n",
    "                for p_ind_clm in p_ind_clms:\n",
    "                    if \"(canceled)\" in p_ind_clm:\n",
    "                        continue\n",
    "                    else:\n",
    "                        p_clean_ind_clms.append(self.clean_text(p_ind_clm))\n",
    "                p_text_input = \"[IPC]\" + p_ipc + \"[TTL]\" + p_ttl\n",
    "                for i in range(len(p_clean_ind_clms)):\n",
    "                    p_text_input += \"[CLMS]\" + p_clean_ind_clms[i]\n",
    "                encoded_p = self.tokenizer(p_text_input, return_tensors=\"pt\", max_length=self.chunk_size, padding=\"max_length\", truncation=True)\n",
    "\n",
    "                n_ttl = re.search(\"<TTL>([\\s\\S]*?)<IPC>\", n).group(1)\n",
    "                n_ttl = n_ttl.lower()\n",
    "                n_ipc = re.search(\"<IPC>([\\s\\S]*?)<ABST>\", n).group(1)\n",
    "                n_ipc = n_ipc[:3]\n",
    "                n_clms = re.search(\"<CLMS>([\\s\\S]*?)<DESC>\", n).group(1)\n",
    "                n_ind_clms = n_clms.split(\"\\n\\n\")\n",
    "                n_clean_ind_clms = []\n",
    "                for n_ind_clm in n_ind_clms:\n",
    "                    if \"(canceled)\" in n_ind_clm:\n",
    "                        continue\n",
    "                    else:\n",
    "                        n_clean_ind_clms.append(self.clean_text(n_ind_clm))\n",
    "                n_text_input = \"[IPC]\" + n_ipc + \"[TTL]\" + n_ttl\n",
    "                for i in range(len(n_clean_ind_clms)):\n",
    "                    n_text_input += \"[CLMS]\" + n_clean_ind_clms[i]\n",
    "                encoded_n = self.tokenizer(n_text_input, return_tensors=\"pt\", max_length=self.chunk_size, padding=\"max_length\", truncation=True)\n",
    "\n",
    "                if self.is_test==False:\n",
    "                    input_ids.append(encoded_q[\"input_ids\"])\n",
    "                    attn_masks.append(encoded_q[\"attention_mask\"])\n",
    "                    labels.append(ids*2)\n",
    "\n",
    "                    input_ids.append(encoded_p[\"input_ids\"])\n",
    "                    attn_masks.append(encoded_p[\"attention_mask\"])\n",
    "                    labels.append(ids*2)\n",
    "\n",
    "                    input_ids.append(encoded_n[\"input_ids\"])\n",
    "                    attn_masks.append(encoded_n[\"attention_mask\"])\n",
    "                    labels.append(ids*2 + 1)\n",
    "                    ids += 1\n",
    "                else:\n",
    "                    ret.append([encoded_q[\"input_ids\"],\n",
    "                                encoded_q[\"attention_mask\"],\n",
    "                                encoded_p[\"input_ids\"],\n",
    "                                encoded_p[\"attention_mask\"],\n",
    "                                encoded_n[\"input_ids\"],\n",
    "                                encoded_n[\"attention_mask\"]])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if self.is_test == False:\n",
    "            input_ids = torch.stack(input_ids, dim=0).squeeze(dim=1)\n",
    "            attn_masks = torch.stack(attn_masks, dim=0).squeeze(dim=1)\n",
    "            labels = torch.tensor(labels, dtype=int)\n",
    "            return input_ids, attn_masks, labels\n",
    "        else:\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac1be73-5254-4e85-8e7f-29e8ed6a0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = \"epoch_end_checkpoints-epoch=01-val_loss=0.21802041.ckpt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13c0a21c-f557-4160-abe2-b8c2d8ad049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralRanker(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 hparams=dict(),\n",
    "                 plm=\"tanapatentlm/patentdeberta_base_spec_1024_pwi\",\n",
    "                 is_train=True,\n",
    "                 loss_type=\"ContrastiveLoss\",\n",
    "                 use_miner=True):\n",
    "        super(NeuralRanker, self).__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        self.save_hyperparameters(ignore=\"hparams\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(plm)\n",
    "        self.config = AutoConfig.from_pretrained(plm)\n",
    "        self.is_train = is_train\n",
    "        self.loss_type = loss_type\n",
    "        self.use_miner = use_miner\n",
    "\n",
    "        if plm == \"tanapatentlm/patentdeberta_base_spec_1024_pwi\":\n",
    "            print(\"initialize PLM from previous checkpoint trained on FGH_v0.3.1\")\n",
    "            self.net = AutoModel.from_pretrained(plm)\n",
    "            state_dict = torch.load(hparams[\"checkpoint\"], map_location=self.device)\n",
    "            new_weights = self.net.state_dict()\n",
    "            old_weights = list(state_dict.items())\n",
    "            i = 0\n",
    "            for k, _ in new_weights.items():\n",
    "                new_weights[k] = old_weights[i][1]\n",
    "                i += 1\n",
    "            self.net.load_state_dict(new_weights)\n",
    "        else:\n",
    "            self.net = AutoModel.from_pretrained(plm)\n",
    "\n",
    "        if self.is_train == False:\n",
    "            self.net.eval()  # change to evaluation mode\n",
    "\n",
    "        if self.loss_type == \"ContrastiveLoss\":\n",
    "            self.metric = losses.ContrastiveLoss()  # default is L2 distance\n",
    "            # # change cosine similarity\n",
    "            # self.metric = losses.ContrastiveLoss(\n",
    "            #     pos_margin=1, neg_margin=0,\n",
    "            #     distance=CosineSimilarity(),\n",
    "            # )\n",
    "        elif self.loss_type == \"TripletMarginLoss\":\n",
    "            self.metric = losses.TripletMarginLoss()\n",
    "        elif self.loss_type == \"MultiSimilarityLoss\":\n",
    "            self.metric = losses.MultiSimilarityLoss()\n",
    "\n",
    "        if self.use_miner:\n",
    "            self.miner = miners.MultiSimilarityMiner()\n",
    "\n",
    "        if \"additional_special_tokens\" in self.hparams and self.hparams[\"additional_special_tokens\"]:\n",
    "            additional_special_tokens = self.hparams[\"additional_special_tokens\"]\n",
    "            self.tokenizer.add_special_tokens({\"additional_special_tokens\": additional_special_tokens})\n",
    "            self.net.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        model_output = self.net(input_ids, attention_mask)\n",
    "        model_output = self.mean_pooling(model_output, attention_mask)\n",
    "        return model_output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(),\n",
    "                                      lr=float(self.hparams.lr),\n",
    "                                      weight_decay=float(self.hparams.weight_decay),\n",
    "                                      eps=float(self.hparams.adam_epsilon))\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attn_masks, labels = batch\n",
    "        embeddings = self(input_ids, attn_masks)\n",
    "        if self.use_miner:\n",
    "            hard_pairs = self.miner(embeddings, labels)\n",
    "            loss = self.metric(embeddings, labels, hard_pairs)\n",
    "        else:\n",
    "            loss = self.metric(embeddings, labels)\n",
    "        self.log(\"train_loss\", loss, batch_size=len(batch))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attn_masks, labels = batch\n",
    "        embeddings = self(input_ids, attn_masks)\n",
    "        loss = self.metric(embeddings, labels)\n",
    "        self.log(\"val_loss\", loss, batch_size=len(batch))\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        print(f\"\\nEpoch {self.current_epoch} | avg_loss:{avg_loss}\\n\")\n",
    "\n",
    "    def predict_step(self, batch, batch_idx: int, dataloader_idx: int=0):\n",
    "        q_input_ids, q_attn_masks, p_input_ids, p_attn_masks, n_input_ids, n_attn_masks = zip(*batch)\n",
    "        q_input_ids = torch.stack(q_input_ids).squeeze(dim=1) \n",
    "        q_attn_masks = torch.stack(q_attn_masks).squeeze(dim=1) \n",
    "        p_input_ids = torch.stack(p_input_ids).squeeze(dim=1) \n",
    "        p_attn_masks = torch.stack(p_attn_masks).squeeze(dim=1) \n",
    "        n_input_ids = torch.stack(n_input_ids).squeeze(dim=1) \n",
    "        n_attn_masks = torch.stack(n_attn_masks).squeeze(dim=1)         \n",
    "        q_emb = self(q_input_ids, q_attn_masks)\n",
    "        p_emb = self(p_input_ids, p_attn_masks)\n",
    "        n_emb = self(n_input_ids, n_attn_masks)\n",
    "        return q_emb, p_emb, n_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa3af5b7-7d43-4e8f-aefc-f4ff7a50d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TripletData(\"../storage/FGH_spec_ind_claim_triplet_v1.4.1s/test_triplet.csv\") \n",
    "collate = custom_collate(is_test=True) \n",
    "test_dataloader = DataLoader(test_set, batch_size=160, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8537b7a-401a-4fc6-9bf1-b96f4085d038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_data_path': '../storage/train_triplet.csv',\n",
       " 'val_data_path': '../storage/valid_triplet.csv',\n",
       " 'test_data_path': '../storage/test_triplet.csv',\n",
       " 'output_path': 'outputs',\n",
       " 'model_path': 'checkpoints',\n",
       " 'load_chkpt': None,\n",
       " 'name': 'Patent_DeBERTa',\n",
       " 'epochs': 10,\n",
       " 'batch_size': 18,\n",
       " 'additional_special_tokens': ['[IPC]', '[TTL]', '[CLMS]', '[ABST]'],\n",
       " 'weight_decay': 0.0,\n",
       " 'lr': '2e-5',\n",
       " 'adam_epsilon': '1e-8',\n",
       " 'warmup_steps': 100,\n",
       " 'checkpoint': 'ipc_title_firstclaims_epoch_2_steps_6000_val_loss_0.13378801833644857.pt'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--setting\", \"-s\", type=str, default=\"default.yaml\", help=\"Experiment settings\")\n",
    "args = parser.parse_args(args=[])\n",
    "hparams = addict.Addict(dict(load_hparams_from_yaml(args.setting)))\n",
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d16728d0-d4ed-415b-afb3-fb90290ec8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize PLM from previous checkpoint trained on FGH_v0.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tanapatentlm/patentdeberta_base_spec_1024_pwi were not used when initializing DebertaModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = NeuralRanker(hparams,\n",
    "                     is_train = False, \n",
    "                     loss_type = \"MultiSimilarityLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9dfca8a2-f702-4d87-b4bc-ada2e5103f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize PLM from previous checkpoint trained on FGH_v0.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tanapatentlm/patentdeberta_base_spec_1024_pwi were not used when initializing DebertaModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") \n",
    "checkpoint = torch.load(ckpt, map_location=device) \n",
    "loaded_dict = checkpoint[\"state_dict\"] \n",
    "model = NeuralRanker(hparams,\n",
    "                     is_train = False, \n",
    "                     loss_type = \"MultiSimilarityLoss\")\n",
    "model.load_state_dict(loaded_dict)  \n",
    "model.eval()\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec75684-074b-4df5-9fbf-da3b4b61a2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecac309657da40a496aac4386a657d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device_cnt = torch.cuda.device_count()\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "\n",
    "vectors = trainer.predict(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c09b2c-174c-44aa-a86c-4ca6dd845838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da9a45-c433-4613-b161-d6fe9b9040cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950cd92-cf83-4684-91dd-837cd027e565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d7013-dee9-46c4-89ad-fa0d43a9a463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
