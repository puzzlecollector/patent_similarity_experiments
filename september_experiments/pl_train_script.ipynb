{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aad6b28-30f0-4090-923c-1f55b4c66e34",
   "metadata": {},
   "source": [
    "# Pytorch Lightning: Create DataLoader and Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bb856b75-47a0-4fa5-b64d-0682ebbb0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler, IterableDataset \n",
    "from pytorch_metric_learning import miners, losses \n",
    "import sys \n",
    "from pathlib import Path \n",
    "import shutil \n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy \n",
    "from pytorch_lightning.callbacks import BasePredictionWriter  \n",
    "from pytorch_lightning.core.saving import load_hparams_from_yaml, update_hparams\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from typing import List\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm.auto import tqdm \n",
    "import re\n",
    "from transformers import (\n",
    "    AdamW, \n",
    "    AutoConfig, \n",
    "    AutoModel, \n",
    "    AutoTokenizer, \n",
    "    get_linear_schedule_with_warmup,\n",
    ") \n",
    "import addict \n",
    "import argparse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0d380c14-5c41-4e1e-9f1e-7b604eaa5d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletData(Dataset): \n",
    "    def __init__(self, path): \n",
    "        super().__init__() \n",
    "        self.data = [] \n",
    "        with Path(path).open(\"r\", encoding=\"utf8\") as f: \n",
    "            for i, triplet in enumerate(f): \n",
    "                query, positive, negative = triplet.strip().split(\",\") \n",
    "                data = [] \n",
    "                data.append(\"../storage/FGH_spec_ind_claim_triplet_v1.4.1/{}.txt\".format(query)) \n",
    "                data.append(\"../storage/FGH_spec_ind_claim_triplet_v1.4.1/{}.txt\".format(positive)) \n",
    "                data.append(\"../storage/FGH_spec_ind_claim_triplet_v1.4.1/{}.txt\".format(negative)) \n",
    "                self.data.append(data) \n",
    "    def __getitem__(self, index): \n",
    "        return self.data[index] \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a0c1f333-8afa-4a8d-a15c-7f12bd57e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_collate(object): \n",
    "    def __init__(self, is_test=False): \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"tanapatentlm/patentdeberta_base_spec_1024_pwi\") \n",
    "        self.tokenizer.add_tokens([\"[IPC]\", \"[TTL]\", \"[CLMS]\", \"[ABST]\"]) \n",
    "        self.chunk_size = 512 \n",
    "        self.is_test = is_test \n",
    "    def clean_text(self, t): \n",
    "        x = re.sub(\"\\d+\",\"\",t) \n",
    "        x = x.replace(\"\\n\",\" \")\n",
    "        x = x.strip() \n",
    "        return x \n",
    "    def __call__(self, batch): \n",
    "        ret = [] # for inference \n",
    "        input_ids, attn_masks, labels = [], [], [] # for training and validation \n",
    "        ids = 0\n",
    "        for idx, triplet in enumerate(batch): \n",
    "            query_txt, positive_txt, negative_txt = triplet \n",
    "            with Path(query_txt).open(\"r\", encoding=\"utf8\") as f: \n",
    "                q = f.read() \n",
    "            with Path(positive_txt).open(\"r\", encoding=\"utf8\") as f: \n",
    "                p = f.read() \n",
    "            with Path(negative_txt).open(\"r\", encoding=\"utf8\") as f: \n",
    "                n = f.read() \n",
    "            \n",
    "            q_ttl = re.search(\"<TTL>([\\s\\S]*?)<IPC>\", q).group(1) \n",
    "            q_ttl = q_ttl.lower() \n",
    "            q_ipc = re.search(\"<IPC>([\\s\\S]*?)<ABST>\", q).group(1) \n",
    "            q_ipc = q_ipc[:3] \n",
    "            q_clms = re.search(\"<CLMS>([\\s\\S]*?)<DESC>\", q).group(1) \n",
    "            q_ind_clms = q_clms.split(\"\\n\\n\") \n",
    "            q_clean_ind_clms = [] \n",
    "            for q_ind_clm in q_ind_clms: \n",
    "                if \"(canceled)\" in q_ind_clm:\n",
    "                    continue \n",
    "                else:\n",
    "                    q_clean_ind_clms.append(self.clean_text(q_ind_clm)) \n",
    "            q_text_input = \"[IPC]\" + q_ipc + \"[TTL]\" + q_ttl \n",
    "            for i in range(len(q_clean_ind_clms)): \n",
    "                q_text_input = q_text_input + \"[CLMS]\" + q_clean_ind_clms[i] \n",
    "            encoded_q = self.tokenizer(q_text_input, return_tensors=\"pt\", max_length=self.chunk_size, padding=\"max_length\", truncation=True) \n",
    "            \n",
    "            p_ttl = re.search(\"<TTL>([\\s\\S]*?)<IPC>\", p).group(1) \n",
    "            p_ttl = p_ttl.lower() \n",
    "            p_ipc = re.search(\"<IPC>([\\s\\S]*?)<ABST>\", p).group(1) \n",
    "            p_ipc = p_ipc[:3] \n",
    "            p_clms = re.search(\"<CLMS>([\\s\\S]*?)<DESC>\", p).group(1) \n",
    "            p_ind_clms = p_clms.split(\"\\n\\n\") \n",
    "            p_clean_ind_clms = [] \n",
    "            for p_ind_clm in p_ind_clms: \n",
    "                if \"(canceled)\" in p_ind_clm: \n",
    "                    continue \n",
    "                else: \n",
    "                    p_clean_ind_clms.append(self.clean_text(p_ind_clm)) \n",
    "            p_text_input = \"[IPC]\" + p_ipc + \"[TTL]\" + p_ttl \n",
    "            for i in range(len(p_clean_ind_clms)):\n",
    "                p_text_input = p_text_input + \"[CLMS]\" + p_clean_ind_clms[i] \n",
    "            encoded_p = self.tokenizer(p_text_input, return_tensors=\"pt\", max_length=self.chunk_size, padding=\"max_length\", truncation=True) \n",
    "            \n",
    "            n_ttl = re.search(\"<TTL>([\\s\\S]*?)<IPC>\", n).group(1) \n",
    "            n_ttl = n_ttl.lower() \n",
    "            n_ipc = re.search(\"<IPC>([\\s\\S]*?)<ABST>\", n).group(1) \n",
    "            n_ipc = n_ipc[:3] \n",
    "            n_clms = re.search(\"<CLMS>([\\s\\S]*?)<DESC>\", n).group(1) \n",
    "            n_ind_clms = n_clms.split(\"\\n\\n\") \n",
    "            n_clean_ind_clms = [] \n",
    "            for n_ind_clm in n_ind_clms: \n",
    "                if \"(canceled)\" in n_ind_clm: \n",
    "                    continue \n",
    "                else:\n",
    "                    n_clean_ind_clms.append(self.clean_text(n_ind_clm)) \n",
    "            n_text_input = \"[IPC]\" + n_ipc + \"[TTL]\" + n_ttl \n",
    "            for i in range(len(n_clean_ind_clms)):  \n",
    "                n_text_input = n_text_input + \"[CLMS]\" + n_clean_ind_clms[i] \n",
    "            encoded_n = self.tokenizer(n_text_input, return_tensors=\"pt\", max_length=self.chunk_size, padding=\"max_length\", truncation=True) \n",
    "            \n",
    "            \n",
    "            if self.is_test != True: \n",
    "                input_ids.append(encoded_q[\"input_ids\"]) \n",
    "                attn_masks.append(encoded_q[\"attention_mask\"])\n",
    "                labels.append(ids*2) \n",
    "            \n",
    "                input_ids.append(encoded_p[\"input_ids\"]) \n",
    "                attn_masks.append(encoded_p[\"attention_mask\"]) \n",
    "                labels.append(ids*2) \n",
    "            \n",
    "                input_ids.append(encoded_n[\"input_ids\"]) \n",
    "                attn_masks.append(encoded_n[\"attention_mask\"]) \n",
    "                labels.append(ids*2 + 1) \n",
    "                ids += 1 \n",
    "            else:\n",
    "                ret.append([encoded_q[\"input_ids\"], \n",
    "                            encoded_q[\"attention_mask\"], \n",
    "                            encoded_p[\"input_ids\"], \n",
    "                            encoded_p[\"attention_mask\"], \n",
    "                            encoded_n[\"input_ids\"], \n",
    "                            encoded_n[\"attention_mask\"]]) \n",
    "                \n",
    "        if self.is_test == False: \n",
    "            input_ids = torch.stack(input_ids, dim=0).squeeze(dim=1) \n",
    "            attn_masks = torch.stack(attn_masks, dim=0).squeeze(dim=1) \n",
    "            labels = torch.tensor(labels, dtype=int)  \n",
    "            return input_ids, attn_masks, labels \n",
    "        else:\n",
    "            return ret \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f144aef3-98d9-4fa7-95ac-87d678038dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TripletData(\"../storage/train_triplet.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2fb28296-4510-4fe2-b2b1-c23ab24e02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate = custom_collate(is_test=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=4, collate_fn=collate, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0954f10-3e28-4b59-ae83-3e2ef0e21c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68527820-a470-48ac-ac6a-5f867957fe5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a793de3-c77a-4b24-bb51-7657b4560c07",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7402623b-8005-49f6-8e31-5eed52002d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsoBN(nn.Module): \n",
    "    def __init__(self, config): \n",
    "        #self.cov = torch.zeros(config.hidden_size, config.hidden_size).cuda() \n",
    "        #self.std = torch.zeros(config.hidden_size).cuda() \n",
    "        self.cov = torch.zeros(config.hidden_size, config.hidden_size)  \n",
    "        self.std = torch.zeros(config.hidden_size)\n",
    "    def forward(self, inputs, momentum=0.05, eps=1e-3, beta=0.5):\n",
    "        if self.training: \n",
    "            x = inputs.detach()\n",
    "            n = x.size(0) \n",
    "            mean = x.mean(dim=0) \n",
    "            y = x - mean.unsqueeze(0) \n",
    "            std = (y**2).mean(0) ** 0.5 \n",
    "            cov = (y.t() @ y) / n \n",
    "            self.cov.data += momentum * (cov.data - self.cov.data) \n",
    "            self.std.data += momentum * (std.data - self.std.data) \n",
    "        corr = torch.clamp(self.cov / torch.ger(self.std, self.std), -1, 1) \n",
    "        gamma = (corr ** 2).mean(1) \n",
    "        denorm = (gamma * self.std) \n",
    "        scale = 1 / (denorm + eps) ** beta \n",
    "        E = torch.diag(self.cov).sum() \n",
    "        new_E = (torch.diag(self.cov) * (scale ** 2)).sum() \n",
    "        m = (E / (new_E + eps)) ** 0.5 \n",
    "        scale *= m \n",
    "        return inputs * scale.unsqueeze(0).detach() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "46ac9ff4-be94-428e-859a-d4bd16aae1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralRanker(pl.LightningModule): \n",
    "    def __init__(self, \n",
    "                 hparams=dict(), \n",
    "                 plm=\"tanapatentlm/patentdeberta_base_spec_1024_pwi\", \n",
    "                 is_train=True, \n",
    "                 use_mean_pool=True, \n",
    "                 use_isobn=False, \n",
    "                 use_contrastive_loss=True): \n",
    "        super(NeuralRanker, self).__init__() \n",
    "        self.hparams.update(hparams) \n",
    "        self.save_hyperparameters(ignore=\"hparams\") \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(plm)\n",
    "        self.config = AutoConfig.from_pretrained(plm)  \n",
    "        self.isobn = IsoBN(self.config) \n",
    "        \n",
    "        print(\"initialize PLM from previous checkpoint trained on FGH_v0.3.1\") \n",
    "        self.net = AutoModel.from_pretrained(plm) \n",
    "        state_dict = torch.load(hparams[\"checkpoint\"], map_location=self.device) \n",
    "        new_weights = self.net.state_dict() \n",
    "        old_weights = list(state_dict.items()) \n",
    "        i = 0 \n",
    "        for k, _ in new_weights.items(): \n",
    "            new_weights[k] = old_weights[i][1] \n",
    "            i += 1 \n",
    "        self.net.load_state_dict(new_weights) \n",
    "        \n",
    "        if is_train == False:  \n",
    "            self.net.eval() # change to evaluation mode  \n",
    "        if use_contrastive_loss: \n",
    "            self.metric = losses.ContrastiveLoss() # default is L2 distance \n",
    "        else: \n",
    "            self.metric = losses.TripletMarginLoss() # default is L2 distance \n",
    "        if \"additional_special_tokens\" in self.hparams and self.hparams[\"additional_special_tokens\"]: \n",
    "            additional_special_tokens = self.hparams[\"additional_special_tokens\"] \n",
    "            self.tokenizer.add_special_tokens({\"additional_special_tokens\": additional_special_tokens}) \n",
    "            self.net.resize_token_embeddings(len(self.tokenizer))  \n",
    "            \n",
    "    def mean_pooling(self, model_output, attention_mask): \n",
    "        token_embeddings = model_output[0] \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float() \n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        model_output = self.net(input_ids, attention_mask)  \n",
    "        if use_mean_pool: \n",
    "            model_output = self.mean_pooling(model_output, attention_mask)  \n",
    "        elif use_isobn: \n",
    "            model_output = self.isobn(model_output)\n",
    "        return model_output \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), \n",
    "                                      lr=float(self.hparams.lr), \n",
    "                                      weight_decay=float(self.hparams.weight_decay), \n",
    "                                      eps=float(self.hparams.adam_epsilon))\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps = self.hparams.warmup_steps, \n",
    "            num_training_steps = self.trainer.estimated_stepping_batches, \n",
    "        ) \n",
    "        scheduler = {\"scheduler\":scheduler, \"interval\": \"step\", \"frequency\": 1} \n",
    "        return [optimizer], [scheduler] \n",
    "\n",
    "    def training_step(self, batch, batch_idx): \n",
    "        input_ids, attn_masks, labels = zip(*batch) \n",
    "        embeddings = self(input_ids, attn_masks) \n",
    "        loss = self.metric(embeddings, labels) \n",
    "        self.log(\"train_loss\", loss, batch_size=len(batch)) \n",
    "        return {\"loss\": loss} \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx): \n",
    "        input_ids, attn_masks, labels = zip(*batch) \n",
    "        embeddings = self(input_ids, attn_masks) \n",
    "        loss = self.metric(embeddings, labels) \n",
    "        self.log(\"val_loss\", loss, batch_size=len(batch_size)) \n",
    "        return {\"val_loss\": loss} \n",
    "\n",
    "    def validation_epoch_end(self, outputs): \n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean() \n",
    "        print(f\"\\nEpoch {self.current_epoch} | avg_loss:{avg_loss}\\n\")\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx: int, dataloader_idx: int=0): \n",
    "        q_input_ids, q_attn_masks, p_input_ids, p_attn_masks, n_input_ids, n_attn_masks = zip(*batch) \n",
    "        q_emb = self(q_input_ids, q_attn_masks) \n",
    "        p_emb = self(p_input_ids, p_attn_masks) \n",
    "        n_emb = self(n_input_ids, n_attn_masks) \n",
    "        return q_emb, p_emb, n_emb \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0401aa8a-61c6-4ca4-86bd-d8040f1a2ce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IsoBN' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [222]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_set, batch_size\u001b[38;5;241m=\u001b[39mhparams\u001b[38;5;241m.\u001b[39mbatch_size, collate_fn\u001b[38;5;241m=\u001b[39mcollate, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m     12\u001b[0m valid_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(valid_set, batch_size\u001b[38;5;241m=\u001b[39mhparams\u001b[38;5;241m.\u001b[39mbatch_size, collate_fn\u001b[38;5;241m=\u001b[39mcollate, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralRanker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m ckpt_callback \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m     17\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     18\u001b[0m     dirpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../storage/checkpoint/v1.4.1s_experiments/DeBERTa_ContrastiveLoss/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     22\u001b[0m ) \n\u001b[1;32m     24\u001b[0m SWA \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mStochasticWeightAveraging(swa_lrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m) \n",
      "Input \u001b[0;32mIn [221]\u001b[0m, in \u001b[0;36mNeuralRanker.__init__\u001b[0;34m(self, hparams, plm, is_train, use_mean_pool, use_isobn, use_contrastive_loss)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(plm)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(plm)  \n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misobn \u001b[38;5;241m=\u001b[39m \u001b[43mIsoBN\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitialize PLM from previous checkpoint trained on FGH_v0.3.1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(plm) \n",
      "Input \u001b[0;32mIn [220]\u001b[0m, in \u001b[0;36mIsoBN.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config): \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#self.cov = torch.zeros(config.hidden_size, config.hidden_size).cuda() \u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#self.std = torch.zeros(config.hidden_size).cuda() \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)   \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(config\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IsoBN' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    parser = argparse.ArgumentParser() \n",
    "    parser.add_argument(\"--setting\", \"-s\", type=str, default=\"setting/default.yaml\", help=\"Experiment settings\") \n",
    "    args = parser.parse_args(args=[])  \n",
    "    hparams = addict.Addict(dict(load_hparams_from_yaml(args.setting)))\n",
    "    \n",
    "    train_set = TripletData(\"../storage/train_triplet.csv\") \n",
    "    valid_set = TripletData(\"../storage/valid_triplet.csv\") \n",
    "    collate = custom_collate(is_test=False)\n",
    "\n",
    "    train_dataloader = DataLoader(train_set, batch_size=hparams.batch_size, collate_fn=collate, shuffle=True) \n",
    "    valid_dataloader = DataLoader(valid_set, batch_size=hparams.batch_size, collate_fn=collate, shuffle=False) \n",
    "    \n",
    "    model = NeuralRanker(hparams)\n",
    "    \n",
    "    ckpt_callback = pl.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\", \n",
    "        dirpath=f\"../storage/checkpoint/v1.4.1s_experiments/DeBERTa_ContrastiveLoss/\", \n",
    "        filename=\"checkpoints-{epoch:02d}-{val_loss:.8f}\",\n",
    "        save_top_k=3, \n",
    "        mode=\"min\" \n",
    "    ) \n",
    "    \n",
    "    SWA = pl.callbacks.StochasticWeightAveraging(swa_lrs=1e-2) \n",
    "    \n",
    "    device_cnt = torch.cuda.device_count() \n",
    "    \n",
    "    trainer = pl.Trainer(gpus=device_cnt, \n",
    "                         max_epochs=hparams.epochs, \n",
    "                         strategy=\"ddp\" if device_cnt > 1 else None, \n",
    "                         callbacks=[ckpt_callback, SWA], \n",
    "                         gradient_clip_val=1.0, \n",
    "                         accumulate_grad_batches=10,\n",
    "                         auto_lr_find=True)\n",
    "    \n",
    "    \n",
    "    print(\"Training Model\") \n",
    "    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb27bb-1396-403f-81a6-0a4b0fa1e33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f617d1-780d-47dc-a56f-5e64a5efa2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "what exactly should we fix? \n",
    "- different types of PLM? \n",
    "- different types of input format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c7137-4606-4aea-ba54-b7502e1922bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c1e62-5926-47c5-b014-c813c29d4cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c23eca-56df-48f1-821d-885625d8ec04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d673f-542d-484b-98f2-2c85846dd00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a220d-1a7c-4445-bf07-ec662803a43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499f4b0-c52d-4e63-982f-dcd309da2a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ccc9a8-d8df-41bd-b03e-174e14e527a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203aa19-aabb-4b50-9bee-8b5610017425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e138ca-2e9b-46fa-8e16-5987099d5002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
