{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline with Bert  \n",
    "\n",
    "I will eventually use dense retreival based models. This is simply a baseline. \n",
    "\n",
    "This BERT model is used for processing the abstract information from patent documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.functional as f \n",
    "from torch.optim import AdamW \n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "from sklearn.model_selection import train_test_split \n",
    "import time \n",
    "import datetime \n",
    "import re \n",
    "import math \n",
    "import sklearn \n",
    "import os\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import BertModel, BertTokenizer, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sample files = 597\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('sample_data')\n",
    "print(\"number of sample files = {}\".format(len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['patent1id', 'patent2id', 'fullpatent1id', 'fullpatent2id', 'patent1abstract', 'patent2abstract', 'patent1claims', 'patent2claims'])\n"
     ]
    }
   ],
   "source": [
    "with open('sample_data/'+files[0]) as f: \n",
    "    data = json.load(f) \n",
    "    print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few problematic json files. For now let's only use samples that we can parse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem occured with file 12001003_4589656.json\n",
      "Extra data: line 114 column 2 (char 25639)\n",
      "problem occured with file 12001021_20060183900.json\n",
      "Extra data: line 395 column 2 (char 126007)\n",
      "problem occured with file 12000641_20070108429.json\n",
      "Extra data: line 125 column 2 (char 30036)\n",
      "problem occured with file 12000959_20050276053.json\n",
      "Extra data: line 100 column 2 (char 14009)\n",
      "problem occured with file 12000891_5700250.json\n",
      "Extra data: line 85 column 2 (char 17949)\n",
      "problem occured with file 12000822_20020020426.json\n",
      "Extra data: line 427 column 2 (char 86260)\n",
      "problem occured with file 12000618_20080055227.json\n",
      "Extra data: line 141 column 2 (char 41587)\n",
      "problem occured with file 12000491_3751333.json\n",
      "Extra data: line 37 column 2 (char 5139)\n",
      "problem occured with file 12000662_20040094050.json\n",
      "Extra data: line 253 column 2 (char 63125)\n",
      "problem occured with file 12000414_6913295.json\n",
      "Extra data: line 73 column 2 (char 14197)\n",
      "problem occured with file 12000811_20020033664.json\n",
      "Extra data: line 53 column 2 (char 11167)\n",
      "problem occured with file 12000872_20050284529.json\n",
      "Extra data: line 46 column 2 (char 10372)\n",
      "problem occured with file 12001025_5994629.json\n",
      "Extra data: line 86 column 2 (char 24674)\n",
      "problem occured with file 12000414_6347640.json\n",
      "Extra data: line 73 column 2 (char 11635)\n",
      "problem occured with file 12000491_4140911.json\n",
      "Extra data: line 47 column 2 (char 8712)\n",
      "problem occured with file 12000618_20070046608.json\n",
      "Extra data: line 130 column 2 (char 46161)\n",
      "problem occured with file 12000222_20050232963.json\n",
      "Extra data: line 133 column 2 (char 23226)\n",
      "problem occured with file 12000564_20030118876.json\n",
      "Extra data: line 89 column 2 (char 25011)\n",
      "problem occured with file 12000959_20070274093.json\n",
      "Extra data: line 68 column 2 (char 13218)\n",
      "problem occured with file 12000618_20090121998.json\n",
      "Extra data: line 107 column 2 (char 40566)\n",
      "problem occured with file 12000222_20060165635.json\n",
      "Extra data: line 117 column 2 (char 21304)\n",
      "problem occured with file 12001062_20080254336.json\n",
      "Extra data: line 102 column 2 (char 25058)\n",
      "problem occured with file 12000414_5529086.json\n",
      "Extra data: line 89 column 2 (char 17655)\n",
      "problem occured with file 12000811_20060103324.json\n",
      "Extra data: line 101 column 2 (char 21074)\n",
      "problem occured with file 12000822_20040168698.json\n",
      "Extra data: line 83 column 2 (char 17035)\n",
      "problem occured with file 12000546_5461557.json\n",
      "Extra data: line 74 column 2 (char 19642)\n",
      "problem occured with file 12000564_20040144579.json\n",
      "Extra data: line 34 column 2 (char 9830)\n",
      "problem occured with file 12000990_5345388.json\n",
      "Extra data: line 91 column 2 (char 29753)\n",
      "problem occured with file 12000242_20040232509.json\n",
      "Extra data: line 140 column 2 (char 21415)\n",
      "problem occured with file 12000891_5242410.json\n",
      "Extra data: line 89 column 2 (char 19854)\n",
      "problem occured with file 12000811_20070262707.json\n",
      "Extra data: line 74 column 2 (char 12850)\n",
      "problem occured with file 12000018_6189895.json\n",
      "Extra data: line 56 column 2 (char 13126)\n",
      "problem occured with file 12000879_6631668.json\n",
      "Extra data: line 69 column 2 (char 15549)\n",
      "problem occured with file 12001062_20060257718.json\n",
      "Extra data: line 49 column 2 (char 14449)\n",
      "problem occured with file 12000298_6851645.json\n",
      "Extra data: line 120 column 2 (char 29592)\n",
      "problem occured with file 12000716_20030156467.json\n",
      "Extra data: line 172 column 2 (char 34440)\n"
     ]
    }
   ],
   "source": [
    "sample_train = [] \n",
    "\n",
    "for idx, file in enumerate(files): \n",
    "    filename = 'sample_data/' + file\n",
    "    if '.json' not in filename: \n",
    "        continue\n",
    "    try: \n",
    "        with open(filename) as f:  \n",
    "            data = json.load(f) \n",
    "            sample_train.append(data)\n",
    "    except Exception as e: \n",
    "        print(\"problem occured with file {}\".format(file))\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the features we have, we are going to use the patent1abstract, patent2abstract and patent1claims and patent2claims. \n",
    "\n",
    "We will build separate models for each pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['patent1id', 'patent2id', 'fullpatent1id', 'fullpatent2id', 'patent1abstract', 'patent2abstract', 'patent1claims', 'patent2claims'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train[0].keys() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "\n",
    "def BERT_Tokenizer(text, MAX_LEN=512): \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = text,  \n",
    "        add_special_tokens = True, \n",
    "        pad_to_max_length = False, \n",
    "        return_attention_mask = True \n",
    "    )\n",
    "    input_id = encoded_dict['input_ids'] \n",
    "    attention_mask = encoded_dict['attention_mask'] \n",
    "    \n",
    "    # get the first 512 tokens\n",
    "    text_len = len(input_id)\n",
    "    if len(input_id) > MAX_LEN: \n",
    "        input_id = input_id[:MAX_LEN] \n",
    "        attention_mask = attention_mask[:MAX_LEN] \n",
    "    else: \n",
    "        input_id = input_id + [0]*(MAX_LEN - len(input_id)) \n",
    "        attention_mask = attention_mask + [0]*(MAX_LEN - len(attention_mask)) \n",
    "    \n",
    "    return input_id, attention_mask, text_len \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560/560 [00:04<00:00, 132.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# store tokenzied abstract and claims \n",
    "abstract1_input_ids, abstract2_input_ids = [], [] \n",
    "abstract1_attention_masks, abstract2_attention_masks = [], [] \n",
    "abstract1_lengths, abstract2_lengths = [], [] \n",
    "labels = []  \n",
    "\n",
    "for i in tqdm(range(len(sample_train)), position=0, leave=True): \n",
    "    a1 = sample_train[i]['patent1abstract'] \n",
    "    a2 = sample_train[i]['patent2abstract'] \n",
    "               \n",
    "    a1_input_id, a1_attention_mask, a1_length = BERT_Tokenizer(a1)\n",
    "    abstract1_input_ids.append(a1_input_id) \n",
    "    abstract1_attention_masks.append(a1_attention_mask)\n",
    "    abstract1_lengths.append(a1_length)   \n",
    "    \n",
    "    a2_input_id, a2_attention_mask, a2_length = BERT_Tokenizer(a2) \n",
    "    abstract2_input_ids.append(a2_input_id)\n",
    "    abstract2_attention_masks.append(a2_attention_mask) \n",
    "    abstract2_lengths.append(a2_length) \n",
    "    \n",
    "    # sample data are all similar \n",
    "    labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stats(arr):\n",
    "    print(\"mean tokens = {}\".format(np.mean(arr))) \n",
    "    print(\"min tokens = {}\".format(np.min(arr))) \n",
    "    print(\"max tokens = {}\".format(np.max(arr))) \n",
    "    print(\"number of data with more than 512 tokens = {}\".format(np.sum(i > 512 for i in arr))) \n",
    "    print(\"percentage of data with more than 512 tokens = {:.2f}%\".format(np.sum(i > 4096 for i in arr) * 100 / len(arr)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tokens = 137.67857142857142\n",
      "min tokens = 14\n",
      "max tokens = 410\n",
      "number of data with more than 512 tokens = 0\n",
      "percentage of data with more than 512 tokens = 0.00%\n",
      "\n",
      "mean tokens = 153.3125\n",
      "min tokens = 24\n",
      "max tokens = 360\n",
      "number of data with more than 512 tokens = 0\n",
      "percentage of data with more than 512 tokens = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "token_stats(abstract1_lengths) \n",
    "\n",
    "token_stats(abstract2_lengths) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([560, 512]),\n",
       " torch.Size([560, 512]),\n",
       " torch.Size([560, 512]),\n",
       " torch.Size([560, 512]),\n",
       " torch.Size([560, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract1_input_ids = torch.tensor(abstract1_input_ids, dtype=int)\n",
    "abstract1_attention_masks = torch.tensor(abstract1_attention_masks, dtype=int) \n",
    "\n",
    "abstract2_input_ids = torch.tensor(abstract2_input_ids, dtype=int)\n",
    "abstract2_attention_masks = torch.tensor(abstract2_attention_masks, dtype=int) \n",
    "\n",
    "labels = torch.tensor(labels, dtype=float)\n",
    "labels = torch.reshape(labels, (-1,1))\n",
    "\n",
    "abstract1_input_ids.shape, abstract1_attention_masks.shape, abstract2_input_ids.shape, abstract2_attention_masks.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([504, 512]),\n",
       " torch.Size([56, 512]),\n",
       " torch.Size([504, 512]),\n",
       " torch.Size([56, 512]),\n",
       " torch.Size([504, 512]),\n",
       " torch.Size([504, 512]),\n",
       " torch.Size([504, 1]),\n",
       " torch.Size([56, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_abstract1_input_ids, val_abstract1_input_ids, train_abstract1_attention_masks, val_abstract1_attention_masks = train_test_split(abstract1_input_ids, \n",
    "                                                                                                                                      abstract1_attention_masks, \n",
    "                                                                                                                                      random_state = 888, \n",
    "                                                                                                                                      test_size = 0.1) \n",
    "\n",
    "train_abstract2_input_ids, val_abstract2_input_ids, train_abstract2_attention_masks, val_abstract2_attention_masks = train_test_split(abstract2_input_ids,\n",
    "                                                                                                                                      abstract2_attention_masks, \n",
    "                                                                                                                                      random_state = 888, \n",
    "                                                                                                                                      test_size = 0.1)\n",
    "\n",
    "\n",
    "\n",
    "_, _, train_labels, val_labels = train_test_split(abstract1_input_ids, \n",
    "                                                  labels, \n",
    "                                                  random_state = 888, \n",
    "                                                  test_size = 0.1)\n",
    "\n",
    "\n",
    "train_abstract1_input_ids.shape, val_abstract1_input_ids.shape, train_abstract1_attention_masks.shape, val_abstract1_attention_masks.shape, train_abstract2_input_ids.shape, train_abstract2_attention_masks.shape, train_labels.shape, val_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_data = TensorDataset(train_abstract1_input_ids, \n",
    "                           train_abstract1_attention_masks, \n",
    "                           train_abstract2_input_ids,  \n",
    "                           train_abstract2_attention_masks, \n",
    "                           train_labels) \n",
    "\n",
    "train_sampler = RandomSampler(train_data) \n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = TensorDataset(val_abstract1_input_ids, \n",
    "                                val_abstract1_attention_masks, \n",
    "                                val_abstract2_input_ids, \n",
    "                                val_abstract2_attention_masks, \n",
    "                                val_labels) \n",
    "\n",
    "val_sampler = SequentialSampler(validation_data)\n",
    "\n",
    "val_dataloader = DataLoader(validation_data, sampler = val_sampler, batch_size = batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model that takes in abstract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_ABSTRACT(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(BERT_ABSTRACT, self).__init__()\n",
    "        self.bert1 = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "        self.bert2 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.fc1 = nn.Linear(768*2, 768) \n",
    "        self.batchnorm1 = nn.BatchNorm1d(768)\n",
    "        self.fc2 = nn.Linear(768, 256) \n",
    "        self.batchnorm2 = nn.BatchNorm1d(256) \n",
    "        self.fc3 = nn.Linear(256, 1) \n",
    "        self.activation = nn.Sigmoid() \n",
    "    \n",
    "    def forward(self, ids1, masks1, ids2, masks2): \n",
    "        outputs1 = self.bert1(input_ids = ids1, \n",
    "                              attention_mask = masks1) \n",
    "        pooler1 = outputs1.pooler_output \n",
    "        \n",
    "        outputs2 = self.bert2(input_ids = ids2, \n",
    "                              attention_mask = masks2) \n",
    "        pooler2 = outputs2.pooler_output \n",
    "                \n",
    "        x = torch.cat((pooler1, pooler2), 1) \n",
    "        fc1 = self.fc1(x) \n",
    "        bn1 = self.batchnorm1(fc1) \n",
    "        fc2 = self.fc2(bn1) \n",
    "        bn2 = self.batchnorm2(fc2) \n",
    "        fc3 = self.activation(self.fc3(bn2)) \n",
    "        return fc3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT_ABSTRACT(\n",
       "  (bert1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bert2): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (batchnorm1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (batchnorm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERT_ABSTRACT() \n",
    "\n",
    "model.cuda() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels): \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten() \n",
    "    labels_flat = labels.flatten() \n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed): \n",
    "    elapsed_rounded = int(round(elapsed)) \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping: \n",
    "    ''' if validation loss does not decrease anymore, we stop training '''\n",
    "    def __init__(self, patience, verbose, delta, path): \n",
    "        self.patience = patience \n",
    "        self.verbose = verbose \n",
    "        self.counter = 0 \n",
    "        self.best_score = None \n",
    "        self.early_stop = False \n",
    "        self.val_loss_min = np.Inf \n",
    "        self.delta = delta \n",
    "        self.path = path \n",
    "    \n",
    "    def __call__(self, val_loss, model): \n",
    "        score = -val_loss \n",
    "        if self.best_score is None:  \n",
    "            self.best_score = score \n",
    "            self.save_checkpoint(val_loss, model) \n",
    "        elif score < self.best_score + self.delta: \n",
    "            self.counter += 1 \n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience: \n",
    "                self.early_stop = True \n",
    "        else: \n",
    "            self.best_score = score \n",
    "            self.save_checkpoint(val_loss, model) \n",
    "            self.counter = 0 \n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model): \n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.7095992743968964\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.7068398803472519\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.7050280412038167\n",
      "\n",
      " Average Training Loss: 0.7046075510600257\n",
      " Training epoch took: 0:00:38\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 1.0478327785219466\n",
      "Validation loss decreased (inf --> 1.047833).  Saving model ...\n",
      "\n",
      "===== Epoch 2 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6952129006385803\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.6935117110610008\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.6912469546000163\n",
      "\n",
      " Average Training Loss: 0.6910049574715751\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5154147318431309\n",
      "Validation loss decreased (1.047833 --> 0.515415).  Saving model ...\n",
      "\n",
      "===== Epoch 3 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6857379525899887\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.6843292877078057\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.682505077123642\n",
      "\n",
      " Average Training Loss: 0.6822415713279967\n",
      " Training epoch took: 0:00:38\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.691442711012704\n",
      "EarlyStopping counter: 1 out of 3\n",
      "\n",
      "===== Epoch 4 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6759418100118637\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.6748712584376335\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.6738911161820094\n",
      "\n",
      " Average Training Loss: 0.673712758790879\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6822461145264762\n",
      "EarlyStopping counter: 2 out of 3\n",
      "\n",
      "===== Epoch 5 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.66919005215168\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.6681515529751778\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.6670772423346837\n",
      "\n",
      " Average Training Loss: 0.6669203478192526\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6373594743864877\n",
      "EarlyStopping counter: 3 out of 3\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 6 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6627789616584778\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.6619004383683205\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.6609767595926921\n",
      "\n",
      " Average Training Loss: 0.6608238598657032\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6119768449238369\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 7 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6567982465028763\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.6558685019612313\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.6549508929252624\n",
      "\n",
      " Average Training Loss: 0.654807898733351\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6235815456935337\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 8 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6508102208375931\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.6499050781130791\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.649034763375918\n",
      "\n",
      " Average Training Loss: 0.6488966799917675\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6231360180037362\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 9 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6452383399009705\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.6442299649119377\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.6431814094384511\n",
      "\n",
      " Average Training Loss: 0.6430325848715646\n",
      " Training epoch took: 0:00:38\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6441455398287091\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 10 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6389374077320099\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.6380881533026695\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.6372539639472962\n",
      "\n",
      " Average Training Loss: 0.6371272121156965\n",
      " Training epoch took: 0:00:38\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6163207973752703\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 11 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6332798689603806\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.6323011517524719\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.6312882155179977\n",
      "\n",
      " Average Training Loss: 0.6311363549459548\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6299194778714862\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 12 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6271313518285752\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.626239949464798\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.6253112544616063\n",
      "\n",
      " Average Training Loss: 0.6251701684225173\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6631420510155814\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 13 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6218228906393051\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.6207111060619355\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.6195966432491938\n",
      "\n",
      " Average Training Loss: 0.6194314133553278\n",
      " Training epoch took: 0:00:38\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6420658911977496\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 14 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6151152193546295\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.6141677916049957\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.6133023659388225\n",
      "\n",
      " Average Training Loss: 0.6131529363374861\n",
      " Training epoch took: 0:00:38\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6206048045839582\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 15 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6091441482305526\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.6081648215651512\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.6071874221165975\n",
      "\n",
      " Average Training Loss: 0.6070667713407486\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6013550758361816\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 16 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.6029023051261901\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.6019227921962738\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.6009483873844147\n",
      "\n",
      " Average Training Loss: 0.6008881803542848\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.6058558140482221\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 17 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5969568520784378\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.5959392264485359\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5949419478575388\n",
      "\n",
      " Average Training Loss: 0.5947927596077086\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5863297070775714\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 18 / 50 =====\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5907029181718826\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.5897227376699448\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5887657543023427\n",
      "\n",
      " Average Training Loss: 0.5886216409622677\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5838807225227356\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 19 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.584589147567749\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.5836098745465279\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5826448728640874\n",
      "\n",
      " Average Training Loss: 0.5824986194807386\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.587882970060621\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 20 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.578468120098114\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.5775068312883377\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5765487482150395\n",
      "\n",
      " Average Training Loss: 0.5764035090567574\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5553085463387626\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 21 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5725535005331039\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.5715759739279747\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5705916206041972\n",
      "\n",
      " Average Training Loss: 0.5704448601556202\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.566575152533395\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 22 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5664760798215867\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.565528205037117\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5646097838878632\n",
      "\n",
      " Average Training Loss: 0.5644683109389411\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.548637866973877\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 23 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5605737268924713\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.5596666648983956\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.5587514768044154\n",
      "\n",
      " Average Training Loss: 0.5586177006600395\n",
      " Training epoch took: 0:00:38\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5512229970523289\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 24 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5547840476036072\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.5538486778736115\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.5529597987731297\n",
      "\n",
      " Average Training Loss: 0.5528232795851571\n",
      " Training epoch took: 0:00:38\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5507566843714032\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 25 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5490960717201233\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.5482188180088997\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.5473691036303838\n",
      "\n",
      " Average Training Loss: 0.547236702744923\n",
      " Training epoch took: 0:00:38\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5413760117122105\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 26 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5435596406459808\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.5427451699972152\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5419457952181498\n",
      "\n",
      " Average Training Loss: 0.5418194437783862\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5302152378218514\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 27 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5388514757156372\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.5378452897071838\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5368742555379867\n",
      "\n",
      " Average Training Loss: 0.5367378460036384\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5491426246506828\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 28 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5330968558788299\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.5322795629501342\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5314522018035253\n",
      "\n",
      " Average Training Loss: 0.5313291625371055\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5124210715293884\n",
      "saving best checkpoint after early stopping!\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 29 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5279856950044632\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.527241724729538\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.526484935482343\n",
      "\n",
      " Average Training Loss: 0.5263671875\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5198005523000445\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 30 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.523127356171608\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.5223933979868889\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5216751903295517\n",
      "\n",
      " Average Training Loss: 0.5215647381449503\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.532537545476641\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 31 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5184872567653656\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.5177654519677162\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5170603891213735\n",
      "\n",
      " Average Training Loss: 0.5169571789484175\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5127059817314148\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 32 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5140482127666474\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.5133671045303345\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5126952558755875\n",
      "\n",
      " Average Training Loss: 0.5125947263505723\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5111634646143232\n",
      "saving best checkpoint after early stopping!\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 33 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5098059505224228\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.5091608688235283\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5085203985373179\n",
      "\n",
      " Average Training Loss: 0.5084237437399607\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.4933441919939859\n",
      "saving best checkpoint after early stopping!\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 34 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5057836771011353\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.5051746144890785\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.5045695990324021\n",
      "\n",
      " Average Training Loss: 0.5044812493854098\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.5046565277235848\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 35 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.5020273000001907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.5014241307973861\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.5008485421538353\n",
      "\n",
      " Average Training Loss: 0.5007604365310971\n",
      " Training epoch took: 0:00:38\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.49922143135751995\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 36 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.49838278591632845\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.497848392277956\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.49731280306975045\n",
      "\n",
      " Average Training Loss: 0.49723181412333534\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.4938392766884395\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 37 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.4951235368847847\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.4945887580513954\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.4940665692090988\n",
      "\n",
      " Average Training Loss: 0.493990215044173\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.4868889365877424\n",
      "saving best checkpoint after early stopping!\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 38 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.4919570803642273\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.4915086530148983\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.4910437524318695\n",
      "\n",
      " Average Training Loss: 0.49097085566747756\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.4811494265283857\n",
      "saving best checkpoint after early stopping!\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 39 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.48900912404060365\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.4885720208287239\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.48814847022295\n",
      "\n",
      " Average Training Loss: 0.4880855442985656\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.4916740655899048\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 40 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.4863385736942291\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.4859508603811264\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.4855785404642423\n",
      "\n",
      " Average Training Loss: 0.48551984864567954\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.47491263491766794\n",
      "saving best checkpoint after early stopping!\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 41 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.4839430794119835\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.4835638917982578\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.48324377586444217\n",
      "\n",
      " Average Training Loss: 0.48318952558532596\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.4823533850056784\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 42 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.4817414477467537\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.4814102575182915\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.4810886964201927\n",
      "\n",
      " Average Training Loss: 0.4810430748122079\n",
      " Training epoch took: 0:00:38\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.4760226735046932\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 43 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.47976101785898206\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.47947883754968645\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.47919613917668663\n",
      "\n",
      " Average Training Loss: 0.4791547703364539\n",
      " Training epoch took: 0:00:38\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.478108035666602\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 44 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.4781081065535545\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.4778271235525608\n",
      "  Batch    60  of     63.    Elapsed: 0:00:36.\n",
      "  current average loss = 0.47758208513259887\n",
      "\n",
      " Average Training Loss: 0.4775450111381591\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.48240023851394653\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 45 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:13.\n",
      "  current average loss = 0.4765982463955879\n",
      "  Batch    40  of     63.    Elapsed: 0:00:25.\n",
      "  current average loss = 0.4763832576572895\n",
      "  Batch    60  of     63.    Elapsed: 0:00:38.\n",
      "  current average loss = 0.4761787121494611\n",
      "\n",
      " Average Training Loss: 0.476150167366815\n",
      " Training epoch took: 0:00:40\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.4785230372633253\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 46 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.47533763349056246\n",
      "  Batch    40  of     63.    Elapsed: 0:00:24.\n",
      "  current average loss = 0.4751398637890816\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.4749829525748889\n",
      "\n",
      " Average Training Loss: 0.4749567764145987\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.49131390878132414\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 47 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.4742713987827301\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.4741309143602848\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.47400230417648953\n",
      "\n",
      " Average Training Loss: 0.47398447233533103\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.47383882318224224\n",
      "saving best checkpoint after early stopping!\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 48 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.47349024266004563\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.4733897879719734\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.4732943192124367\n",
      "\n",
      " Average Training Loss: 0.47328051262431675\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.4704805782863072\n",
      "saving best checkpoint after early stopping!\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 49 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.47295346558094026\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.4728806182742119\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.47283421009778975\n",
      "\n",
      " Average Training Loss: 0.47282700926538496\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.47138812286513193\n",
      "We are out of patience\n",
      "\n",
      "===== Epoch 50 / 50 =====\n",
      "Training...\n",
      "  Batch    20  of     63.    Elapsed: 0:00:12.\n",
      "  current average loss = 0.47261184453964233\n",
      "  Batch    40  of     63.    Elapsed: 0:00:23.\n",
      "  current average loss = 0.4725929036736488\n",
      "  Batch    60  of     63.    Elapsed: 0:00:35.\n",
      "  current average loss = 0.4725739861528079\n",
      "\n",
      " Average Training Loss: 0.47257347286693635\n",
      " Training epoch took: 0:00:37\n",
      "\n",
      "Running Validation\n",
      " Average validation loss: 0.4724675246647426\n",
      "We are out of patience\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs \n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "train_losses, val_losses = [], [] \n",
    "### binary crossentropy loss ### \n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "### early stopping ### \n",
    "early_stopping = EarlyStopping(patience = 3, verbose = True, delta=0, path=\"BERT_ABSTRACT.pt\")\n",
    "tolerance = True \n",
    "\n",
    "### initialize gradient ###\n",
    "model.zero_grad() \n",
    "\n",
    "for epoch_i in range(0, epochs):  \n",
    "    ### Training ### \n",
    "    print(\"\")\n",
    "    print(\"===== Epoch {:} / {:} =====\".format(epoch_i + 1, epochs))\n",
    "    print(\"Training...\")\n",
    "    t0 = time.time() \n",
    "    total_loss = 0 \n",
    "    model.train() \n",
    "    for step, batch in enumerate(train_dataloader): \n",
    "        if step%20 == 0 and not step == 0: \n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print('  current average loss = {}'.format(total_loss / step))\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        \n",
    "        b_ids1, b_masks1, b_ids2, b_masks2, b_labels = batch \n",
    "        outputs = model(ids1 = b_ids1, \n",
    "                        masks1 = b_masks1, \n",
    "                        ids2 = b_ids2, \n",
    "                        masks2 = b_masks2) \n",
    "        \n",
    "\n",
    "        \n",
    "        loss = criterion(outputs.float(), b_labels.float()) \n",
    "        total_loss += loss.item() \n",
    "        loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "        optimizer.step() \n",
    "        scheduler.step() \n",
    "        model.zero_grad() \n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader) \n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\" Average Training Loss: {}\".format(avg_train_loss)) \n",
    "    print(\" Training epoch took: {:}\".format(format_time(time.time() - t0))) \n",
    "    \n",
    "    ### Validation ### \n",
    "    print(\"\")\n",
    "    print(\"Running Validation\") \n",
    "    t0 = time.time() \n",
    "    model.eval() \n",
    "    \n",
    "    eval_loss = 0 \n",
    "    \n",
    "    for batch in val_dataloader: \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        b_ids1, b_masks1, b_ids2, b_masks2, b_labels = batch \n",
    "        with torch.no_grad(): \n",
    "            outputs = model(ids1 = b_ids1, \n",
    "                            masks1 = b_masks1, \n",
    "                            ids2 = b_ids2, \n",
    "                            masks2 = b_masks2)\n",
    "        \n",
    "        loss = criterion(outputs.float(), b_labels.float())\n",
    "        eval_loss += loss.item() \n",
    "        \n",
    "    avg_val_loss = eval_loss / len(val_dataloader)  \n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(\" Average validation loss: {}\".format(avg_val_loss))  \n",
    "    if tolerance == True:  \n",
    "        early_stopping(avg_val_loss, model) \n",
    "    elif tolerance == False: \n",
    "        if avg_val_loss == np.min(val_losses): \n",
    "            print(\"saving best checkpoint after early stopping!\") \n",
    "            torch.save(model.state_dict(), \"BERT_ABSTRACT_\" + str(epoch_i))\n",
    "    \n",
    "    if early_stopping.early_stop: \n",
    "        print(\"We are out of patience\") \n",
    "        tolerance = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa65535be80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(val_losses, label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_abstract1 = sample_train[0]['patent1abstract']\n",
    "test_abstract2 = sample_train[0]['patent2abstract'] \n",
    "\n",
    "test1_input_id, test1_attention_mask, test1_length = BERT_Tokenizer(test_abstract1) \n",
    "test2_input_id, test2_attention_mask, test_length = BERT_Tokenizer(test_abstract2) \n",
    "\n",
    "test1_input_id = torch.tensor(test1_input_id, dtype=int)\n",
    "test1_input_id = torch.reshape(test1_input_id, (1,512)) \n",
    "test1_input_id = test1_input_id.to(device)\n",
    "test1_attention_mask = torch.tensor(test1_attention_mask, dtype=int) \n",
    "test1_attention_mask = torch.reshape(test1_attention_mask, (1,512))\n",
    "test1_attention_mask = test1_attention_mask.to(device)\n",
    "\n",
    "test2_input_id = torch.tensor(test2_input_id, dtype=int) \n",
    "test2_input_id = torch.reshape(test2_input_id, (1,512))\n",
    "test2_input_id = test2_input_id.to(device)\n",
    "test2_attention_mask = torch.tensor(test2_attention_mask, dtype=int) \n",
    "test2_attention_mask = torch.reshape(test2_attention_mask, (1,512)) \n",
    "test2_attention_mask = test2_attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT_ABSTRACT(\n",
       "  (bert1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bert2): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (batchnorm1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (batchnorm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load best checkpoint ### \n",
    "checkpoint = torch.load('BERT_ABSTRACT_47') \n",
    "test_model = BERT_ABSTRACT() \n",
    "test_model.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score = 0.46026527881622314\n"
     ]
    }
   ],
   "source": [
    "test_model.eval() \n",
    "\n",
    "with torch.no_grad(): \n",
    "    outputs = test_model(ids1=test1_input_id, \n",
    "                         masks1=test1_attention_mask, \n",
    "                         ids2=test2_input_id, \n",
    "                         masks2=test2_attention_mask) \n",
    "\n",
    "score = outputs.detach().cpu().numpy().flatten()[0]\n",
    "\n",
    "print(\"Similarity Score = {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
